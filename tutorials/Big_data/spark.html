Spark for Data Engineering

Data Lifecycle

The Raw Data typically means data as generated with multiple data types, formats and across multiple systems

Integrate data from multiple sources

Explore to see patterns, noise, data quality, missing values etc.

Aggregate: Further
Crules: Data cleaning and security rules, aggregation logic to transform the data into a better and more prepared for the tagret usecase

Going down this lifecycle reduces the size of data. You have millions of customers where each customer has a lot information and at the end of the cycle the only insight you're interested to know from that data is that are they going to buy a specific product? Yes or No?
