<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <title>Using Sentiment Analysis to Design a Drug Recommendation Application</title>
    <meta content="" name="description">
    <meta content="" name="keywords">
    <!-- Favicons -->
    <link href="../../assets/img/web-icon.png" rel="icon">
    <link href="../../assets/img/apple-touch-icon.png" rel="apple-touch-icon">
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Roboto:300,300i,400,400i,500,500i,700,700i&display=swap" rel="stylesheet">
    <!-- Vendor CSS Files -->
    <link href="../../assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="../../assets/vendor/animate.css/animate.min.css" rel="stylesheet">
    <link href="../../assets/vendor/icofont/icofont.min.css" rel="stylesheet">
    <link href="../../assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
    <link href="../../assets/vendor/venobox/venobox.css" rel="stylesheet">
    <link href="../../assets/vendor/owl.carousel/assets/owl.carousel.min.css" rel="stylesheet">
    <link href="../../assets/vendor/aos/aos.css" rel="stylesheet">
    <!-- Template Main CSS File -->
    <link href="../../assets/css/style.css" rel="stylesheet">
    <link href="../../assets/css/prism.css" rel="stylesheet">
  </head>
  <body>
    <!-- ======= Header ======= -->
    <header id="header" class="fixed-top ">
      <div class="container">
        <div class="logo float-left">
                <h1 class="text-light"><a href="../../index.html"><span>pyDataScientist</a></h1>
        </div>
        <nav class="nav-menu float-right d-none d-lg-block">
          <ul>
            <li><a href="../../index.html">Home</a></li>
            <li class="active"><a href="../../projects.html">Projects</a></li>
            <li class="drop-down"><a href="../../notes.html">Tutorials</a>
            <ul>
              <li><a href="../../notes.html#Pandas">Pandas</a></li>
              <li class="drop-down"><a href="../../notes.html#Machine-Learning">Machine Learning</a>
              <ul>
                <li><a href="linear_regression.html">Linear Regression</a></li>
                <li><a href="linear_regression.html">Support Vector Machines</a></li>
                <li><a href="linear_regression.html">Naive Bayes</a></li>
                <li><a href="linear_regression.html">Decision Trees</a></li>
                <li><a href="linear_regression.html">Ensemble Methods</a></li>
              </ul>
            </li>
            <li><a href="../../notes.html#Linear-Algebra">Linear Algebra</a></li>
            <li><a href="../../notes.html#Visualization">Visualization</a></li>
            <li><a href="../../notes.html#Jupyter-Notebook">Jupyter Notebook</a></li>
            <li><a href="../../notes.html#Git">Git</a></li>
            <li><a href="../../notes.html#Bash">Bash</a></li>
            <li><a href="../../notes.html#Deep-Learning">Deep Learning</a></li>
            <li><a href="../../notes.html#Conda">Conda</a></li>
            <li><a href="../../notes.html#Numerical-Methods">Numerical Methods</a></li>
            <li><a href="../../notes.html#Big-Data">Big Data</a></li>
            <li><a href="../../notes.html#Classification">Classification</a></li>
            <li><a href="../../notes.html#Pattern-Recognition">Pattern Recognition</a></li>
            <li><a href="../../notes.html#SQL">SQL</a></li>
          </ul>
        </li>
        <li><a href="../../about.html">About Me</a></li>
        <li><a href="../../contact.html">Contact</a></li>
      </ul>
      </nav><!-- .nav-menu -->
    </div>
    </header><!-- End Header -->
    <main id="main">
      <!-- ======= About Us Section ======= -->
      <section class="breadcrumbs">
        <div class="container">
          <div class="d-flex justify-content-between align-items-center">
            <h2>Projects</h2>
            <ol>
              <li><a href="../../projects2.html">Projects</a></li>
              <li>Using Sentiment Analysis to Design a Drug Recommendation Application</li>
            </ol>
          </div>
        </div>
      </section>
      <!-- End Header -->
      <section class="tutorials" data-aos="fade-up" data-aos-easing="ease-in-out" data-aos-duration="1000">
        <div class="container">
          <div class="row">
            <div class="col-12">
                <h2 id="title"><font color='green'>Opti</font><font color='orange'>Med</font> - What<font color='orange'>Medicine</font> <font color='green'>Best</font> Suits Your Need?</h2>
                <div style="text-align:center">
                  <span><a id="fig1" style="font-size: 1.5em;"></a></span>
                </div>
                <p class="text-align: center;">
                  <img src="project_cover.jpeg" style="width: 90%; display: block; margin: 10px auto 20px;"/>
                </p>
                
                <h3 id="motivation">Motivation</h3>
                <p>
                  The idea behind this project is to analyze the patient review of different drugs (obtained from from Drugs.com) and use the results to design a <a href="https://optimed-stage.herokuapp.com">web application</a> to help the patient find the best drugs according to their symptoms.
                </p>
                <p>
                  I like to break down the idea that I have into a few parts to make it more clear and enhances the thought process for potential development of each part:
                </p>
                <ul>
                  <li>
                    <p>
                      I spent some time and looked at the different data sources that I knew such as <a href="https://www.reddit.com/r/dataisbeautiful"><strong>Data is beautiful</strong></a>, <a href="https://data.world/"><strong>data.world</strong></a>, <a href="https://tinyletter.com/data-is-plural/letters/data-is-plural-2020-06-17-edition"><strong>Data is plural</strong></a>, <a href="https://www.kaggle.com/datasets"><strong>kaggle</strong></a>, <a href="https://archive.ics.uci.edu/ml/datasets.php"><strong>UCI datasets</strong></a> and so on and found a <a href="https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29"><strong>Drug Review Dataset (Drugs.com)</strong></a> about cutomer reviews of 3671 unique drugs.
                    </p>
                  </li>
                  <li>I thought there are probably many people like me neither know nor look for the drug that best fits their symptoms and just go with that one drug that they always relied on when they had a headache ... So I thought about the ways that this data set can be used to find the best hit from the data set given:</li>
                  <li>the symptoms</li>
                  <li>location (living close to CVS, walgreens etc.)</li>
                  <li>budget</li>
                  <li>potential allergies or intolerance</li>
                  <li>other drugs being taken currently (if any) that can cause drug interactions</li>
                  <li>urgency of the situation (might influence the importance of the location)</li>
                  <li>...</li>
                </ul>
                <p>of the user.</p>
                <p>
                  My <strong>Initial Goal</strong> was to
                  <ul>
                    <li>Build a natural language processing model to recommend the right medicine according to the user condition</li>
                  </ul>
                  with a <strong>Final Goal</strong> being
                  <ul>
                    <li>Use other resources such as <strong><a href="https://www.drugbank.ca/">Drug Bank</a></strong> to make the model more robust by considering potential <strong><em>drug interactions, side effects</em></strong> etc.</li>
                    <li>Use other APIs such as <strong><a href="https://www.goodrx.com/">GoodRx</a></strong> to build a <strong>real-time model</strong> that considers other user-defined factors such as <strong><em>location, price range, and ease of transportation, weather condition, delivery options</em></strong> etc. to make a more calculated decision</li>
                    <li>Expand the usefulness of the app to <strong><em>medical professionals</em></strong> so that they can offer <strong><em>patients-customized solutions</em></strong> </strong></li>
                  </ul>
                  </p>
                <h3 id="loading-libraries">Loading the required libraries</h3>
                <pre class="prettyprint lang-language"><code class="language-python">import os
import re
import sys
import nltk
import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import chart_studio.plotly as py 
import plotly.graph_objs as go #importing graphical objects
from matplotlib import cm as cm
from IPython.display import Image
from matplotlib import rc, rcParams
from IPython.core.display import HTML
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
from viz import *
from toggle_cell import toggle_code as hide_cell
from matplotlib.colors import Normalize
from scipy import stats
from scipy.stats import norm
warnings.filterwarnings('ignore')
rcParams['font.family'] = 'serif'
rc('font',**{'family':'serif','serif':['Helvetica']})
rc('text', usetex=False)
# rc('text.latex', preamble=r'\usepackage{underscore}')
np.set_printoptions(precision=3)
pd.set_option('display.float_format', lambda x: '%.3f' % x)
sns.set_style('white')
sns.set(rc={"figure.dpi":100})</code></pre>
                <h2 id="data-exploration">Data Exploration and Preprocessing</h2>
                <p>The data was obtained from <a href="https://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29#">UCI Drug Review Dataset (Drugs.com)</a>. The following is the data description provided by the reference:</p>
                <h3>Data Set Information</h3>
                <p>
                  The dataset provides patient reviews on specific drugs along with related conditions and a 10 star patient rating reflecting overall patient satisfaction. The data was obtained by crawling online pharmaceutical review sites. The intention was to study
                </p>
                <ol>
                  <li>
                    Sentiment analysis of drug experience over multiple facets, i.e. sentiments learned on specific aspects such as effectiveness and side effects,
                  </li>
                  <li>
                    the transferability of models among domains, i.e. conditions, and
                  </li>
                  <li>
                    the transferability of models among different data sources (see <a href='https://archive.ics.uci.edu/ml/machine-learning-databases/00461/'>UCI Drug Review Dataset (Druglib.com)</a>).
                  </li>
                </ol>
                <p>
                  The data is split into a train (75%) a test (25%) partition (see publication) and stored in two .tsv (tab-separated-values) files, respectively.
                </p>
                <h4>Data Usage Policy</h4>
                <p>When using this dataset, you agree that you</p>
                <ol>
                  <li>only use the data for research purposes</li>
                  <li>don't use the data for any commerical purposes</li>
                  <li>don't distribute the data to anyone else</li>
                </ol>
                <h4>Attribute Information</h4>
                <ol style="list-style-type: decimal">
                  <li><strong>uniqueID</strong>: the ID unique to each patient that has written the review</li>
                  <li><strong>drugName</strong> (categorical): name of the drug</li>
                  <li><strong>condition</strong> (categorical): condition that the drug was used to address</li>
                  <li><strong>review</strong> (text): patient review</li>
                  <li><strong>rating</strong> (numerical 0-10)</li>
                  <li><strong>date</strong> (date): the date review was posted</li>
                  <li><strong>usefulCount</strong> (numerical): number of users that found the review useful</li>
                </ol>
                <pre class="prettyprint lang-language"><code class="language-python">df_train = pd.read_csv('data/drugsComTrain_raw.csv', parse_dates=['date'])
df_test = pd.read_csv('data/drugsComTest_raw.csv', parse_dates=['date'])
df_train.head()</code></pre>
                <table class="table table-striped">
                  <thead>
                    <tr style="text-align: left;">
                      <th></th>
                      <th>uniqueID</th>
                      <th>drugName</th>
                      <th>condition</th>
                      <th>review</th>
                      <th>rating</th>
                      <th>date</th>
                      <th>usefulCount</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <th scope="row">0</th>
                      <td>206461</td>
                      <td>Valsartan</td>
                      <td>Left Ventricular Dysfunction</td>
                      <td>"It has no side effect, I take it in combinati...</td>
                      <td>9</td>
                      <td>2012-05-20</td>
                      <td>27</td>
                    </tr>
                    <tr>
                      <th scope="row">1</th>
                      <td>95260</td>
                      <td>Guanfacine</td>
                      <td>ADHD</td>
                      <td>"My son is halfway through his fourth week of ...</td>
                      <td>8</td>
                      <td>2010-04-27</td>
                      <td>192</td>
                    </tr>
                    <tr>
                      <th scope="row">2</th>
                      <td>92703</td>
                      <td>Lybrel</td>
                      <td>Birth Control</td>
                      <td>"I used to take another oral contraceptive, wh...</td>
                      <td>5</td>
                      <td>2009-12-14</td>
                      <td>17</td>
                    </tr>
                    <tr>
                      <th scope="row">3</th>
                      <td>138000</td>
                      <td>Ortho Evra</td>
                      <td>Birth Control</td>
                      <td>"This is my first time using any form of birth...</td>
                      <td>8</td>
                      <td>2015-11-03</td>
                      <td>10</td>
                    </tr>
                    <tr>
                      <th scope="row">4</th>
                      <td>35696</td>
                      <td>Buprenorphine / naloxone</td>
                      <td>Opiate Dependence</td>
                      <td>"Suboxone has completely turned my life around...</td>
                      <td>9</td>
                      <td>2016-11-27</td>
                      <td>37</td>
                    </tr>
                  </tbody>
                </table>
                <h4>Basic Statistics of the Data</h4>
                <pre class="prettyprint lang-language"><code class="language-python">pd.concat([df_train.describe(),df_test.describe()], axis=1)</code></pre>
                <table class="table table-striped">
                  <thead>
                    <tr style="text-align: left;">
                      <th></th>
                      <th>uniqueID</th>
                      <th>rating</th>
                      <th>usefulCount</th>
                      <th>uniqueID</th>
                      <th>rating</th>
                      <th>usefulCount</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <th>count</th>
                      <td>161297.000</td>
                      <td>161297.000</td>
                      <td>161297.000</td>
                      <td>53766.000</td>
                      <td>53766.000</td>
                      <td>53766.000</td>
                    </tr>
                    <tr>
                      <th>mean</th>
                      <td>115923.585</td>
                      <td>6.994</td>
                      <td>28.005</td>
                      <td>116386.701</td>
                      <td>6.977</td>
                      <td>27.990</td>
                    </tr>
                    <tr>
                      <th>std</th>
                      <td>67004.445</td>
                      <td>3.272</td>
                      <td>36.404</td>
                      <td>67017.740</td>
                      <td>3.285</td>
                      <td>36.173</td>
                    </tr>
                    <tr>
                      <th>min</th>
                      <td>2.000</td>
                      <td>1.000</td>
                      <td>0.000</td>
                      <td>0.000</td>
                      <td>1.000</td>
                      <td>0.000</td>
                    </tr>
                    <tr>
                      <th scope="row">25%</th>
                      <td>58063.000</td>
                      <td>5.000</td>
                      <td>6.000</td>
                      <td>58272.500</td>
                      <td>4.000</td>
                      <td>6.000</td>
                    </tr>
                    <tr>
                      <th scope="row">50%</th>
                      <td>115744.000</td>
                      <td>8.000</td>
                      <td>16.000</td>
                      <td>116248.500</td>
                      <td>8.000</td>
                      <td>16.000</td>
                    </tr>
                    <tr>
                      <th scope="row">75%</th>
                      <td>173776.000</td>
                      <td>10.000</td>
                      <td>36.000</td>
                      <td>174586.750</td>
                      <td>10.000</td>
                      <td>36.000</td>
                    </tr>
                    <tr>
                      <th>max</th>
                      <td>232291.000</td>
                      <td>10.000</td>
                      <td>1291.000</td>
                      <td>232284.000</td>
                      <td>10.000</td>
                      <td>949.000</td>
                    </tr>
                  </tbody>
                </table>
                <pre class="prettyprint lang-language"><code class="language-python">print(f'train and test shapes are {df_train.shape} and {df_test.shape}, respectively.')
</code></pre>
                <pre><code>train and test shapes are (161297, 7) and (53766, 7), respectively.</code></pre>
                <h2>Data Exploration and Preprocessing</h2>
                <p>
                  We can start by looking at the reviews. 
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">df_train['review'].tail()</code></pre>
<pre>161292    "I wrote my first report in Mid-October of 201...
161293    "I was given this in IV before surgey. I immed...
161294    "Limited improvement after 4 months, developed...
161295    "I&#38;&#35;039&#59;ve been on thyroid medication 49 years...
161296    "I&#38;&#35;039&#59;ve had chronic constipation all my adu...
Name: review, dtype: object</pre>
                <p>
                  Patient reviews include some unwanted html characters which can be removed using <code class='module'>'html.parser'</code> module of <code class="library"><a href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a></code>.
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">from bs4 import BeautifulSoup

def remove_html(raw_review):
# 1. Remove HTML characters
    return BeautifulSoup(raw_review, 'html.parser').get_text()
df_test['review'].apply(remove_html)
df_train['review'].apply(remove_html).tail()</code></pre>
                <pre><code>161292    "I wrote my first report in Mid-October of 201...
161293    "I was given this in IV before surgey. I immed...
161294    "Limited improvement after 4 months, developed...
161295    "I've been on thyroid medication 49 years, I s...
161296    "I've had chronic constipation all my adult li...
Name: review, Length: 161297, dtype: object</code></pre>
                <p>
                  The same issue exists for the <code>condition</code> column so we repeat the cleaning for this column as well. However, we first need to change the data-type of this column from <code>'O'</code> (object) to <code>string</code>.
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">df_test['condition']=df_test['condition'].astype(str).apply(remove_html)
df_train['condition']=df_train['condition'].astype(str).apply(remove_html)</code></pre>
                <h3 id="data-imputation">Data Imputation</h3>
                <p>
                  We will examine the column to identify the missing values
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">for col in df_train.columns:
    if df_train[col].isnull().values.any():
        print('train data has {0:d} missing values in column "{1:s}" ({2:.3f}%)'
              .format(len(df_train[df_train[col].isna()]),
                      col, 
                      len(df_train[df_train[col].isna()]) / len(df_train)))
    if df_test[col].isnull().values.any():
        print('test data has {0:d} missing values in column "{1:s}" ({2:.3f}%)'
              .format(len(df_test[df_test[col].isna()]),
                      col, 
                      len(df_test[df_test[col].isna()]) / len(df_test)))</code></pre>
                <pre><code>train data has 899 missing values in column "condition" (0.006%)
test data has 295 missing values in column "condition" (0.005%)</code></pre>
                <p>
                  We drop the missing elements as the number of missing values are not significant relative to the entire population. 
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">df_train = df_train.dropna(axis=0)
df_test = df_test.dropna(axis=0)</code></pre>
                <p>
                  Next, we'll make some <em>observations</em> regarding the relatonships between the features using Exploratory Data Analysis. 
                </p>
                <h3 id="EDA">Exploratory Data Analysis</h3>
                <p>
                  We can take a look at the unique values for <code>uniqueID</code>, <code>drugName</code>, and <code>condition</code> to:
                <ul>
                  <li>See if there are <strong>multiple reviews</strong> from a single user,</li>
                  <li>Get a sense of the number of <strong>unique drugs</strong> in the data set,</li>
                  <li>Find the drugs that are most commonly used,</li>
                  <li>...</li>
                </ul>
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">df_all = df_train.append(df_test, ignore_index=True)
for col in ['uniqueID', 'drugName', 'condition']:
    print('{0:d} unique values for {1:s} in train data of length {2:d}' \
        .format(len(df_train[col].unique().tolist()), col, len(df_train)))
    print('{0:d} unique values for {1:s} in test data of length {2:d}' \
        .format(len(df_test[col].unique().tolist()), col, len(df_test)))
    print('{0:d} unique values for {1:s} in the entire data of length {2:d}' \
        .format(len(df_all[col].unique().tolist()), col, len(df_all)))</code></pre>
                <pre><code>161297 unique values for uniqueID in train data of length 161297
53766 unique values for uniqueID in test data of length 53766
215063 unique values for uniqueID in the entire data of length 215063
3436 unique values for drugName in train data of length 161297
2637 unique values for drugName in test data of length 53766
3671 unique values for drugName in the entire data of length 215063
885 unique values for condition in train data of length 161297
709 unique values for condition in test data of length 53766
917 unique values for condition in the entire data of length 215063</code></pre>
                <p>
                  Note that the total number of <code>uniqueID</code>s is equal to the sum of <code>uniqueID</code>s in the train and test data, meaning that each patient has written one review. We can now make some <em>observations</em> regarding some of the features of the dataset.
                </p>
                <h4>User rating histogram for train and test data</h4>
                <pre class="prettyprint lang-language"><code class="language-python">sns.set(style="white", palette="muted")
f, axes = plt.subplots(1, 2, figsize=(14, 7), sharex=False, sharey=False)
sns.despine(left=False)
colors = ['blue', 'orange']
data_labels = ['train', 'test']
plots, handles = [], []
for i, data in enumerate([df_train, df_test]):
    plots.append(sns.distplot(data['rating'],
                              kde=False, 
                              color=colors[i], 
                              ax=axes[i], 
                              label=data_labels[i])
                )
    plots[i].set_xticks(np.arange(1,11))
    h, l = plots[i].get_legend_handles_labels()
    handles.append(h[0])
plt.legend(labels=data_labels, 
           handles=handles, 
           bbox_to_anchor=(1,1,0,0), 
           loc='center left', 
           fontsize=12)
plt.suptitle('Patient review-rating distributions for train and test data', fontsize=20)
plt.tight_layout()
plt.show()</code></pre>
                <div style="text-align: center;">
                  <a id="fig2" style="font-size: 1.5em;">Figure 2</a>
                </div>
                <p class="text-align; center;">
                  <img src="output_23_0.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">sns.set(style="white", palette="muted")
f, ax = plt.subplots(1, 1, figsize=(7, 7))
sns.despine(left=False)
df_all['year'] = df_all['date'].dt.year
df_all['month'] = df_all['date'].dt.month
years=sorted(df_all['year'].unique())
g = sns.distplot(df_all['year'],bins=len(years),
                 kde=False, 
                 ax=ax)
# fix the ticks and labels
for i,rect in enumerate(g.patches):
    rect.set_width(1)
    rect.set_x(years[i]-0.5)
ax.set_xticks(list(np.asarray(years)))
plt.suptitle('Number of reviews per year', fontsize=20)
plt.tight_layout()
plt.show()</code></pre>
                <div style="text-align:center">
                <a id="fig3" style="font-size: 1.5em;">Figure 3</a>
                </div>
                <p class="text-align; center;">
                  <img src="output_24_0.png" style="width: 60%; display: block; margin: 10px auto 20px;"/>
                </p>
                
                <h4>Variation of review count over the years</h4>
                <pre class="prettyprint lang-language"><code class="language-python">sns.set(style="white", palette="muted")
f, ax = plt.subplots(1, 1, figsize=(7, 7))
sns.despine(left=False)
month_names = [month_name[x] for x in np.arange(1,13)];
df_all['month'] = df_all['date'].dt.month
months=sorted(df_all['month'].unique())
g = sns.distplot(df_all['month'],bins=len(months),
                 kde=False, 
                 ax=ax)
# fix the ticks and ticklabels
for i,rect in enumerate(g.patches):
    rect.set_width(1)
    rect.set_x(months[i]-0.5)
g.set_xticks(months)
g.set_xticklabels(month_names, rotation=90)
plt.suptitle('Number of reviews for each month', fontsize=20)
plt.tight_layout()
plt.show()</code></pre>
                <div style="text-align:center">
                <a id="fig4" style="font-size: 1.5em;">Figure 4</a>
                </div>
                <p class="text-align; center;">
                  <img src="output_25_0.png" style="width: 60%; display: block; margin: 10px auto 20px;"/>
                </p>
                <h4>Mean monthly and annual ratings</h4>
                <pre class="prettyprint lang-language"><code class="language-python">f, ax = plt.subplots(2, 1, figsize=(14, 14), sharex=False, dpi=200)
g_y = sns.boxplot(x='year', y='rating',data=df_all, ax=ax[0], palette="Paired", showmeans=True)
g_m = sns.boxplot(x='month', y='rating',data=df_all, ax=ax[1], palette="Paired", showmeans=True)
g_m.set_xticklabels(month_names);
plt.suptitle('Average user rating for each month and year', fontsize=22, y=0.92);</code></pre>
                <div style="text-align:center">
                  <a id="fig5" style="font-size: 1.5em;">Figure 5</a>
                </div>
                <p class="text-align; center;">
                  <img src="output_26_0.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
                </p>
                <p>Hmmm... Drugs getting worse or user becoming more critical?</p>
                <h4>Drugs that are prescribed for many conditions</h4>
                <p>
                  Another interesting observation is to look at the numebr of drugs that are used to address mutiple different medical conditions and see how the user ratings change for each condition. In other words, if a drug is known to be good for addressing condition 'A' does that mean it will do a good job in adressing condition 'B'?
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">n_drugs = 20
drugs_w_most_cond = df_all.groupby(['drugName'])['condition'] \
      .nunique() \
      .sort_values(ascending=False)[:n_drugs]
print(drugs_w_most_cond)</code></pre>
                <pre>drugName
Prednisone        39
Gabapentin        31
Ciprofloxacin     25
Doxycycline       25
Amitriptyline     24
Neurontin         23
Metronidazole     23
Venlafaxine       23
Lyrica            22
Dexamethasone     22
Zoloft            21
Cymbalta          20
Triamcinolone     20
Azithromycin      20
Clarithromycin    19
Seroquel          19
Effexor XR        19
Cipro             18
Sertraline        18
Naproxen          18
Name: condition, dtype: int64</pre>
                <pre class="prettyprint lang-language"><code class="language-python">fig, ax = plt.subplots(1, 1, figsize=(16, 6), dpi=200)
drug_arr = []
for i, (drug, count) in enumerate(drugs_w_most_cond.items()):
    drug_arr += count*[drug]
    tmp_df = pd.DataFrame(drug_arr, columns=['drug'])
g = sns.countplot(x='drug', data=tmp_df, ax=ax)
g.set_xlabel(g.get_xlabel(), fontsize=22)
g.set_ylabel(g.get_ylabel(), fontsize=22, labelpad=10)
ylabs = g.get_yticklabels()
plt.setp(ax.get_xticklabels(), rotation=45, ha='right', fontsize=16)
plt.setp(ax.get_yticklabels(), fontsize=16)
g.set_title('Drugs addressing the most unmber of unique medical conditions',y=1.05, fontsize=28)
sns.despine(left=False)</pre></code>
                <div style="text-align:center">
                <a id="fig6" style="font-size: 1.5em;">Figure 6</a>
                </div>
                <p class="text-align; center;">
                  <img src="output_27_0.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
                </p>
                  <h4>Conditions that are treated using multiple different drugs</h4>
                <p>
                  Similar to previous analysis, we can take a look at the conditions that are treated using variety of drugs.
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">n_conditions = 20
conditions_w_most_drugs = df_all.groupby(['condition'])['drugName'] \
                          .nunique() \
                          .sort_values(ascending=False)[:n_conditions]
print(conditions_w_most_drugs)</code></pre>
                <pre>condition
Not Listed / Othe                      253
Pain                                   219
Birth Control                          181
High Blood Pressure                    146
Acne                                   127
Depression                             115
Rheumatoid Arthritis                   107
Diabetes, Type 2                        97
Allergic Rhinitis                       95
Insomnia                                85
Osteoarthritis                          84
Bipolar Disorde                         82
Anxiety                                 81
Abnormal Uterine Bleeding               77
Endometriosis                           64
3 users found this comment helpful.     62
Psoriasis                               61
Migraine                                60
ADHD                                    58
4 users found this comment helpful.     57
Name: drugName, dtype: int64
                </pre>
                <p>
                  Note that there are some conditions that we need to remove, i.e. <code>Not Listed / Othe </code>, <code>3 users found this comment helpful</code> , and <code>4 users found this comment helpful</code>. We can define a pattern using <strong>regular expressions</strong> to find the problematic rows as follows.
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">df_all[df_all.condition.str.contains('(^nan$|Not Listed|found this comment helpful)')].head()</code></pre>
                <table class="table table-striped">
                  <thead>
                    <tr style="text-align: left;">
                      <th></th>
                      <th>uniqueID</th>
                      <th>drugName</th>
                      <th>condition</th>
                      <th>review</th>
                      <th>rating</th>
                      <th>date</th>
                      <th>usefulCount</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <th>104</th>
                      <td>220696</td>
                      <td>Loestrin 24 Fe</td>
                      <td>2 users found this comment helpful.</td>
                      <td>"I'm 16 and  I have been on Loestrin 24 f...</td>
                      <td>3</td>
                      <td>2010-11-03</td>
                      <td>2</td>
                    </tr>
                    <tr>
                      <th>194</th>
                      <td>67383</td>
                      <td>Provera</td>
                      <td>4 users found this comment helpful.</td>
                      <td>"I'm 24 years old and have always had a p...</td>
                      <td>1</td>
                      <td>2016-03-27</td>
                      <td>4</td>
                    </tr>
                    <tr>
                      <th>241</th>
                      <td>81588</td>
                      <td>Yaz</td>
                      <td>3 users found this comment helpful.</td>
                      <td>"I took Yaz for a little over 2 years.  From a...</td>
                      <td>3</td>
                      <td>2010-06-01</td>
                      <td>3</td>
                    </tr>
                    <tr>
                      <th>262</th>
                      <td>132965</td>
                      <td>Loestrin 24 Fe</td>
                      <td>4 users found this comment helpful.</td>
                      <td>"Took this pill for 1.) Acne and 2.) Birth Con...</td>
                      <td>2</td>
                      <td>2014-06-24</td>
                      <td>4</td>
                    </tr>
                    <tr>
                      <th>389</th>
                      <td>91050</td>
                      <td>Norco</td>
                      <td>11 users found this comment helpful.</td>
                      <td>"I have suffered with low back pain - 2 surger...</td>
                      <td>9</td>
                      <td>2009-03-15</td>
                      <td>11</td>
                    </tr>
                  </tbody>
                </table>
                <p>
                  We drop the rows corresponding to the mis-parsed conditions and then plot the data.
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">bad_indices = df_all[df_all.condition.str.contains('(^nan$|Not Listed|found this comment helpful)')].index
df_all.drop(bad_indices, inplace=True)
df_all.reset_index(drop=True)
conditions_w_most_drugs = df_all.groupby(['condition'])['drugName'] \
                          .nunique() \
                          .sort_values(ascending=False)[1:n_conditions+1]

fig, ax = plt.subplots(1, 1, figsize=(16, 6), dpi=200)
condition_arr = []
for i, (condition, count) in enumerate(conditions_w_most_drugs.items()):
    condition_arr += count*[condition]
    tmp_df = pd.DataFrame(condition_arr, columns=['condition'])
g = sns.countplot(x='condition', data=tmp_df, ax=ax)
g.set_xlabel(g.get_xlabel(), fontsize=22)
g.set_ylabel(g.get_ylabel(), fontsize=22, labelpad=10)
ylabs = g.get_yticklabels()
plt.setp(ax.get_xticklabels(), rotation=45, ha='right', fontsize=16)
plt.setp(ax.get_yticklabels(), fontsize=16)
g.set_title('Medical conditions with the most number of unique drugs addressing them',y=1.05, fontsize=28)
sns.despine(left=False)</pre></code>
                <div style="text-align:center">
                <a id="fig7" style="font-size: 1.5em;">Figure 7</a>
                </div>
                <p class="text-align; center;">
                  <img src="output_28_0.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
                </p>
                <h4>Variation of <code>usefulCount</code> with <code>rating</code></h4>
                <p>
                  One of the questions that I had was whether the positiveness/negativeness of the rating influence the number of peole that find the review useful?
                <pre class="prettyprint lang-language"><code class="language-python">f, ax = plt.subplots(1, 1, figsize=(14, 6), dpi=200)
g = sns.boxplot(y='usefulCount', x='rating',data=df_all, 
                ax=ax, palette="Paired", showmeans=True, 
                showfliers=False)
g.set_title('The distribution of usefulCount for different drug ratings.'+
            '\n(outliers are not shown)', fontsize=24, y=1.02);</code></pre>
                <div style="text-align:center">
                  <a id="fig8" style="font-size: 1.5em;">Figure 8</a>
                </div>
                <p class="text-align; center;">
                  <img src="output_29_0.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
                </p>
                <p>
                  I find this result really interesting. What we can infer from it is that <strong>a positive review is more likely to be found useful than a negative review by others who read the review</strong>. 
                </p>
                <p>
                  As we observed in <a href="#fig2">Figure 2</a>, the highly-rated reviews are more frequent compared to the poorly-rated ones. We need to <strong>define a metric</strong> for a <strong><font color='green'>positive</font></strong> vs. <strong><font color='red'>negative</font></strong> to be able to further inspect the quality of the reviews. Gouping the reviews into positive and negative categories can shed light on the correlation of the review ratings with more than just one other feature.
                  For now, we classify the reviews with rating below 5.5 as <strong><font color='red'>negative</font></strong> and otherwise <strong><font color='green'>positive</font></strong>.
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">df_all['rating_binary'] = df_all['rating'].apply(lambda x: 'positive' if x>5.5 else 'negative')
print("The dataset includes {0:d} positive and {1:d} negative reviews" \
      .format(len(df_all[df_all["rating_binary"]=="positive"]), \
              len(df_all[df_all["rating_binary"]=="negative"])))</code></pre>
                <pre>The dataset includes 148682 positive and 63424 negative reviews</pre>
                <pre class="prettyprint lang-language"><code class="language-python">df_all.head()</code></pre>
                <table class="table table-striped">
                  <thead>
                    <tr style="text-align: right;">
                      <th></th>
                      <th>uniqueID</th>
                      <th>drugName</th>
                      <th>condition</th>
                      <th>review</th>
                      <th>rating</th>
                      <th>date</th>
                      <th>usefulCount</th>
                      <th>year</th>
                      <th>month</th>
                      <th>rating_binary</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <th score="row">0</th>
                      <td>206461</td>
                      <td>Valsartan</td>
                      <td>Left Ventricular Dysfunction</td>
                      <td>"It has no side effect, I take it in combinati...</td>
                      <td>9</td>
                      <td>2012-05-20</td>
                      <td>27</td>
                      <td>2012</td>
                      <td>5</td>
                      <td>positive</td>
                    </tr>
                    <tr>
                      <th score="row">1</th>
                      <td>95260</td>
                      <td>Guanfacine</td>
                      <td>ADHD</td>
                      <td>"My son is halfway through his fourth week of ...</td>
                      <td>8</td>
                      <td>2010-04-27</td>
                      <td>192</td>
                      <td>2010</td>
                      <td>4</td>
                      <td>positive</td>
                    </tr>
                    <tr>
                      <th score="row">2</th>
                      <td>92703</td>
                      <td>Lybrel</td>
                      <td>Birth Control</td>
                      <td>"I used to take another oral contraceptive, wh...</td>
                      <td>5</td>
                      <td>2009-12-14</td>
                      <td>17</td>
                      <td>2009</td>
                      <td>12</td>
                      <td>negative</td>
                    </tr>
                    <tr>
                      <th score="row">3</th>
                      <td>138000</td>
                      <td>Ortho Evra</td>
                      <td>Birth Control</td>
                      <td>"This is my first time using any form of birth...</td>
                      <td>8</td>
                      <td>2015-11-03</td>
                      <td>10</td>
                      <td>2015</td>
                      <td>11</td>
                      <td>positive</td>
                    </tr>
                    <tr>
                      <th score="row">4</th>
                      <td>35696</td>
                      <td>Buprenorphine / naloxone</td>
                      <td>Opiate Dependence</td>
                      <td>"Suboxone has completely turned my life around...</td>
                      <td>9</td>
                      <td>2016-11-27</td>
                      <td>37</td>
                      <td>2016</td>
                      <td>11</td>
                      <td>positive</td>
                    </tr>
                  </tbody>
                </table>
                <p>
                  I use <a href="https://seaborn.pydata.org/index.html"><code>Seaborn</code></a>'s <a href="https://seaborn.pydata.org/generated/seaborn.boxenplot.html"><code>boxenplot()</code></a> (or <a href="https://vita.had.co.nz/papers/letter-value-plot.html">Letter-value plot</a>) for visualization. It can be thought of as an extension of boxplot that is more convenient for large data as it offers more precise estimates of quantiles beyond the quartiles which might be important but missed when using boxplot. Furthermore, I include two plots for this part where the first plot (<a href="#fig9">Figure 9</a>) shows the data where the top and bottom 0.01% of the population are removed and the second plot (<a href="#fig10">Figure 10</a>) shows the full data.
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">f, ax = plt.subplots(1, 1, figsize=(14, 14), dpi=200)
sns.set_style('dark')
bar_width = 0.8
q_low = df_all["usefulCount"].quantile(0.01)
q_hi  = df_all["usefulCount"].quantile(0.99)
df_outliers_removed = df_all[(df_all["usefulCount"] < q_hi) &#038; (df_all["usefulCount"] > q_low)]
means_positive = df_all[df_all['rating_binary']=='positive'].groupby('year')['usefulCount'].mean()
means_negative = df_all[df_all['rating_binary']=='negative'].groupby('year')['usefulCount'].mean()
means = np.hstack([means_positive,means_negative])
g = sns.boxenplot(x="year", y="usefulCount", hue="rating_binary",width=bar_width,
                  palette="pastel", data=df_outliers_removed, ax=ax)
g.set_xlabel(g.get_xlabel(), fontsize=18)
g.set_ylabel(g.get_ylabel(), fontsize=18)
for i,mu in enumerate(means):
    shift = bar_width/4*pow(-1,i+1); # shift left for positive, right for negative
    g.text(i//2+shift, mu, f"{np.round(mu,1)}", 
           fontdict={'color':'black', 'fontsize':10}, rotation=0, ha='center', va='center',
           bbox=dict(facecolor='white', alpha=0.4, edgecolor=None))
plt.legend(bbox_to_anchor=(0.5, 1.05), loc='center', borderaxespad=0., title='rating binary')
g.set_title('Distribution of usefulCount for positive and negative reviews in each year.\n'+
            '(outliers removed)', fontsize=24, y=1.1);</code></pre>
                <div style="text-align:center">
                <a id="fig9" style="font-size: 1.5em;">Figure 9</a>
                </div>
                <p class="text-align; center;">
                  <img src="output_31_0.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">f, ax = plt.subplots(1, 1, figsize=(14, 28), dpi=200)
sns.set_style('dark')
bar_width = 0.8
means_positive = df_all[df_all['rating_binary']=='positive'].groupby('year')['usefulCount'].mean()
means_negative = df_all[df_all['rating_binary']=='negative'].groupby('year')['usefulCount'].mean()
means = np.hstack([means_positive,means_negative])
g = sns.boxenplot(x="year", y="usefulCount", hue="rating_binary",width=bar_width,
                  palette="pastel", data=df_all, ax=ax)
g.set_xlabel(g.get_xlabel(), fontsize=18)
g.set_ylabel(g.get_ylabel(), fontsize=18)
for i,mu in enumerate(means):
    shift = bar_width/4*pow(-1,i+1); # shift left for positive, right for negative
    g.text(i//2+shift, mu, f"{np.round(mu,1)}", 
           fontdict={'color':'black', 'fontsize':10}, rotation=0, ha='center', va='center',
           bbox=dict(facecolor='white', alpha=0.4, edgecolor=None))
plt.legend(bbox_to_anchor=(0.5, 1.02), loc='center', borderaxespad=0., title='rating binary')
plt.setp(ax.get_xticklabels(), fontsize=14)
plt.setp(ax.get_yticklabels(), fontsize=14)
g.set_title('Distribution of usefulCount for positive and negative reviews in each year', fontsize=24, y=1.04);</code></pre>
                <div style="text-align:center">
                  <a id="fig10" style="font-size: 1.5em;">Figure 10</a>
                </div>
                <p class="text-align; center;">
                  <img src="output_35_0.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
                </p>
                <p>
                  There are some interesting detils tht are revealed using Figures <a href="#fig9">9</a> and <a href="#fig10">10</a>.
                </p>
                <ul>
                  <li>
                    In support of the point brought up previously, that <em>positive reviews are more likely to be found useful</em>, we can see that the mean <code>usefulCount</code> for positive reviews is higher than that of the negative reviews in 8 out of 10 years (every year except 2010 and 2015).
                  </li>
                  <li>
                    2012 and 2014 are the only years where the mean of <code>usefulCount</code> is lower than its median for both positive and negative reviews (same for negative reviews of 2008). What this tells us is that there are 
                  </li>
                  <li>
                    The median of <code>usefulCount</code> for positive reviews is higher than that of the negative reviews for all years.
                  </li>
                  <li>
                    Note that hwo excluding the reviews with very high <code>usefulCount</code> can influence our interpretation. In this specific project, the reviews with high <code>usefulCount</code> can't and shouldn't be excluded because they are simply <strong>not outliers</strong>.
                  </li>
                  <li>
                    The reason why almost all the outliers (denoted by the diamond symbol) in both figures <a href="#fig9">9</a> and <a href="#fig10">10</a> correspond to the observations with high <code>usefulCount</code> is that there are a lot of reviews where <code>usefulCount=0</code> and in fact a quick investigation shows that the number of observations with <code>usefulCount=0</code> is more than 1% of all the reviews. This is also the reason behind the mean of the population dropping for all the situations (The <code>usefulCount</code> can't be negative!)
                  </li>
                <pre class="prettyprint lang-language"><code class="language-python">pct = abs((len(df_all[(df_all["usefulCount"] &gt; q_low)])-len(df_all))/len(df_all))*100
print(f"{pct:.1f}%")</code></pre>
<pre>3.8%</pre>     
                  <li>
                    Note that the number of reviews that were useful, be it positive or negative, declines with time which is expected because the older reviews are read more and there are not a lot of new drugs joining the market every year.
                  </li>
                </ul>
                <h2>Sentiment Analysis</h2>
                <p>
                  <strong>Sentiment analysis</strong> is the process of analyzing text/voice data using <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing (NLP)</a>, <a href="/wiki/Text_analytics" class="mw-redirect" title="Text analytics">text analysis</a>, and <a href="/wiki/Computational_linguistics" title="Computational linguistics">computational linguistics</a> to extract and determine the emotional tone, attitude, or opinion carried by the data. The use of sentiment analysis is common in areas such as healthcare, social media, and marketing to study surveys, reviews, user comments, customer text/voice etc.
                </p>
                <h3>Data preparation for sentiment analysis</h3>
                <h4>Stopwords</h4>
                <p>
                   <strong>stopwords</strong> are words that we do not want them taking up space in our database, or taking up valuable processing time. For this, we can remove them easily by storing a list of words that we consider to be stop words. Even though when it comes to <strong><em>sentiment analysis</em></strong> the popular opinion is to remove <strong>stopwords</strong>, removing them is <strong>not always the best idea, especially if the context of the phrases may influence the performance of the algorithm</strong>. 
                </p>
                <p>
                   For now we will remove the <strong>stopwords</strong> from the <code>review</code> column but we first <strong><em>make sure that the ones that carry an emotional meaning are not removed</em></strong> as they are important for sentiment analysis; For instance, there are many words (see below) with <strong>negative connotation</strong> that are required to be <strong>removed them from the stopwords</strong>:
                </p>
                <table class="table table-striped" style="margin: auto; width: fit-content;padding-bottom: 1em;">
                  <tbody>
                    <tr>
                      <td>aren't</td>
                      <td>haven't</td>
                      <td>not</td>
                    </tr>
                    <tr>
                      <td>couldn't</td>
                      <td>isn't</td>
                      <td>can't</td>
                    </tr>
                    <tr>
                      <td>didn't</td>
                      <td>mightn't</td>
                      <td>shan't</td>
                    </tr>
                    <tr>
                      <td>doesn't</td>
                      <td>mustn't</td>
                      <td>shouldn't</td>
                    </tr>
                    <tr>
                      <td>don't</td>
                      <td>needn't</td>
                      <td>wasn't</td>
                    </tr>
                    <tr>
                      <td>hadn't</td>
                      <td>no</td>
                      <td>weren't</td>
                    </tr>
                    <tr>
                      <td>hasn't</td>
                      <td>nor</td>
                      <td>wouldn't</td>
                    </tr>
                  </tbody>
                </table>
                <h4>Cleaning, tokenization, and stemming</h4>
                <p>
                  We already scanned the reviews to remove unwanted <code class="language">html</code> characters using the <code>'html.parser'</code> of <code class="library">BeautifulSoup</code>. However, there is some further cleaning necessary besides the removal of unwanted characters. We also need to <strong>tokenize</strong> the <strong>reviews</strong>, i.e. <em>break it up to pieces</em> such as <strong>words, keywords, phrases, symbols</strong> etc. before sttempting to train the model for <em>Sentiment Analysis</em>.
                </p>
                <p>
                  We will define a simple function for text-cleaning, tokenization, and removal of the important negative verbs from the list of <strong>stopwords</strong>. We also take advantage of <code class="library">nltk</code>'s <strong>stopwords</strong> library to make sure all the unwanted words are removed from the reviews. Lastly, we use <code class="library">nltk</code>'s <code class="method">SnowballStemmer</code> which is meant to <a href="https://www.nltk.org/howto/stem.html">remove morphological affixes from words, leaving only the word stem</a>. This can reduce the computation cost significantly.
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">from wordcloud import WordCloud, STOPWORDS
import nltk
from nltk import PorterStemmer as Porter_stemmer
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from bs4 import BeautifulSoup
nltk.download('stopwords')
Snowball_stemmer = SnowballStemmer('english', ignore_stopwords=True)
nltk_stopwords = stopwords.words('english')
stopWords = STOPWORDS.union(nltk_stopwords)
remove_from_stopWords = ["aren't", "couldn't", "didn't", "doesn't", "don't", "hadn't", "hasn't",
                         "haven't", "isn't", "mightn't", "mustn't", "needn't", "no", "nor", "not",
                         "can't", "shan't", "shouldn't", "wasn't", "weren't", "wouldn't"]

for i in remove_from_stopWords:
    stopWords.remove(i)

def clean_review(raw_review, stopWords=stopWords, stemmer=Snowball_stemmer):
    
    # 1. Substitute all non-letter characters with a space
    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)
    
    # 2. lower letters and remove spaces
    words = letters_only.lower().split()
    
    # 3. Stopwords 
    meaningfulWords = [w for w in words if not w in stopWords]
    
    # 4. Stemming
    stemming_words = [stemmer.stem(w) for w in meaningfulWords]
    # having
    # had     ======> have
    # have
    
    # 5. space join words and return
    return( ' '.join(stemming_words))</code></pre>
                <p>
                  We can run a quick sanity check!
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">print(df_train['review'][110])</code></pre>
                <pre>"Been dealing with restless leg syndrome.for about 2 years. It kept me from falling asleep. First they gave me flexiril. And it did nothing. Than a miracle came about and I was prescribed reprinol and my legs haven't twitched since . Amazing drug I must say. My sleep has improved greatly"</pre>
                <pre class="prettyprint lang-language"><code class="language-python">print(clean_review(df_train['review'][110]))</code></pre>
                <pre>deal restless leg syndrom year kept fall asleep first gave flexiril noth miracl came prescrib reprinol leg twitch amaz drug must say sleep improv great</pre>
                <p>
                  Note that not all the words are going to look meaningful since we see the stems.
                </p>
                <h4>WordCloud</h4>
                <p>
                  <strong>wordcloud</strong> is a nice way to visually represent of words. I use python's <code class="library"><a href="https://amueller.github.io/word_cloud/">WordCloud</a></code> library for this purpose. We can take a look at most frequent words in positive and negative reviews using a customized wordcloud function.
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">def cloud_of_words(text_df, mask=None, fig_size=(16, 5), collocations=True,
                   max_words=150, width=800, height=250, background_color="#006666",
                   stopWords=None, relative_scaling=0, title='word cloud', 
                   title_size=28, random_state=None, max_font_size=100, colormap=None):

    wordcloud = WordCloud(stopwords=stopWords, max_words=max_words, collocations=collocations,
                          max_font_size=max_font_size, random_state=random_state,
                          width=width, height=height, mask=mask, background_color=background_color,
                          colormap=colormap, relative_scaling=relative_scaling).generate(str(text_df))

    plt.figure(figsize=fig_size)
    plt.imshow(wordcloud)
    plt.title(title, fontsize=title_size, y=1.02)
    plt.axis('off')
    plt.tight_layout()</code></pre>
                <p>
                  We can now visualize the wordcloud of positive
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">cloud_of_words(df_all[df_all['rating_binary']=='positive']['review'], 
               stopWords=stopWords, 
               title='Frequent words in positive reviews', 
               colormap='magma', 
               background_color='steelblue')</code></pre>
               <div>
                 <a id="fig11" style="font-size: 1.5em;">Figure 11</a>
               </div>
               <p class="text-align; center;">
                 <img src="output_54_0.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
               </p>
                <p>
                  and negative reviews.
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">cloud_of_words(df_all[df_all['rating_binary']=='negative']['review'], 
               stopWords=stopWords, 
               title='Frequent words in negative reviews', 
               colormap='autumn')</code></pre>
               <div>
                 <a id="fig12" style="font-size: 1.5em;">Figure 12</a>
               </div>
               <p class="text-align; center;">
                 <img src="output_56_0.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
               </p>
               <h4>n-grams</h4>
                <p>
                   <strong>n-gram</strong> is a contiguous sequence of n items from a given sample of text or speech (<a href="https://en.wikipedia.org/wiki/N-gram">ref</a>). <strong>n-gram</strong>s are widely used in <strong>NLP</strong> for applications such as spelling correction, word breaking and text summarization. The following table shows the n-gram representation of <em>"This is a beautiful present!"</em> for n=1 (<em>unigram</em>), n=2 (<em>bigram</em>), n=3 (<em>trigram</em>), and n=4(<em>4</em>-gram). 
                </p>
                <table class="table table-striped" style="margin: auto; width: fit-content;padding-bottom: 1em;">
                  <thead>
                    <tr style="text-align: left;">
                      <th>unigram</th>
                      <th>bigram</th>
                      <th>trigram</th>
                      <th>4-gram</th> 
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>This</td>
                      <td>This is</td>
                      <td>This is a</td>
                      <td>This is a beautiful</td>
                    </tr>
                    <tr>
                      <td>is</td>
                      <td>is a</td>
                      <td>is a beautiful</td>
                      <td>is a beautiful present!</td>
                    </tr>
                    <tr>
                      <td>a</td>
                      <td>a beautiful</td>
                      <td>a beautiful present!</td>
                      <td></td>
                    </tr>
                    <tr>
                      <td>beautiful</td>
                      <td>beautiful present!</td>
                      <td></td>
                      <td></td>
                    </tr>
                    <tr>
                      <td>present!</td>
                      <td></td>
                      <td></td>
                      <td></td>
                    </tr>
                  </tbody>
                </table>
                <p>
                  Next, we define a simple function to generate <em>n-grams</em>. We will use this function later on in our training pipeline.
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">def gen_n_grams(sentence, n=4, stopWords=stopWords):
    token = [token for token in sentence.lower().split(
        ' ') if token != '' if token not in stopWords]
    n_grams = zip(*[token[i:] for i in range(n)])
    return [' '.join(gram) for gram in n_grams]

gen_n_grams('The beautiful moment that all of us have been waiting for has finally arrived!')</code></pre>
                <pre>['beautiful moment us waiting',
 'moment us waiting finally',
 'us waiting finally arrived!']</pre>
                <p>
                  Note that the stopwords are removed before the <em>n-grams</em> are generated.
                </p>
                <h5>Visualizing most frequent <em>n-grams</em></h5>

                  
                </h5>  
             </p></div></div></section>

        </div><!-- End .row -->

      </div><!-- End .container -->

    </section><!-- End Blog Section -->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer" data-aos="fade-up" data-aos-easing="ease-in-out" data-aos-duration="500">

    <div class="footer-newsletter">
      <div class="container">
        <div class="row">
          <div class="col-lg-6">
            <h4>Newsletter</h4>
            <p>Subscribe here to get notified when a new material is posted</p>
          </div>
          <div class="col-lg-6">
            <form action="" method="post">
              <input type="email" name="email"><input type="submit" value="Subscribe">
            </form>
          </div>
        </div>
      </div>
    </div>

    <div class="footer-top">
      <div class="container">
        <div class="row">

          <div class="col-lg-4 col-md-6 footer-links">
            <h4>Useful Links</h4>
            <ul>
              <li><i class="bx bx-chevron-right"></i> <a href="#">Home</a></li>
              <li><i class="bx bx-chevron-right"></i> <a href="#">About Me</a></li>
              <li><i class="bx bx-chevron-right"></i> <a href="#">Projects</a></li>
              <li><i class="bx bx-chevron-right"></i> <a href="#">Notes</a></li>
              <!-- <li><i class="bx bx-chevron-right"></i> <a href="#">Privacy policy</a></li> -->
            </ul>
          </div>
<!-- 
          <div class="col-lg-3 col-md-6 footer-links">
            <h4>Our Services</h4>
            <ul>
              <li><i class="bx bx-chevron-right"></i> <a href="#">Web Design</a></li>
              <li><i class="bx bx-chevron-right"></i> <a href="#">Web Development</a></li>
              <li><i class="bx bx-chevron-right"></i> <a href="#">Product Management</a></li>
              <li><i class="bx bx-chevron-right"></i> <a href="#">Marketing</a></li>
              <li><i class="bx bx-chevron-right"></i> <a href="#">Graphic Design</a></li>
            </ul>
          </div> -->

          <div class="col-lg-4 col-md-6 footer-contact">
            <h4>Contact</h4>
            <p>
              913 Boylston Street<br>
              Boston, MA 02115<br>
              United States <br><br>
              <strong>Phone:</strong> +1 617 460 0555<br>
              <strong>Email:</strong> alimehdirahim@gmail.com<br>
            </p>

          </div>

          <div class="col-lg-4 col-md-6 footer-info">
            <h3>About Me</h3>
            <p>A data science enthusiast! I love developing new machine learning algorithms to address real-world problems. I try to learn something new on a daily basis.</p>
            <div class="social-links mt-3">
              <a href="https://github.com/alineu" class="Github"><i class="bx bxl-github"></i></a>
              <a href="https://www.linkedin.com/in/alimehdizadehrahimi/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
              <a href="https://twitter.com/estoyali" class="twitter"><i class="bx bxl-twitter"></i></a>
              <a href="https://instagram.com/aliexplores" class="instagram"><i class="bx bxl-instagram"></i></a>
              </div>
          </div>
        </div>
      </div>
    </div>

    <div class="container">
      <div class="copyright">
        &copy; Copyright 2021 <strong><span>Ali Mehdizadeh</span></strong>. All Rights Reserved
      </div>
      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/free-bootstrap-template-corporate-moderna/ -->
        <p>
          Page design is adopted from <a href="https://bootstrapmade.com/">BootstrapMade</a>.<br>Page icons were taken from <a href="https://www.flaticon.com">flaticon.com</a>.
        </p>
      </div>
    </div>
  </footer><!-- End Footer -->
  <a href="#" class="back-to-top"><i class="icofont-simple-up"></i></a>
                                                                <!-- Vendor JS Files -->
  <script type="text/javascript" src="../../assets/vendor/jquery/jquery.min.js"></script>
  <script type="text/javascript" src="../../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script type="text/javascript" src="../../assets/vendor/jquery.easing/jquery.easing.min.js"></script>
  <script type="text/javascript" src="../../assets/vendor/php-email-form/validate.js"></script>
  <script type="text/javascript" src="../../assets/vendor/venobox/venobox.min.js"></script>
  <script type="text/javascript" src="../../assets/vendor/waypoints/jquery.waypoints.min.js"></script>
  <script type="text/javascript" src="../../assets/vendor/counterup/counterup.min.js"></script>
  <script type="text/javascript" src="../../assets/vendor/owl.carousel/owl.carousel.min.js"></script>
  <script type="text/javascript" src="../../assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script type="text/javascript" src="../../assets/vendor/prism/prism.js"></script>
  <script type="text/javascript" src="../../assets/vendor/aos/aos.js"></script>
                                                                <!-- Template Main JS File -->
  <script src="../../assets/js/main.js"></script>
                                                                <cript type="text/javascript" async src="//cdnjs.cloudflare.com/ajax/libs/mMathJax.Hub.Config({
tex2jax: {
inlineMath: [["$", "$"], ["\\(", "\\)"]],
processEscapes: true