<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <title>Projects - Credit Card Transactions Data Analysis - How can Machine Learning Help with Fraud Detection? (Part 1)</title>
    <meta content="" name="description">
    <meta content="" name="keywords">
    <!-- Favicons -->
    <link href="../../assets/img/web-icon.png" rel="icon">
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Roboto:300,300i,400,400i,500,500i,700,700i&display=swap" rel="stylesheet">
    <!-- Vendor CSS Files -->
    <link href="../../assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="../../assets/vendor/animate.css/animate.min.css" rel="stylesheet">
    <link href="../../assets/vendor/icofont/icofont.min.css" rel="stylesheet">
    <link href="../../assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
    <link href="../../assets/vendor/venobox/venobox.css" rel="stylesheet">
    <link href="../../assets/vendor/owl.carousel/assets/owl.carousel.min.css" rel="stylesheet">
    <link href="../../assets/vendor/aos/aos.css" rel="stylesheet">
    <!-- Template Main CSS File -->
    <link href="../../assets/css/style.css" rel="stylesheet">
    <link href="../../assets/css/prism.css" rel="stylesheet">
  </head>
  <body>
    <!-- ======= Header ======= -->
    <header id="header" class="fixed-top ">
      <div class="container">
        <div class="logo float-left">
          <h1 class="text-light"><a href="../../index.html"><span>Alimr.dev</span></a></h1>
        </div>
        <nav class="nav-menu float-right d-none d-lg-block">
          <ul>
            <li><a href="../../index.html">Home</a></li>
            <li class="active"><a href="../../projects.html">Projects</a></li>
            <li class="drop-down"><a href="../../tutorials.html">Tutorials</a>
            <ul>
              <li><a href="../../tutorials.html#Pandas">Pandas</a></li>
              <li class="drop-down"><a href="../../tutorials.html#Machine-Learning">Machine Learning</a>
              <ul>
                <li><a href="linear_regression.html">Linear Regression</a></li>
                <li><a href="linear_regression.html">Support Vector Machines</a></li>
                <li><a href="linear_regression.html">Naive Bayes</a></li>
                <li><a href="linear_regression.html">Decision Trees</a></li>
                <li><a href="linear_regression.html">Ensemble Methods</a></li>
              </ul>
              </li>
              <li><a href="../../tutorials.html#Linear-Algebra">Linear Algebra</a></li>
              <li><a href="../../tutorials.html#Visualization">Visualization</a></li>
              <li><a href="../../tutorials.html#Jupyter-Notebook">Jupyter Notebook</a></li>
              <li><a href="../../tutorials.html#Git">Git</a></li>
              <li><a href="../../tutorials.html#Bash">Bash</a></li>
              <li><a href="../../tutorials.html#Deep-Learning">Deep Learning</a></li>
              <li><a href="../../tutorials.html#Conda">Conda</a></li>
              <li><a href="../../tutorials.html#Numerical-Methods">Numerical Methods</a></li>
              <li><a href="../../tutorials.html#Big-Data">Big Data</a></li>
              <li><a href="../../tutorials.html#Classification">Classification</a></li>
              <li><a href="../../tutorials.html#Pattern-Recognition">Pattern Recognition</a></li>
              <li><a href="../../tutorials.html#SQL">SQL</a></li>
            </ul>
            </li>
            <li><a href="../../about.html">About Me</a></li>
            <li><a href="../../contact.html">Contact</a></li>
          </ul>
        </nav><!-- .nav-menu -->
      </div>
    </header><!-- End Header -->
    <main id="main">
      <!-- ======= About Us Section ======= -->
      <section class="breadcrumbs">
        <div class="container">
          <div class="d-flex justify-content-between align-items-center">
            <h2>Projects</h2>
            <ol>
              <li><a href="../../projects.html">Projects</a></li>
              <li>Credit Card Transactions Data Analysis - How can Machine Learning Help with Fraud Detection? (Part 1)</li>
            </ol>
          </div>
        </div>
      </section>
      <!-- End Header -->
      <section class="projects" data-aos="fade-up" data-aos-easing="ease-in-out" data-aos-duration="1000">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <h3>Table of contents</h3>
              <ul>
                <li><a href="#intro">Introduction</a></li>
                <li><a href="#summary">Project Summary</li>
                <!-- <li><a href="#part1">Part 1</li>  -->
                <li><a href="#data_collection">Data Collection and Metadata</a></li>
                <li><a href="#data_prep">Data preparation</a></li>
                <ul>
                <li><a href="#load_libs">Loading the libraries</a></li>
                <li><a href="#load_data">Loading data</a></li>
                <li><a href="#normalization">Normalizing the features</a></li>
                </ul>
                <li><a href="#eda">Exploratory data analysis</a></li>
                <ul>
                <li><a href="#correlation">Correlation plot</a></li>
                <li><a href="#feature_importance">Feature importance</a></li>
                <li><a href="#class_dists">Class distributions for the 5 most important fatures</a></li>
                </ul>
                <li><a href="#model_sel">Model Selection</a></li>
                <ul>
                <li><a href="#notes_on_model_sel">Notes regarding model selection process</a></li>
                <li><a href="#modelselection">Comparing the model performances to find the winner model</a></li>
                </ul>
                <li><a href="#xgb_tuning">Hyperparameter tuning of the XGBoost model</a></li>
                <ul>
                <li><a href="#data_prep_cv">Data preparation for CV</a></li>
                <li><a href="#helper_funcs">Helper functions for CV</a></li>
                <li><a href="#hyper_param_groups">Dividing the hyperparameters into orthogonal groups</a></li>
                <li><a href="#gs1">Gridsearch for parameter group 1</a></li>
                <li><a href="#bias-variance">Acceptance threshold in the context of bias-variance trade-off</a></li>
                <li><a href="#gs2">Gridsearch for parameter group 2</a></li>
                <li><a href="#gs3">Gridsearch for parameter group 3</a></li>
                <ul>
                  <li><a href="#gs3_notes_1">Evolution of <code>eval_metrics</code> with the number of boosting rounds and <code>learning_rate</code></a></li>
                  <li><a href="#gs3_notes_2">Variation of <code>logloss</code> and <code>PR-AUC</code> for <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">gamma=3</code> and different values of <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">alpha</code></a></li>
                </ul>
                <li><a href="#gs4">Gridsearch for parameter group 4</a></li>
                <ul>
                <li><a href="#gs4_notes_1">Important note regarding overfitting and the <em><a href="#acceptance_threshold">acceptance threshold</a></em></a></li>
                <li><a href="#gs4_notes_2">Variation of <code>logloss</code> and <code>PR-AUC</code> for <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">scale_pos_weight=[1,5,20,100,1000]</code></a></li>

                </ul>
                <li><a href="#gs5">Gridsearch for parameter group 5</a></li>
                <ul>
                  <li><a href="#gs5_notes_1">How <strong>Early Stopping</strong> influences the optimal <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">learning_rate</code></a></li>
                  <li><a href="#gs5_notes_2">Variation of <code>logloss</code> and <code>PR-AUC</code> for <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">alpha=0.001</code> and different values of <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">gamma</code></a></li>
                </ul>
                <li><a href="#best_threshold">Plotting the PR curves and finding the best threshold for the winner model</a></li>
                <li><a href="#baseline">Comparison with the baseline model</a></li>
                <li><a href="#conclusion">Summary and Conclusion</a></li>
                </ul>
              <h3><a class="header_arg" id="intro"></a>Introduction</h3>
              <p>
                Credit card fraud comes in different forms; phishing, skimming and identity theft to name a few. This project focuses on developing a supervised machine learning model that will provide the best results in revealing and preventing fraudulent transactions. Part 1 of the project compares the accuracy of different machine learning models in classifying the credit card transaction data into valid and fraud transactions.
              </p>
              <h3><a class="header_arg" id="summary"></a>Project Summary</h3>

              <h4>Part 1: Find the best predictive model among the common ML algorithms</h4>
              <ul>
                <li>Data collection and metadata</li>
                <li>Data preparation</li>
                <li>Model selection</li>
                <li>Choosing the right performance metric</li>
                <li>Train and evaluate the models</li>
                <li>Comparing the model performances to find the winner model</li>
                <li>Hypertuning the winner model's parameters</li>
                <li>Comparing the winner model with the baseline model</li>
              </ul>
              
              <h4><a class="header_arg" id="part2"></a>Part 2: Compare the accuracy of the winner of Part 1 with other algorithms that are specialized for rare-event analysis</h4>
              <ul>
                <li>Models for rare-event classification</li>
                  <ul>
                    <li>SMOTE</li>
                    <li>Autoencoders</li>
                  </ul>
                <li>Train and evaluate the models</li>
                <li>Compare the model performances and find the winner model</li>
                <li>Make predictions</li>
                <li>Conclusion</li>
              </ul>
              
              <h3><a class="header_arg" id="part1"></a>Part 1</h3> 
              
              <h3><a class="header_arg" id="data_collection"></a>Data Collection and Metadata</h3>
              About this data set:
              
              <ul>
              <li>The dataset contains <strong>transactions made by credit cards in September 2013 by European cardholders</strong>.</li>
              <li>The dataset presents transactions that occurred in two days, where we have <strong>492 fraud transactions among 284,807 transactions</strong>.</li>
              <li>The dataset is <strong>extremely imbalanced</strong> with the positive class (frauds) account for only <strong>0.172%</strong> of all transactions.</li>
              <li>The dataset contains <strong>only numerical input variables which are the result of a PCA transformation</strong>. </li>
              <li><strong>Due to confidentiality</strong>, the original features and <strong>more background information about the data is not provided</strong>. </li>
              <li>The dataset has 30 features, $\{V_1, V_2, ... V_{28}\}$ from PCA and two features which have not been transformed with PCA that are <strong>Time</strong> and <strong>Amount</strong>. </li>
              <li><strong>Time</strong> shows the time elapsed (in seconds) between each transaction and the first transaction in the dataset and <strong>Amount</strong> is the transaction amount.</li>
              <li>Finally,<strong>Class</strong> is the <strong>response variable</strong> and it takes value <strong>1 in case of fraud and 0 otherwise</strong>.</li>
              </ul>
              <table class="table table-bordered" style="display: contents;">
                <thead>
                  <tr style="text-align: center;">
                    <th style="background-color: #ffebe6;"></th>
                    <th>Category</th>
                    <th style="background-color: #ffebe6;">Type</th>
                    <th>Method</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th scope="row">1</th>
                    <td rowspan="8" style="background-color: white; vertical-align:  middle;">Supervised Learning</td>
                    <td rowspan="8" style="background-color: #ffebe6; vertical-align: middle;">Classification</td>
                    <td>Logistic Regression (LR)</td>
                  </tr>
                  <tr>
                    <th scope="row">2</th>
                    <td>Support Vector Machines (SVM)</td>
                  </tr>
                  <tr>
                    <th scope="row">3</th>
                    <td>K-Nearest Neighbours (KNN)</td>
                  </tr>
                  <tr>
                    <th scope="row">4</th>
                    <td>Naive Bayes (NB)</td>
                  </tr>
                  <tr>
                    <th scope="row">5</th>
                    <td>Artificial Neural Networks (ANN)</td>
                  </tr>
                  <tr>
                    <th scope="row">6</th>
                    <td>Random Forests (RF)</td>
                  </tr>
                  <tr>
                    <th scope="row">7</th>
                    <td>Decision Trees (DT)</td>
                  </tr>
                  <tr>
                    <th scope="row">8</th>
                    <td>XGBoost (XGB)</td>
                  </tr>
                </tbody>
              </table>
              <h3><a class="header_arg" id="load_prep"></a>Data preparation</h3>
              
              <h4><a class="header_arg" id="load_libs"></a>Loading the libraries</h4>
              
              
<pre class="prettyprint lang-language"><code class="language-python">import warnings
import matplotlib
import numpy as np
import pandas as pd
import seaborn as sns
from collections import Counter
import matplotlib.pyplot as plt
from matplotlib import cm as cm
warnings.filterwarnings('ignore')
from IPython.display import Image
from matplotlib import rc, rcParams
from IPython.core.display import HTML
matplotlib.rcParams['font.family'] = 'serif'
rc('font',**{'family':'serif','serif':['Times']})
rc('text', usetex=False)
rc('text.latex', preamble=r'\usepackage{underscore}')
pd.set_option('display.float_format', lambda x: '%.2f' % x)
sns.set(rc={"figure.dpi":100})
sns.set_style('white')</code></pre>
              <h4><a class="header_arg" id="load_data"></a>Loading data</h4>
<pre class="prettyprint lang-language"><code class="language-python">df = pd.read_csv("creditcard.csv")df.head()</code></pre>
              

<table class="table table-striped">
  <thead>
    <tr style="text-align: right;">
                    <th></th>
                    <th>Time</th>
                    <th>V1</th>
                    <th>V2</th>
                    <th>V3</th>
                    <th>V4</th>
                    <th>V5</th>
                    <th>V6</th>
                    <th>V7</th>
                    <th>V8</th>
                    <th>V9</th>
                    <th>...</th>
                    <th>V21</th>
                    <th>V22</th>
                    <th>V23</th>
                    <th>V24</th>
                    <th>V25</th>
                    <th>V26</th>
                    <th>V27</th>
                    <th>V28</th>
                    <th>Amount</th>
                    <th>Class</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th scope="row">0</th>
                    <td>0.00</td>
                    <td>-1.36</td>
                    <td>-0.07</td>
                    <td>2.54</td>
                    <td>1.38</td>
                    <td>-0.34</td>
                    <td>0.46</td>
                    <td>0.24</td>
                    <td>0.10</td>
                    <td>0.36</td>
                    <td>...</td>
                    <td>-0.02</td>
                    <td>0.28</td>
                    <td>-0.11</td>
                    <td>0.07</td>
                    <td>0.13</td>
                    <td>-0.19</td>
                    <td>0.13</td>
                    <td>-0.02</td>
                    <td>149.62</td>
                    <td>0</td>
                  </tr>
                  <tr>
                    <th scope="row">1</th>
                    <td>0.00</td>
                    <td>1.19</td>
                    <td>0.27</td>
                    <td>0.17</td>
                    <td>0.45</td>
                    <td>0.06</td>
                    <td>-0.08</td>
                    <td>-0.08</td>
                    <td>0.09</td>
                    <td>-0.26</td>
                    <td>...</td>
                    <td>-0.23</td>
                    <td>-0.64</td>
                    <td>0.10</td>
                    <td>-0.34</td>
                    <td>0.17</td>
                    <td>0.13</td>
                    <td>-0.01</td>
                    <td>0.01</td>
                    <td>2.69</td>
                    <td>0</td>
                  </tr>
                  <tr>
                    <th scope="row">2</th>
                    <td>1.00</td>
                    <td>-1.36</td>
                    <td>-1.34</td>
                    <td>1.77</td>
                    <td>0.38</td>
                    <td>-0.50</td>
                    <td>1.80</td>
                    <td>0.79</td>
                    <td>0.25</td>
                    <td>-1.51</td>
                    <td>...</td>
                    <td>0.25</td>
                    <td>0.77</td>
                    <td>0.91</td>
                    <td>-0.69</td>
                    <td>-0.33</td>
                    <td>-0.14</td>
                    <td>-0.06</td>
                    <td>-0.06</td>
                    <td>378.66</td>
                    <td>0</td>
                  </tr>
                  <tr>
                    <th scope="row">3</th>
                    <td>1.00</td>
                    <td>-0.97</td>
                    <td>-0.19</td>
                    <td>1.79</td>
                    <td>-0.86</td>
                    <td>-0.01</td>
                    <td>1.25</td>
                    <td>0.24</td>
                    <td>0.38</td>
                    <td>-1.39</td>
                    <td>...</td>
                    <td>-0.11</td>
                    <td>0.01</td>
                    <td>-0.19</td>
                    <td>-1.18</td>
                    <td>0.65</td>
                    <td>-0.22</td>
                    <td>0.06</td>
                    <td>0.06</td>
                    <td>123.50</td>
                    <td>0</td>
                  </tr>
                  <tr>
                    <th scope="row">4</th>
                    <td>2.00</td>
                    <td>-1.16</td>
                    <td>0.88</td>
                    <td>1.55</td>
                    <td>0.40</td>
                    <td>-0.41</td>
                    <td>0.10</td>
                    <td>0.59</td>
                    <td>-0.27</td>
                    <td>0.82</td>
                    <td>...</td>
                    <td>-0.01</td>
                    <td>0.80</td>
                    <td>-0.14</td>
                    <td>0.14</td>
                    <td>-0.21</td>
                    <td>0.50</td>
                    <td>0.22</td>
                    <td>0.22</td>
                    <td>69.99</td>
                    <td>0</td>
                  </tr>
                </tbody>
              </table>
              <p>5 rows × 31 columns</p>
<pre class="prettyprint lang-language"><code class="language-python">df.columns</code></pre>
<pre>Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',
       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',
       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',
       'Class'],
       dtype='object')</pre>      
<pre class="prettyprint lang-language"><code class="language-python">counter = Counter(df['Class'])
print(f'Class distribution of the response variable: {counter}')
print(f'Minority class corresponds to {100*counter[1]/(counter[0]+counter[1]):.3f}% of the data')</code></pre>
<pre>Class distribution of the response variable: Counter({0: 284315, 1: 492})
Minority class corresponds to 0.173% of the data</pre>
<pre class="prettyprint lang-language"><code class="language-python">df.describe()</code></pre>
              <table class="table table-striped">
                <thead>
                  <tr style="text-align: right;">
                    <th></th>
                    <th>Time</th>
                    <th>V1</th>
                    <th>V2</th>
                    <th>V3</th>
                    <th>V4</th>
                    <th>V5</th>
                    <th>V6</th>
                    <th>V7</th>
                    <th>V8</th>
                    <th>V9</th>
                    <th>...</th>
                    <th>V21</th>
                    <th>V22</th>
                    <th>V23</th>
                    <th>V24</th>
                    <th>V25</th>
                    <th>V26</th>
                    <th>V27</th>
                    <th>V28</th>
                    <th>Amount</th>
                    <th>Class</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th scope="row">count</th>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>...</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                  </tr>
                  <tr>
                    <th scope="row">mean</th>
                    <td>94813.86</td>
                    <td>0.00</td>
                    <td>0.00</td>
                    <td>-0.00</td>
                    <td>0.00</td>
                    <td>0.00</td>
                    <td>0.00</td>
                    <td>-0.00</td>
                    <td>0.00</td>
                    <td>-0.00</td>
                    <td>...</td>
                    <td>0.00</td>
                    <td>-0.00</td>
                    <td>0.00</td>
                    <td>0.00</td>
                    <td>0.00</td>
                    <td>0.00</td>
                    <td>-0.00</td>
                    <td>-0.00</td>
                    <td>88.35</td>
                    <td>0.00</td>
                  </tr>
                  <tr>
                    <th scope="row">std</th>
                    <td>47488.15</td>
                    <td>1.96</td>
                    <td>1.65</td>
                    <td>1.52</td>
                    <td>1.42</td>
                    <td>1.38</td>
                    <td>1.33</td>
                    <td>1.24</td>
                    <td>1.19</td>
                    <td>1.10</td>
                    <td>...</td>
                    <td>0.73</td>
                    <td>0.73</td>
                    <td>0.62</td>
                    <td>0.61</td>
                    <td>0.52</td>
                    <td>0.48</td>
                    <td>0.40</td>
                    <td>0.33</td>
                    <td>250.12</td>
                    <td>0.04</td>
                  </tr>
                  <tr>
                    <th scope="row">min</th>
                    <td>0.00</td>
                    <td>-56.41</td>
                    <td>-72.72</td>
                    <td>-48.33</td>
                    <td>-5.68</td>
                    <td>-113.74</td>
                    <td>-26.16</td>
                    <td>-43.56</td>
                    <td>-73.22</td>
                    <td>-13.43</td>
                    <td>...</td>
                    <td>-34.83</td>
                    <td>-10.93</td>
                    <td>-44.81</td>
                    <td>-2.84</td>
                    <td>-10.30</td>
                    <td>-2.60</td>
                    <td>-22.57</td>
                    <td>-15.43</td>
                    <td>0.00</td>
                    <td>0.00</td>
                  </tr>
                  <tr>
                    <th scope="row">25%</th>
                    <td>54201.50</td>
                    <td>-0.92</td>
                    <td>-0.60</td>
                    <td>-0.89</td>
                    <td>-0.85</td>
                    <td>-0.69</td>
                    <td>-0.77</td>
                    <td>-0.55</td>
                    <td>-0.21</td>
                    <td>-0.64</td>
                    <td>...</td>
                    <td>-0.23</td>
                    <td>-0.54</td>
                    <td>-0.16</td>
                    <td>-0.35</td>
                    <td>-0.32</td>
                    <td>-0.33</td>
                    <td>-0.07</td>
                    <td>-0.05</td>
                    <td>5.60</td>
                    <td>0.00</td>
                  </tr>
                  <tr>
                    <th scope="row">50%</th>
                    <td>84692.00</td>
                    <td>0.02</td>
                    <td>0.07</td>
                    <td>0.18</td>
                    <td>-0.02</td>
                    <td>-0.05</td>
                    <td>-0.27</td>
                    <td>0.04</td>
                    <td>0.02</td>
                    <td>-0.05</td>
                    <td>...</td>
                    <td>-0.03</td>
                    <td>0.01</td>
                    <td>-0.01</td>
                    <td>0.04</td>
                    <td>0.02</td>
                    <td>-0.05</td>
                    <td>0.00</td>
                    <td>0.01</td>
                    <td>22.00</td>
                    <td>0.00</td>
                  </tr>
                  <tr>
                    <th scope="row">75%</th>
                    <td>139320.50</td>
                    <td>1.32</td>
                    <td>0.80</td>
                    <td>1.03</td>
                    <td>0.74</td>
                    <td>0.61</td>
                    <td>0.40</td>
                    <td>0.57</td>
                    <td>0.33</td>
                    <td>0.60</td>
                    <td>...</td>
                    <td>0.19</td>
                    <td>0.53</td>
                    <td>0.15</td>
                    <td>0.44</td>
                    <td>0.35</td>
                    <td>0.24</td>
                    <td>0.09</td>
                    <td>0.08</td>
                    <td>77.16</td>
                    <td>0.00</td>
                  </tr>
                  <tr>
                    <th scope="row">max</th>
                    <td>172792.00</td>
                    <td>2.45</td>
                    <td>22.06</td>
                    <td>9.38</td>
                    <td>16.88</td>
                    <td>34.80</td>
                    <td>73.30</td>
                    <td>120.59</td>
                    <td>20.01</td>
                    <td>15.59</td>
                    <td>...</td>
                    <td>27.20</td>
                    <td>10.50</td>
                    <td>22.53</td>
                    <td>4.58</td>
                    <td>7.52</td>
                    <td>3.52</td>
                    <td>31.61</td>
                    <td>33.85</td>
                    <td>25691.16</td>
                    <td>1.00</td>
                  </tr>
                </tbody>
              </table>
              <p>8 rows × 31 columns</p>

              <h3><a class="header_arg" id="normalization"></a>Normalizing the features</h3>
              <p>
                I use <a href="https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)">Re-scaling (min-max normalization)</a> to normalize the features. Re-scaling transforms all the numerical features to the range $[-1,\,1]$
              </p>
              
$$
\text{min-max normalization: }x \rightarrow -1 + \frac{2(x-min(x))}{max(x)-min(x)}
$$
              <pre class="prettyprint lang-language"><code class="language-python">df.iloc[:,:30] = -1 + (df.iloc[:,:30] - df.iloc[:,:30].min())*2 / (df.iloc[:,:30].max() - df.iloc[:,:30].min())</code></pre>
              <pre class="prettyprint lang-language"><code class="language-python">df.describe()</code></pre>
              <table class="table table-striped">
                <thead>
                  <tr style="text-align: right;">
                    <th></th>
                    <th>Time</th>
                    <th>V1</th>
                    <th>V2</th>
                    <th>V3</th>
                    <th>V4</th>
                    <th>V5</th>
                    <th>V6</th>
                    <th>V7</th>
                    <th>V8</th>
                    <th>V9</th>
                    <th>...</th>
                    <th>V21</th>
                    <th>V22</th>
                    <th>V23</th>
                    <th>V24</th>
                    <th>V25</th>
                    <th>V26</th>
                    <th>V27</th>
                    <th>V28</th>
                    <th>Amount</th>
                    <th>Class</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th scope="row">count</th>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>...</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                    <td>284807.00</td>
                  </tr>
                  <tr>
                    <th scope="row">mean</th>
                    <td>0.10</td>
                    <td>0.92</td>
                    <td>0.53</td>
                    <td>0.67</td>
                    <td>-0.50</td>
                    <td>0.53</td>
                    <td>-0.47</td>
                    <td>-0.47</td>
                    <td>0.57</td>
                    <td>-0.07</td>
                    <td>...</td>
                    <td>0.12</td>
                    <td>0.02</td>
                    <td>0.33</td>
                    <td>-0.24</td>
                    <td>0.16</td>
                    <td>-0.15</td>
                    <td>-0.17</td>
                    <td>-0.37</td>
                    <td>-0.99</td>
                    <td>0.00</td>
                  </tr>
                  <tr>
                    <th scope="row">std</th>
                    <td>0.55</td>
                    <td>0.07</td>
                    <td>0.03</td>
                    <td>0.05</td>
                    <td>0.13</td>
                    <td>0.02</td>
                    <td>0.03</td>
                    <td>0.02</td>
                    <td>0.03</td>
                    <td>0.08</td>
                    <td>...</td>
                    <td>0.02</td>
                    <td>0.07</td>
                    <td>0.02</td>
                    <td>0.16</td>
                    <td>0.06</td>
                    <td>0.16</td>
                    <td>0.01</td>
                    <td>0.01</td>
                    <td>0.02</td>
                    <td>0.04</td>
                  </tr>
                  <tr>
                    <th scope="row">min</th>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>...</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>-1.00</td>
                    <td>0.00</td>
                  </tr>
                  <tr>
                    <th scope="row">25%</th>
                    <td>-0.37</td>
                    <td>0.89</td>
                    <td>0.52</td>
                    <td>0.64</td>
                    <td>-0.57</td>
                    <td>0.52</td>
                    <td>-0.49</td>
                    <td>-0.48</td>
                    <td>0.57</td>
                    <td>-0.12</td>
                    <td>...</td>
                    <td>0.12</td>
                    <td>-0.03</td>
                    <td>0.33</td>
                    <td>-0.33</td>
                    <td>0.12</td>
                    <td>-0.26</td>
                    <td>-0.17</td>
                    <td>-0.38</td>
                    <td>-1.00</td>
                    <td>0.00</td>
                  </tr>
                  <tr>
                    <th scope="row">50%</th>
                    <td>-0.02</td>
                    <td>0.92</td>
                    <td>0.54</td>
                    <td>0.68</td>
                    <td>-0.50</td>
                    <td>0.53</td>
                    <td>-0.48</td>
                    <td>-0.47</td>
                    <td>0.57</td>
                    <td>-0.08</td>
                    <td>...</td>
                    <td>0.12</td>
                    <td>0.02</td>
                    <td>0.33</td>
                    <td>-0.22</td>
                    <td>0.16</td>
                    <td>-0.17</td>
                    <td>-0.17</td>
                    <td>-0.37</td>
                    <td>-1.00</td>
                    <td>0.00</td>
                  </tr>
                  <tr>
                    <th scope="row">75%</th>
                    <td>0.61</td>
                    <td>0.96</td>
                    <td>0.55</td>
                    <td>0.71</td>
                    <td>-0.43</td>
                    <td>0.54</td>
                    <td>-0.47</td>
                    <td>-0.46</td>
                    <td>0.58</td>
                    <td>-0.03</td>
                    <td>...</td>
                    <td>0.13</td>
                    <td>0.07</td>
                    <td>0.34</td>
                    <td>-0.12</td>
                    <td>0.20</td>
                    <td>-0.07</td>
                    <td>-0.16</td>
                    <td>-0.37</td>
                    <td>-0.99</td>
                    <td>0.00</td>
                  </tr>
                  <tr>
                    <th scope="row">max</th>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>...</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                    <td>1.00</td>
                  </tr>
                </tbody>
              </table>
              <p>8 rows × 31 columns</p>
              <h3><a class="header_arg" id="eda"></a>Exploratory data analysis</h3>
              <h4><a class="header_arg" id="correlation"></a>Correlation plot</h4>
              <pre class="prettyprint lang-language"><code class="language-python">fig, ax = plt.subplots(1, 1, figsize=(14,14))
corr = df.corr()
ax_ = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True,
    ax=ax,
    cbar_kws={'shrink': 0.82}
)
ax_.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
)
ax.set_title('Correlation between principal components of the credit card dataset', fontsize=18, y=1.02);</code></pre>
              <div style="text-align:center">
                 <a id="fig1" style="font-size: 1.5em;">Figure 1</a>
              </div>
              <p class="text-align: center;">
                <img src="output_1.png" style="width: 90%; display: block; margin: 10px auto 20px;"/>
              </p>              
              <p>
                As we can see in <a href="#fig1">Figure 1</a>, most of the data features are not correlated. This is because before publishing, most of the features have been transformed using Principal Component Analysis (PCA). The importance of these features, however, could be assessed using the <code class="module">RandomForestClassifier.feature_importances_</code> of <code class="module">sklearn.ensemble</code> which requires training a classification tree on the data or <code class="method">SelectKbest</code> method from <code class="module">sklearn.feature_selection</code> for a general model.
              </p>
              <h4><a class="header_arg" id="feature_importance"></a>Feature importance</h4>
              <pre class="prettyprint lang-language"><code class="language-python">from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif

def get_feature_importance_Kbest(X, y, feature_names, n_features_to_plot=30):
    kbest = SelectKBest(score_func=f_classif, k=n_features_to_plot)
    fit = kbest.fit(X, y)
    feature_ids_sorted = np.argsort(fit.scores_)[::-1]
    features_sorted_by_importance = np.array(feature_names)[feature_ids_sorted]
    try:
        feature_importance_array = np.vstack((
        features_sorted_by_importance[:n_features_to_plot], 
        np.array(sorted(fit.scores_)[::-1][:n_features_to_plot])
    )).T
    except:
        feature_importance_array = np.vstack((
        features_sorted_by_importance[:], 
        np.array(sorted(fit.scores_)[::-1][:])
    )).T
            
    feature_importance_data = pd.DataFrame(feature_importance_array, columns=['feature', 'score'])
    feature_importance_data['score'] = feature_importance_data['score'].astype(float)
    return feature_importance_data

# Using RF feature_importances_
def get_feature_importance_RF(X, y, feature_names, n_features_to_plot=30):
    RF = RandomForestClassifier(random_state=0)
    RF.fit(X, y)
    importances_RF = RF.feature_importances_
    feature_ids_sorted = np.argsort(importances_RF)[::-1]
    features_sorted_by_importance = np.array(feature_names)[feature_ids_sorted]
    try:
        feature_importance_array = np.vstack((
        features_sorted_by_importance[:n_features_to_plot], 
        np.array(sorted(importances_RF)[::-1][:n_features_to_plot])
    )).T
    except:
        feature_importance_array = np.vstack((
        features_sorted_by_importance[:], 
        np.array(sorted(importances_RF)[::-1][:])
    )).T
    feature_importance_data = pd.DataFrame(feature_importance_array, columns=['feature', 'score'])
    feature_importance_data['score'] = feature_importance_data['score'].astype(float)
    return feature_importance_data

# Features and response variable
X = df.iloc[:,:30].values
y = df.iloc[:,30].values
feature_names = list(df.columns.values[:30])

# Using KBest
importances_KBest = get_feature_importance_Kbest(X, y, feature_names)

# Using RF
importances_RF = get_feature_importance_RF(X, y, feature_names)</code></pre>
                    
<pre class="prettyprint lang-language"><code class="language-python">pd.concat([importances_RF,importances_KBest],axis=1)</code></pre>
              
              <table class="table table-striped">
                <thead>
                  <tr>
                    <th style="background-color: #ffebe6;"></th>
                    <th colspan="2" style="background-color: #FFCABD;">RF</th>
                    <th colspan="2" style="background-color: #ffebe6;">KBest</th>
                  </tr>
                  <tr style="text-align: right;">
                    <th></th>
                    <th>feature</th>
                    <th>score</th>
                    <th>feature</th>
                    <th>score</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th scope="row">0</th>
                    <td>V14</td>
                    <td>0.14</td>
                    <td>V17</td>
                    <td>33979.17</td>
                  </tr>
                  <tr>
                    <th scope="row">1</th>
                    <td>V17</td>
                    <td>0.14</td>
                    <td>V14</td>
                    <td>28695.55</td>
                  </tr>
                  <tr>
                    <th scope="row">2</th>
                    <td>V12</td>
                    <td>0.13</td>
                    <td>V12</td>
                    <td>20749.82</td>
                  </tr>
                  <tr>
                    <th scope="row">3</th>
                    <td>V10</td>
                    <td>0.09</td>
                    <td>V10</td>
                    <td>14057.98</td>
                  </tr>
                  <tr>
                    <th scope="row">4</th>
                    <td>V16</td>
                    <td>0.08</td>
                    <td>V16</td>
                    <td>11443.35</td>
                  </tr>
                  <tr>
                    <th scope="row">5</th>
                    <td>V11</td>
                    <td>0.06</td>
                    <td>V3</td>
                    <td>11014.51</td>
                  </tr>
                  <tr>
                    <th scope="row">6</th>
                    <td>V9</td>
                    <td>0.04</td>
                    <td>V7</td>
                    <td>10349.61</td>
                  </tr>
                  <tr>
                    <th scope="row">7</th>
                    <td>V18</td>
                    <td>0.03</td>
                    <td>V11</td>
                    <td>6999.36</td>
                  </tr>
                  <tr>
                    <th scope="row">8</th>
                    <td>V7</td>
                    <td>0.02</td>
                    <td>V4</td>
                    <td>5163.83</td>
                  </tr>
                  <tr>
                    <th scope="row">9</th>
                    <td>V4</td>
                    <td>0.02</td>
                    <td>V18</td>
                    <td>3584.38</td>
                  </tr>
                  <tr>
                    <th scope="row">10</th>
                    <td>V26</td>
                    <td>0.02</td>
                    <td>V1</td>
                    <td>2955.67</td>
                  </tr>
                  <tr>
                    <th scope="row">11</th>
                    <td>V21</td>
                    <td>0.02</td>
                    <td>V9</td>
                    <td>2746.60</td>
                  </tr>
                  <tr>
                    <th scope="row">12</th>
                    <td>V1</td>
                    <td>0.01</td>
                    <td>V5</td>
                    <td>2592.36</td>
                  </tr>
                  <tr>
                    <th scope="row">13</th>
                    <td>V6</td>
                    <td>0.01</td>
                    <td>V2</td>
                    <td>2393.40</td>
                  </tr>
                  <tr>
                    <th scope="row">14</th>
                    <td>V3</td>
                    <td>0.01</td>
                    <td>V6</td>
                    <td>543.51</td>
                  </tr>
                  <tr>
                    <th scope="row">15</th>
                    <td>V8</td>
                    <td>0.01</td>
                    <td>V21</td>
                    <td>465.92</td>
                  </tr>
                  <tr>
                    <th scope="row">16</th>
                    <td>Time</td>
                    <td>0.01</td>
                    <td>V19</td>
                    <td>344.99</td>
                  </tr>
                  <tr>
                    <th scope="row">17</th>
                    <td>V5</td>
                    <td>0.01</td>
                    <td>V20</td>
                    <td>115.00</td>
                  </tr>
                  <tr>
                    <th scope="row">18</th>
                    <td>V2</td>
                    <td>0.01</td>
                    <td>V8</td>
                    <td>112.55</td>
                  </tr>
                  <tr>
                    <th scope="row">19</th>
                    <td>V20</td>
                    <td>0.01</td>
                    <td>V27</td>
                    <td>88.05</td>
                  </tr>
                  <tr>
                    <th scope="row">20</th>
                    <td>V19</td>
                    <td>0.01</td>
                    <td>Time</td>
                    <td>43.25</td>
                  </tr>
                  <tr>
                    <th scope="row">21</th>
                    <td>V27</td>
                    <td>0.01</td>
                    <td>V28</td>
                    <td>25.90</td>
                  </tr>
                  <tr>
                    <th scope="row">22</th>
                    <td>V22</td>
                    <td>0.01</td>
                    <td>V24</td>
                    <td>14.85</td>
                  </tr>
                  <tr>
                    <th scope="row">23</th>
                    <td>Amount</td>
                    <td>0.01</td>
                    <td>Amount</td>
                    <td>9.03</td>
                  </tr>
                  <tr>
                    <th scope="row">24</th>
                    <td>V13</td>
                    <td>0.01</td>
                    <td>V13</td>
                    <td>5.95</td>
                  </tr>
                  <tr>
                    <th scope="row">25</th>
                    <td>V15</td>
                    <td>0.01</td>
                    <td>V26</td>
                    <td>5.65</td>
                  </tr>
                  <tr>
                    <th scope="row">26</th>
                    <td>V24</td>
                    <td>0.01</td>
                    <td>V15</td>
                    <td>5.08</td>
                  </tr>
                  <tr>
                    <th scope="row">27</th>
                    <td>V28</td>
                    <td>0.01</td>
                    <td>V25</td>
                    <td>3.12</td>
                  </tr>
                  <tr>
                    <th scope="row">28</th>
                    <td>V25</td>
                    <td>0.01</td>
                    <td>V23</td>
                    <td>2.05</td>
                  </tr>
                  <tr>
                    <th scope="row">29</th>
                    <td>V23</td>
                    <td>0.01</td>
                    <td>V22</td>
                    <td>0.18</td>
                  </tr>
                </tbody>
              </table>
              
              
              
              <p>
                Note that the two methods rank the features similarly in terms of importance. 
              </p>
              
              <h4><a class="header_arg" id="class_dists"></a>Class distributions for the 5 most important fatures</h4>
              
              
<pre class="prettyprint lang-language"><code class="language-python">cols = ["#009900", "#ffcc00", "#0099cc", "#cc0066", "#666699"]
fig = plt.figure(figsize=(8,8), dpi=100)
important_features = ['V14','V17','V12','V10','V16']
theta=2*np.pi/5
offset = np.pi/2 - theta
radius = 0.3
pentagon_vertices = [[0.5+radius*(np.cos(x*theta+offset)), 
                      0.5+radius*(np.sin(x*theta+offset))] 
                     for x in range(5)]
for i, col in enumerate(important_features):
    tmp_ax =fig.add_axes([pentagon_vertices[i][0],
                          pentagon_vertices[i][1],
                          .25,
                          .25])
    sns.kdeplot(df[df['Class']==0][col], shade=True, color=cols[i], ax=tmp_ax)
    sns.kdeplot(df[df['Class']==1][col], shade=False, color=cols[i], ax=tmp_ax, linestyle="--")
    tmp_ax.set_xticklabels([])
    tmp_ax.set_yticklabels([])
    tmp_ax.set_title(important_features[i], fontsize=14)
    if i==len(important_features)-1:
        leg = tmp_ax.get_legend()
        leg.legendHandles[0].set_color('k')
        leg.legendHandles[1].set_color('k')
    plt.legend([], [], frameon=False)
fig.legend(leg.legendHandles, ['valid','fraud'], loc='center')
sns.despine(left=True)
plt.suptitle('Kernel Density Estimate (KDE) plot of class distributions for \n 5 most important features', x=0.64, y=1.15);</code></pre>
              <div style="text-align:center">
                 <a id="fig2" style="font-size: 1.5em;">Figure 2</a>
              </div>
              <p class="text-align: center;">
                <img src="output_2.png" style="width: 70%; display: block; margin: 10px auto 20px;"/>
              </p>
              <h3><a class="header_arg" id="model_sel"></a>Model Selection</h3>
              
              <p>
                This part explains the model selection process. We use the <code class="library">sklearn</code> implementation of the candidate models except for the <strong>XGBoost</strong> classifier where we use the <code class="library"><a href="https://xgboost.readthedocs.io/">xgboost</a></code> library. 
              </p>
              
              <h4><a class="header_arg" id="notes_on_model_sel"></a>Notes regarding the model selection process</h4>
              <ul>
                <li>
                  Due to the extreme class imbalance in the dataset, we take advantage of the <code class="library">sklearn</code>'s <strong>Stratified K-Folds cross-validator</strong> (<code class="method">StratifiedKFold</code>) to ensure that the observations are distributed with a similar class ratio $\bigg(\dfrac{\text{positive class count}}{\text{negative class count}}\bigg)$ across the folds.
                </li>
                <li>
                  We choose <code class="arg">scoring='average_precision'</code> as the model evaluation criteria. Ideally we'd want to use <a id="prcurve"><strong>Precision-Recall Area Under Curve (PR-AUC)</strong></a> as the criteria but since it is not provided by <code class="library">sklearn</code>, we use <code class="arg">average_precision</code> (<strong>AP</strong>) which <em>summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight</em>:
                </li>
              $$
              \text{AP} = \sum_n (R_n - R_{n-1}) P_n,
              $$
                <p>
                  where $P_n$ and $R_n$ are the precision and recall, respectively, at the $n$-th threshold [<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html">ref</a>].
                </p>
              </ul>
              
              
<pre class="prettyprint lang-language"><code class="language-python"># A whole host of Scikit-learn models
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
import time

# We use StratifiedKFold to guarantee similar distribution in train and test data
# We set the train:test ratio to 4:1 - The train set will ultimately be split
# into train and validation sets itself, with the same ratio! See below

#              X
#  ___________/\___________
# /                        \
#  x_1, x_2, x_3, ... , x_n

#       X_train       X_test
#  _______/\_______   __/\__
# /                \ /     \

# X_test will be unseen until the very last step!

#       X_train     
#  _______/\_______ 
# /                \
#         ||
#         ||
#         \/
#
#     X_tr     X_val 
#  ____/\____  _/\_ 
# /          \/    \ 

num_splits = 5
skf = StratifiedKFold(n_splits=num_splits, random_state=1, shuffle=True)

# Features and response variable
X = df.iloc[:,:30].values
y = df.iloc[:,30].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)

model_dict = {'CT':[DecisionTreeClassifier()], 
              'GB':[GradientBoostingClassifier()], 
              'KNN':[KNeighborsClassifier()],
              'LR':[LogisticRegression()], 
              'NB':[GaussianNB()], 
              'RF':[RandomForestClassifier()], 
              'SVC':[SVC()], 
              'XGB':[XGBClassifier()]}</code></pre>

              <h3><a class="header_arg" id="modelselection"></a>Comparing the model performances to find the winner model</h3>
              
              
              <pre class="prettyprint lang-language"><code class="language-python">for model_name, model in model_dict.items():
                  
    # train the model
    t_start = time.time()
    cv_results = cross_val_score(model[0], 
                                 X_train, 
                                 y_train, 
                                 cv=skf, 
                                 scoring='average_precision', 
                                 verbose=10, 
                                 n_jobs=-1) 
    
    # save the results
    calc_time = time.time() - t_start
    model_dict[model_name].append(cv_results)
    model_dict[model_name].append(calc_time)
    print(("{0} model gives an AP of {1:.2f}% with a standard deviation "
           "{2:.2f} (took {3:.1f} seconds)").format(model_name, 
                                                    100*cv_results.mean(), 
                                                    100*cv_results.std(), 
                                                    calc_time)
         )</code></pre>
<pre>        [Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.
        [Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   15.6s remaining:   23.4s
        [Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   15.8s remaining:   10.5s
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   16.9s remaining:    0.0s
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   16.9s finished
        [Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.
    
        CT model gives an AP of 53.17% with a standard deviation 3.44 (took 17.1 seconds)
    
        [Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  4.6min remaining:  6.9min
        [Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:  4.6min remaining:  3.1min
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  4.7min remaining:    0.0s
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  4.7min finished
        [Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.
    
        GB model gives an AP of 59.87% with a standard deviation 4.48 (took 279.6 seconds)
    
        [Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  1.8min remaining:  2.7min
        [Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:  2.0min remaining:  1.3min
    
        KNN model gives an AP of 78.70% with a standard deviation 3.31 (took 131.1 seconds)
    
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.2min remaining:    0.0s
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  2.2min finished
        [Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.
        [Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.9s remaining:    4.3s
        [Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:    2.9s remaining:    1.9s
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.1s remaining:    0.0s
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.1s finished
        [Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.
    
        LR model gives an AP of 72.42% with a standard deviation 2.56 (took 3.3 seconds)
    
        [Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    0.5s remaining:    0.8s
        [Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:    0.5s remaining:    0.4s
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s remaining:    0.0s
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s finished
        [Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.
    
        NB model gives an AP of 8.54% with a standard deviation 0.50 (took 0.7 seconds)
    
        [Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  2.7min remaining:  4.0min
        [Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:  2.7min remaining:  1.8min
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.0min remaining:    0.0s
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  3.0min finished
        [Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.
    
        RF model gives an AP of 84.02% with a standard deviation 3.00 (took 180.4 seconds)
    
        [Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:   10.4s remaining:   15.6s
        [Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:   10.5s remaining:    7.0s
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   10.7s remaining:    0.0s
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:   10.7s finished
        [Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.
    
        SVC model gives an AP of 75.22% with a standard deviation 3.54 (took 10.8 seconds)
    
        [Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:  1.6min remaining:  2.4min
        [Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:  1.6min remaining:  1.1min
    
        XGB model gives an AP of 84.89% with a standard deviation 2.92 (took 98.1 seconds)
    
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.6min remaining:    0.0s
        [Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  1.6min finished</pre>
              <p>
                <code class="method">XGB</code> and <code class="method">RF</code> outperform the other algorithms with <code class="method">XGB</code> being slightly more accurate than the random forest classifier so as we said previously, we choose <code class="library">XGB</code> as the winner. In the next part, we will go through the process of tuning the model.
              </p>
              <h3><a class="header_arg" id="xgb_tuning"></a>Hyperparameter tuning of the XGBoost model</h3>
              
              <p>
                In this part, we'd like to find the set of model parameters that work best on our credit card fraud detection problem. How can we configure the model so that we get the best model performance for the dataset we have and the evaluation metric that we specify?
              </p>
              <p>
                We are aiming to achieve this while ensuring that the resulted parameters don't cause overfitting. We use <code class="method">XGBClassifier()</code> with the default parameters as the baseline classifier. The following describes a summary of the steps and <strong>some important notes regarding the model parameters and cross-validation methodology</strong>:
              </p>
              <ul>
                <li>
                  We split the data into training and test splits while maintaining the class ratio (exactly what we did in the <a href="#modelselection">model selection</a>)
                </li>
                <li>
                  Model performance on the test data <code>(X_test, y_test)</code> is not going to be assessed until the final model evaluation where both the baseline classifier and the tuned model are going to be tested against the <em>unseen</em> test data
                </li>
                <li>
                  We use <code class="method"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html">RepeatedStratifiedKFold()</a></code> which repeats the <code class="method"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html">StratifiedKFold()</a></code> $n_{\mathrm{repeats}}$ times ($3\leq n_{\mathrm{repeats}} \leq 10$) and outputs the model performance-metric (PR-AUC in this case) as the mean across all folds and all repeats. This improves the estimate of the mean model performance-metric with the downside of increased model-evaluation cost. We use $n_{\mathrm{repeats}}=5$ in this work.
                </li>
                <li>
                  <strong>early stopping</strong> is one way to reduce overfitting of training data. Here's a brief summary of how it works:
                </li>
                <ul>
                  <li>
                    The user specifies an <em>evaluation metric</em> (<strong><code class="arg">eval_metric</code></strong>) and a <em>validation set</em> (<strong><code class="arg">eval_set</code></strong>) to be monitored during the training process.
                  </li>
                  <li>
                    The evaluation metric for the <code class="arg">eval_set</code> needs to <em>"improve"</em> at least once every <strong><code class="arg">early_stopping_rounds</code></strong> of boosting rounds so that the training continues.
                  </li>
                  <li>
                    <strong>Early stopping terminates the training process when the number of boosting rounds with no improvement in evaluation-metric for the <code class="arg">eval_set</code> reaches <code class="arg">early_stopping_rounds</code></strong>.
                  </li>
                  <li>
                    <code class="library">XGBoost</code> offers a wide range of evaluation metrics (see <a href="https://xgboost.readthedocs.io/en/latest/parameter.html">here</a> for the full list) that are either minimized (RMSE, log loss, etc.) or maximized (MAP, NDCG, AUC) during the training process. We use <code>aucpr</code> for this classification problem for the reasons discussed earlier.
                  </li>
                </ul>
                <li>
                  Normally, we would use <code class="library">sklearn</code>`s <code clas="method"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridsearchCV()</a></code> to find the hyperparameters of the model. This works great, especially if the model is a <em>native</em> sklearn model. Infact, most sklearn models come with their CV version, e.g., <code class="method">LogisticRegressionCV()</code>, which performs grid search over the user-defined hyperparameter dictionary. However, as we mentioned, when we try to use <code class="library">XGBoost</code>'s <strong>early stopping</strong>, we need to specify an <code class="arg">eval_set</code> to compare the evaluation-metric performance on it with that of the training data. Now if we use <code class="method">GridSearchCV()</code>, ideally we would want <code class="library">XGBoost</code> to use the held-out fold as the evaluation set to determine the optimal number of boosting rounds, however, that doesn't seem to be possible when using <code class="method">GridSearchCV()</code>. One way is to use a static held-out set for XGBoost but that doesn't make sense as it defeats the purpose of cross validation. To address this issue, I defined a custom cross-validation function that for each hyperparameter-set candidate:</li>
                <ol>
                  <li>
                    For each fold, finds the <strong>optimal number of boosting rounds</strong>, the number of rounds where the <code class="arg">eval_metric</code> reached its best value (<code class="arg">best_score</code>) for the <code class="arg">eval_set</code>.
                  </li>
                  <li>
                    Reports the mean and standard deviation of the <code class="arg">eval_metric</code> (for both training and evaluation data) and <strong>best number of rounds</strong> by averaging over the folds.</li>
                  <li>
                    Finally, returns the optimal combination of hyperparameters that gave the best <code class="arg">eval_metric</code>.</li>
                </ol>
                <li>
                  <strong>Note</strong>: <code class="arg">xgboost.best_ntree_limit</code> 
                  you do best_nrounds = int(best_nrounds / 0.8) you consider that your validation set was 20% of your whole training data (another way of saying that you performed a 5-fold cross-validation).

The rule can then be generalized as:

n_folds = 5
best_nrounds = int((res.shape[0] - estop) / (1 - 1 / n_folds))
                </li>
                <li>
                  Because <code class="method">XGBClassifier()</code> accepts <strong>a lot of hyperparameters</strong>, it would be computationally inefficient and extremely time-consuming to iterate over the enormous number of hyperparameter combinations. What we can do instead is to divide the parameters into a few independent or weakly-interacting groups and find the best hyperparameter combination for each group. The optimal set of parameters can ultimately be determined as the union of the best hyperparameter groups. The following is the hyperparameter groups and the order that we optimize them (<a href="https://towardsdatascience.com/              beyond-grid-search-hypercharge-hyperparameter-tuning-for-xgboost-7c78f7a2929d">reference</a>):
                </li>
                  <ol>
                    <li><code class="arg">max_depth</code>, <code class="arg">min_child_weight</code></li>
                    <li><code class="arg">colsample_bytree</code>, <code class="arg">colsample_bylevel</code>, <code class="arg">subsample</code></li>
                    <li><code class="arg">alpha</code>, <code class="arg">gamma</code></li>
                    <li><code class="arg">scale_pos_weight</code></li>
                    <li><code class="arg">learning_rate</code></li>
                  </ol>
                
                <li>
                  To <strong>ensure that the model hyperparameters don't cause overfitting</strong>, we save the model's <code class="arg">eval_metric</code> results for both training and validation data. At the end, we throw out the set of hyperparameters where the model exhibits a better performance for the training data than the validation data.
                </li>
                <li>
                  Because we are using <strong>early stopping</strong>, the model throws out the optimal number of trees (<code class="arg">n_estimators</code>) for each fold. For each parameter group, the best number of boosting trees is returned as the mean over the folds. 
                </li>
                <li>
                  We use a Python <code class="method">dictionary</code>, <code>progress</code>, to record the changes in validation and training data <code>eval_metric(s)</code>. This can help a lot with assessing whether the model is overfitting or not.
                </li>
                <li>
                  Our efforts to reduce overfitting are in accordance with <a href="https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html">XGBoost's Notes on Parameter Tuning</a>.
                </li>
              </ul>
            </li>
            <h4><a class="header_arg" id="data_prep_cv"></a>Data preparation for CV</h4>
              
              
              
<pre class="prettyprint lang-language"><code class="language-python">import xgboost as xgb
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import train_test_split

# load data
X = df.iloc[:,:30].values
y = df.iloc[:,30].values
all_features = list(df.columns.values[:30])

# train-test stratified split; stratify=y is used to ensure
# that the two splits have similar class distribution
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)

model = XGBClassifier(tree_method='gpu_hist', 
                      objective='binary:logistic')
    
# fold parameters
num_splits = 4
num_reps = 5
rand_state = 123
rskf = RepeatedStratifiedKFold(n_splits=num_splits,
                               n_repeats=num_reps, 
                               random_state=rand_state)</code></pre>
              <h4><a class="header_arg" id="helper_funcs"></a>Helper functions for CV</h4>
              <p>First, we define a function that for the given set of hyperparameters, trains the <code class="method">              XGBClassifier()</code> and spits out the model's mean-over-folds performance metrics.</p>
                    
<button class="btn btn-primary plus-minus-code collapsed" type="button" data-toggle="collapse" data-target="#collapseCVOutput1" aria-expanded="false" aria-controls="collapseCVOutput1">
</button>  
<div class="collapse" id="collapseCVOutput1" aria-expanded="true" style="height: 0px;">
<pre class="prettyprint lang-language"><code class="language-python">from sklearn.metrics import average_precision_score
from datetime import datetime, timedelta
from itertools import product
from pprint import pprint
np.set_printoptions(threshold=5)

Early_Stop_Rounds = 400
Num_Boosting_Rounds = 4000

from sklearn.metrics import average_precision_score
from datetime import datetime, timedelta
from itertools import product
from pprint import pprint
import random
np.set_printoptions(threshold=5)

Early_Stop_Rounds = 400
Num_Boosting_Rounds = 4000

def cv_summary(X_data, y_data, params, feature_names, cv, 
               objective_func='binary:logistic', 
               eval_metric='aucpr', 
               spit_out_fold_results=False, 
               spit_out_summary=True, 
               num_boost_rounds=Num_Boosting_Rounds, 
               early_stop_rounds=Early_Stop_Rounds, 
               save_fold_preds=False):
    """
    Returns cv results for a given set of parameters
    """
    
    # number of folds
    num_splits = cv.cvargs['n_splits']
    # num_rounds: total number of rounds (= n_repeats*n_folds)
    num_rounds = cv.get_n_splits()
    
    # train_scores and val_scores store the mean over all repeats of 
    # the in-sample and held-out predictions, respectively
    train_scores = np.zeros(num_rounds)
    valid_scores = np.zeros(num_rounds)
    best_num_estimators = np.zeros(num_rounds)
    
    # track eval results
    eval_results = []
    fold_preds = {'train':[], 'eval':[]}
        
    for i, (train_index, valid_index) in enumerate(cv.split(X_data, y_data)):
        
        # which repeat
        rep = i//num_splits + 1 
        
        # which fold
        fold = i%num_splits + 1
        
        # XGBoost train uses DMatrix 
        xg_train = xgb.DMatrix(X_data[train_index,:], 
                               feature_names=feature_names, 
                               label = y_data[train_index])
        
        xg_valid = xgb.DMatrix(X_data[valid_index,:], 
                               feature_names=feature_names, 
                               label = y_data[valid_index])   
        # set evaluation 
        params['tree_method'] = 'gpu_hist'
        params['objective'] = objective_func
        params['eval_metric'] = eval_metric
        
        # track eval results
        progress = dict()
        
        # train using train data
        if early_stop_rounds is not None:
            clf = xgb.train(params, xg_train, 
                            num_boost_round = num_boost_rounds, 
                            evals=[(xg_train, "train"), (xg_valid, "eval")], 
                            early_stopping_rounds=early_stop_rounds, 
                            evals_result=progress, 
                            verbose_eval=False)
            
            # for validation data we don't need to set ntree_limit because from
            # XGBoost docs:
            #
            # fit():
            #
            # if early_stopping_rounds is not None:
            #     self.best_ntree_limit = self._Booster.best_ntree_limit
            # ...
            # ...
            # predict():
            #
            # if ntree_limit is None:
            #     ntree_limit = getattr(self, "best_ntree_limit", 0)
            
            y_pred_vl = clf.predict(xgb.DMatrix(X_train[valid_index,:], 
                                                feature_names=feature_names))
            
            # for train data, we scale up the number of rounds, i.e., we consider that the 
            # validation set size was 1/num_splits of the training data size. Essentially
            # what we report as the optimal number of boosting trees is for a data with 
            # the size of the training data
            
            best_num_estimators[i] = int(clf.best_ntree_limit / (1 - 1 / num_splits))
#           clf.setattr("best_ntree_limit", best_num_estimators[i])
            
            y_pred_tr = clf.predict(xgb.DMatrix(X_train[train_index,:], 
                                                feature_names=feature_names))
            
            
        else:
            clf = xgb.train(params, xg_train, 
                            num_boost_round = num_boost_rounds, 
                            evals=[(xg_train, "train"), (xg_valid, "eval")], 
                            verbose_eval=False, 
                            evals_result=progress)     
            best_num_estimators[i] = num_boost_rounds
            y_pred_tr = clf.predict(xgb.DMatrix(X_train[train_index,:], 
                                                feature_names=feature_names),  
                                                ntree_limit=0)
            y_pred_vl = clf.predict(xgb.DMatrix(X_train[valid_index,:], 
                                                feature_names=feature_names), 
                                                ntree_limit=0)
        
        train_scores[i] = average_precision_score(y_train[train_index], y_pred_tr)
        valid_scores[i] = average_precision_score(y_train[valid_index], y_pred_vl)
        
        if save_fold_preds:
            fold_preds['train'].append([y_train[train_index], y_pred_tr])
            fold_preds['eval'].append([y_train[valid_index], y_pred_vl])

        
        if spit_out_fold_results:
            
            print(f"\n Repeat {rep}, Fold {fold} -", 
                  f"PR-AUC tr = {train_scores[i]:<.3f},", 
                  f"PR-AUC vl = {valid_scores[i]:<.3f}",
                  f"(diff = {train_scores[i] - valid_scores[i]:<.4f})",
                  )
            if early_stop_rounds is not None:
                print(f" best number of boosting rounds tr = {best_num_estimators[i]:<.0f}")
                
        eval_results.append(progress)
        
    # End of each repeat
    if spit_out_summary:
        
        print(f"\nSummary:\n", 
              f"mean PR-AUC training =  {np.average(train_scores):<.3f}\n",
              f"mean PR-AUC validation = {np.average(valid_scores):<.3f}\n",
              f"mean PR-AUC difference = {np.average(train_scores-valid_scores):<.4f}"
             )
        if early_stop_rounds is not None:
            print(f" average number of boosting rounds tr = {np.average(best_num_estimators):<.0f}")
    out = [(np.average(x), np.std(x)) for x in [train_scores, valid_scores, best_num_estimators]]
    return out, eval_results, fold_preds, clf
    
def cv_search_params(X_data, y_data, param_dict, feature_names, cv, 
                     objective_func='binary:logistic', eval_metric='aucpr', 
                     spit_out_fold_results=False, spit_out_summary=True, 
                     num_boost_rounds=Num_Boosting_Rounds, 
                     early_stop_rounds=None, random_grid_search = False, 
                     save_fold_preds=False,
                     num_random_search_candidates=10):
    """
    Returns the cv_summary() for all the combinations of the given parameter dictionary
    """
    
    search_results = []
    param_values = [
        x if (isinstance(x, list) or isinstance(x, type(np.linspace(1,2,2))))
          else [x] for x in param_dict.values()
                 ]
    param_dict = dict(zip(tuple(param_dict.keys()), param_values))
    num_search_candidates = len(list(product(*param_values)))
    all_search_params = []
    num_rand_search_candidates = num_random_search_candidates
    random_sample_ids = random.sample(range(num_search_candidates), num_rand_search_candidates)
    all_search_params = []
    for i, search_params in enumerate(product(*param_values)):
        current_params = dict(zip(tuple(param_dict.keys()), search_params))
        all_search_params.append(current_params)
    if random_grid_search:
        all_search_params = list(np.array(all_search_params)[random_sample_ids])
    evals = [[] for i in range(len(all_search_params))]
    print(f"Grid search started at {datetime.now()}")   
    print(f"Total number of hyperparameter candidates = {len(evals)}")     
    for i, search_params in enumerate(all_search_params):
        start_time = datetime.now()
        current_params = search_params
        print(f'\nCV {i+1} on:\n')
        pprint({k: v for k, v in current_params.items() if len(param_dict[k])>1})
        print('\n started!')
        results, evals[i], _, _ = cv_summary(X_data, y_data, 
                                             current_params, feature_names, 
                                             cv, objective_func=objective_func, 
                                             eval_metric=eval_metric, 
                                             spit_out_fold_results=spit_out_fold_results, 
                                             spit_out_summary=spit_out_summary, 
                                             early_stop_rounds=early_stop_rounds, 
                                             num_boost_rounds=num_boost_rounds)
        end_time = datetime.now()
        time_taken = f"{(end_time-start_time).total_seconds():.2f}"
        if len(evals)>1:
            print(f'CV {i+1} ended! (took {time_taken} seconds)')
        else:
            print(f'CV ended! (took {time_taken} seconds)')
        [(tr_avg, tr_std), 
        (val_avg, val_std), 
        (numtrees_avg, numtrees_std)] = results
        search_results.append([current_params, 
                               tr_avg, tr_std, 
                               val_avg, val_std, 
                               numtrees_avg, numtrees_std, 
                               tr_avg-val_avg, time_taken])
        
    print(f"Grid search ended at {datetime.now()}")  
    search_df = pd.DataFrame(search_results, 
                             columns=['current_params', 'tr_avg', 'tr_std', 
                                      'val_avg', 'val_std', 'numtrees_avg', 
                                      'numtrees_std', 'diff', 'time_taken'])
    
    return search_df, evals</code></pre></div>
              <h4><a class="header_arg" id="hyper_param_groups"></a>Dividing the hyperparameters into orthogonal groups</h4>
              <p>
                Next, we define a hyperparameter dictionary and update it with some initial parameters for the model:
              </p>
<pre class="prettyprint lang-language"><code class="language-python">cur_params = {
    'max_depth': 3,
    'min_child_weight': 5,
    'subsample': 1,
    'colsample_bytree': 1,
    'colsample_bylevel': 1,
    'alpha': 1,
    'gamma': 1,
    'scale_pos_weight': 1,
    'learning_rate': 2e-3,
}</code></pre>
              
              <p>
                We then define the parameter groups that are going to be the target of grid search:
              </p>
<pre class="prettyprint lang-language"><code class="language-python"># current xgb parameters
param_group_1 = {'max_depth': [3, 4], 'min_child_weight': [1, 10, 20, 30, 40]}
param_group_2 = {'subsample': np.linspace(0.1, 1, 5), 
                 'colsample_bytree': np.linspace(0.1, 1, 5), 
                 'colsample_bylevel': np.linspace(0.1, 1, 5)}
param_group_3 = {'alpha': np.logspace(-6, 3, 4), 'gamma': np.linspace(1, 9, 5)}
param_group_4 = {'scale_pos_weight': [0.5, 1, 2, 5, 10, 20, 50, 100, 500, 1000]}
param_group_5 = {'learning_rate': np.logspace(-4, -2, 11)}</code></pre>
              <h4><a class="header_arg" id="gs1"></a>Gridsearch for parameter group 1: <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;"> max_depth </code> and  <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;"> min_child_weight </code></h4>
              <p>
                We update the <code>cur_params</code> before performing the hyperparameter search. Also, it's a good practice to save the grid search results to minimize the chances of data loss. We use <code class="method">.to_pickle()</code> method of <code class="library">pandas</code> to save the resulting <code class="object">DataFrame</code>.
              </p>
              <pre class="prettyprint lang-language"><code class="language-python">search_params = param_group_1
cur_params.update(search_params)
pprint(cur_params)</code></pre>
              <pre>    {'alpha': 1,
     'colsample_bylevel': 1,
     'colsample_bytree': 1,
     'gamma': 1,
     'learning_rate': 0.002,
     'max_depth': [3, 4],
     'min_child_weight': [1, 10, 20, 30, 40],
     'scale_pos_weight': 1,
     'subsample': 1}</pre>
              
<pre class="prettyprint lang-language"><code class="language-python">tmp_df, evals = cv_search_params(X_train, y_train, cur_params, all_features, rskf)
tmp_df.to_pickle('hyperparams_round1.pkl')</code></pre>
              <pre>   Grid search on:

{'alpha': 1,
 'colsample_bylevel': 1,
 'colsample_bytree': 1,
 'gamma': 1,
 'learning_rate': 0.002,
 'max_depth': [3, 4],
 'min_child_weight': [1, 10, 20, 30, 40],
 'scale_pos_weight': 1,
 'subsample': 1}

started at 2021-06-04 12:41:46.384232
Total number of hyperparameter candidates = 10

CV 1 on:

{'max_depth': 3, 'min_child_weight': 1}

 started!

 Repeat 1, Fold 1 - PR-AUC tr = 0.816, PR-AUC vl = 0.752 (diff = 0.0639)
best number of boosting rounds tr = 386

 Repeat 1, Fold 2 - PR-AUC tr = 0.871, PR-AUC vl = 0.813 (diff = 0.0584)
best number of boosting rounds tr = 5289

 Repeat 1, Fold 3 - PR-AUC tr = 0.823, PR-AUC vl = 0.789 (diff = 0.0336)
best number of boosting rounds tr = 1209

...

Repeat 5, Fold 2 - PR-AUC tr = 0.762, PR-AUC vl = 0.739 (diff = 0.0232)
best number of boosting rounds tr = 1397

 Repeat 5, Fold 3 - PR-AUC tr = 0.708, PR-AUC vl = 0.742 (diff = -0.0342)
best number of boosting rounds tr = 838

 Repeat 5, Fold 4 - PR-AUC tr = 0.700, PR-AUC vl = 0.683 (diff = 0.0165)
best number of boosting rounds tr = 1110

Summary:
 mean PR-AUC training =  0.728
 mean PR-AUC validation = 0.718
 mean PR-AUC difference = 0.0094
CV 10 ended! (took 319.31 seconds)
Grid search ended at 2021-06-04 14:17:32.954835
​</pre>
              <pre class="prettyprint lang-language"><code class="language-python">tmp_df</code></pre>
<table class="table table-striped">
  <thead>
    <tr style="text-align: left;">
      <th></th>
      <th>current_params</th>
      <th>tr_avg</th>
      <th>tr_std</th>
      <th>val_avg</th>
      <th>val_std</th>
      <th>numtrees_avg</th>
      <th>numtrees_std</th>
      <th>diff</th>
      <th>time_taken</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">0</th>
      <td>{'max_depth': 3, 'min_child_weight': 1, 'subsa...</td>
      <td>0.847</td>
      <td>0.025</td>
      <td>0.810</td>
      <td>0.039</td>
      <td>3163.150</td>
      <td>1663.705</td>
      <td>0.037</td>
      <td>696.55</td>
    </tr>
    <tr>
      <th scope="row">1</th>
      <td>{'max_depth': 3, 'min_child_weight': 10, 'subs...</td>
      <td>0.848</td>
      <td>0.018</td>
      <td>0.816</td>
      <td>0.032</td>
      <td>4030.550</td>
      <td>1415.623</td>
      <td>0.032</td>
      <td>857.18</td>
    </tr>
    <tr>
      <th scope="row">2</th>
      <td>{'max_depth': 3, 'min_child_weight': 20, 'subs...</td>
      <td>0.808</td>
      <td>0.024</td>
      <td>0.779</td>
      <td>0.051</td>
      <td>2194.450</td>
      <td>1672.490</td>
      <td>0.029</td>
      <td>518.94</td>
    </tr>
    <tr>
      <th scope="row">3</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.784</td>
      <td>0.017</td>
      <td>0.767</td>
      <td>0.040</td>
      <td>1859.400</td>
      <td>1292.788</td>
      <td>0.017</td>
      <td>452.01</td>
    </tr>
    <tr>
      <th scope="row">4</th>
      <td>{'max_depth': 3, 'min_child_weight': 40, 'subs...</td>
      <td>0.728</td>
      <td>0.027</td>
      <td>0.718</td>
      <td>0.052</td>
      <td>1168.550</td>
      <td>277.548</td>
      <td>0.009</td>
      <td>319.31</td>
    </tr>
    <tr>
      <th scope="row">5</th>
      <td>{'max_depth': 4, 'min_child_weight': 1, 'subsa...</td>
      <td>0.874</td>
      <td>0.042</td>
      <td>0.817</td>
      <td>0.037</td>
      <td>3179.400</td>
      <td>1812.708</td>
      <td>0.057</td>
      <td>783.87</td>
    </tr>
    <tr>
      <th scope="row">6</th>
      <td>{'max_depth': 4, 'min_child_weight': 10, 'subs...</td>
      <td>0.844</td>
      <td>0.026</td>
      <td>0.806</td>
      <td>0.050</td>
      <td>3240.800</td>
      <td>1910.967</td>
      <td>0.038</td>
      <td>769.04</td>
    </tr>
    <tr>
      <th scope="row">7</th>
      <td>{'max_depth': 4, 'min_child_weight': 20, 'subs...</td>
      <td>0.810</td>
      <td>0.025</td>
      <td>0.780</td>
      <td>0.051</td>
      <td>2319.850</td>
      <td>1727.073</td>
      <td>0.030</td>
      <td>574.93</td>
    </tr>
    <tr>
      <th scope="row">8</th>
      <td>{'max_depth': 4, 'min_child_weight': 30, 'subs...</td>
      <td>0.784</td>
      <td>0.018</td>
      <td>0.767</td>
      <td>0.040</td>
      <td>1858.900</td>
      <td>1292.927</td>
      <td>0.017</td>
      <td>455.43</td>
    </tr>
    <tr>
      <th scope="row">9</th>
      <td>{'max_depth': 4, 'min_child_weight': 40, 'subs...</td>
      <td>0.728</td>
      <td>0.027</td>
      <td>0.718</td>
      <td>0.052</td>
      <td>1168.550</td>
      <td>277.548</td>
      <td>0.009</td>
      <td>319.31</td>
    </tr>
  </tbody>
</table>
              <p>
                We can remove the rows where the difference between the training and validation performance is significant. I take the <strong>threshold</strong> for this difference to be <strong>2%</strong>, meaning that <em>we disregard the parameters that cause a difference in performance $>2\%$ between training and validation sets</em>. For the rest of this project, we will refer to this threshold as the <a id="acceptance_threshold" class="definition"></a><em><strong>acceptance threshold</strong></em> (<em>not to be confused with the precision-recall threshold</em>) and to this condition as the <a id="acceptance_condition" class="definition"><a><em><strong>acceptance condition</a></strong></em>. This reduces the chances that the final model parameters cause overfitting. At the same time, we sort the output by <code>val_avg</code> which is the mean <strong>PR-AUC</strong> over the validation sets. 
              </p>
              <pre class="prettyprint lang-language"><code class="language-python">acceptance_threshold=0.02
cur_params = param_df[param_df['diff']&#60;acceptance_threshold] \
    .sort_values('val_avg', ascending=False)</code></pre>
<table class="table table-striped">
  <thead>
    <tr style="text-align: left;">
      <th></th>
      <th>current_params</th>
      <th>tr_avg</th>
      <th>tr_std</th>
      <th>val_avg</th>
      <th>val_std</th>
      <th>numtrees_avg</th>
      <th>numtrees_std</th>
      <th>diff</th>
      <th>time_taken</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">3</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.784</td>
      <td>0.017</td>
      <td>0.767</td>
      <td>0.040</td>
      <td>1859.400</td>
      <td>1292.788</td>
      <td>0.017</td>
      <td>452.01</td>
    </tr>
    <tr>
      <th scope="row">8</th>
      <td>{'max_depth': 4, 'min_child_weight': 30, 'subs...</td>
      <td>0.784</td>
      <td>0.018</td>
      <td>0.767</td>
      <td>0.040</td>
      <td>1858.900</td>
      <td>1292.927</td>
      <td>0.017</td>
      <td>455.43</td>
    </tr>
    <tr>
  </tbody>
</table>
              <p>
                <code class="arg">max_depth=3</code> and <code class="arg">min_child_weight=30</code> gives a reasonable PR-AUC while ensuring that the model's performance on the test data remains close to that of the training data. Note that using <code class="arg">max_depth=4</code> gives the same performance as <code class="arg">max_depth=3</code> so we move forward with the latter to reduce the complexity of the model. Next, we examine the second hyperparameter group.
              </p>
              <h4><a class="header_arg" id="bias-variance"></a>Acceptance threshold in the context of bias-variance trade-off</h4>
              <p>
                We can think of <a href="#acceptance_condition">acceptance condition</a> in the context of <strong>bias-variance trade-off</strong>. 
              </p>
              <p>
                We say a model has a <strong>high bias</strong> if it fails to use all the data in the observations. Such model relies mostly on the general information without taking into account the specifics. In contrast, we say a model has a <strong>high variance</strong> if it over-use theinformation in the data, i.e., relies too much on the specific data that is being trained on. Such model usually fails to generalize what it has learnt and reproduce its high performance when predicting the outcome for new, <em>unseen data</em>.
              </p>              
              <p>
                We can demonstrate this using an example relevant to our problem. The table below shows <code>aucpr</code> for two models I and II. When comparing the two models, model I exhibits a very high <code>aucpr</code> on the training data (<strong>low bias</strong>) but a significant difference in performance when tested on the test data (<strong>high variance</strong>); Model II on the other hand gives a comparatively lower <code>aucpr</code> on both the training and test data (<strong>high bias</strong>) but the model's performance on both datasets is quite similar (<strong>low variance</strong>)
              </p>
              <table class="table table-borderless">
                <thead>
                  <tr style="text-align: center;">
                    <th rowspan="3" style="background-color: white;"></th>
                    <th colspan="2" style="background-color: #fce986;">Model I</th>
                    <th colspan="2" style="background-color: #fcf4c5;">Model II</th>
                  </tr>
                  <tr style="text-align: center;">
                    <th>train</th>
                    <th>test</th>
                    <th>train</th>
                    <th>test</th>
                  </tr>
                  <tr style="text-align: center;">
                    <th>0.99</th>
                    <th style="background-color: #ffe4de;">0.8</th>
                    <th>0.75</th>
                    <th style="background-color: #ffe4de;">0.73</th>
                  </tr>
                  <tr style="text-align: center;">
                    <th style="background-color: #fce986;">Bias</th>
                    <th colspan="2" style="background-color: #faeeeb;">low</th>
                    <th colspan="2" style="background-color: #ff957a;">high</th>
                  </tr>
                  <tr style="text-align: center;">
                    <th style="background-color: #fcf4c5;">Variance</th>
                    <th colspan="2" style="background-color: #ff957a;">high</th>
                    <th colspan="2" style="background-color: #faeeeb;">low</th>
                  </tr>
                </thead>
              </table>
              <p>
                <strong>high bias </strong> and <strong>high variance</strong> both lead to errors in model performance. Ideally we would want the model to have low bias (less error on training data) and low variance (less difference between the model performance between the train and testing data). <strong>Bias-variance trade-off</strong> is the <strong>sweet spot</strong> where the model performs between the errors introduced by the bias and the variance. In this project, I used <a href="#acceptance_condition">acceptance condition</a> to set a value for <strong>Bias-variance trade-off</strong>; however, depending on the business goal, one may use a totally different criteria to find the best model hyperparameters.
              </p>
              <h4><a class="header_arg" id="gs2"></a>Gridsearch for parameter group 2: <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">colsample_bylevel</code>, <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">colsample_bytree</code>, and  <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">subsample</code></h4>
              <p>
                We update <code>cur_params</code> using the parameters obtained in previous step.
              </p>
              <pre class="prettyprint lang-language"><code class="language-python">cur_params.update({'max_depth': 3, 'min_child_weight': 30})
search_params = param_group_2
cur_params.update(search_params)
pprint(cur_params)</code></pre>
              <pre>{'alpha': 1,
 'colsample_bylevel': array([0.1  , 0.325, 0.55 , 0.775, 1.   ]),
 'colsample_bytree': array([0.1  , 0.325, 0.55 , 0.775, 1.   ]),
 'gamma': 1,
 'learning_rate': 0.002,
 'max_depth': 3,
 'min_child_weight': 30,
 'scale_pos_weight': 1,
 'subsample': array([0.1  , 0.325, 0.55 , 0.775, 1.   ])}</pre>
<pre class="prettyprint lang-language"><code class="language-python">tmp_df, evals = cv_search_params(X_train, y_train, cur_params, all_features, rskf)</code></pre>
              <pre>   Grid search started at 2021-05-24 10:48:53.507958
   Total number of hyperparameter candidates = 125
   
   CV 1 on:

{'subsample': 0.1, 'colsample_bylevel': 0.1, 'colsample_bytree': 0.1}

 started!

Summary:
   
   
  mean PR-AUC training =  0.211
  mean PR-AUC validation = 0.197
  mean PR-AUC difference = 0.014
  best number of boosting rounds = 703
 CV 1 ended! (took 191.91 seconds)

    ...
  
   CV 125 on:

{'subsample': 1.0, 'colsample_bylevel': 1.0, 'colsample_bytree': 1.0}

 started!

Summary:
 mean PR-AUC training =  0.784
 mean PR-AUC validation = 0.767
 mean PR-AUC difference = 0.0171
 best number of boosting rounds = 1859
CV 125 ended! (took 475.21 seconds)

   Grid search ended at 2021-05-24 12:27:25.227479</pre>
              <pre class="prettyprint lang-language"><code class="language-python">tmp_df[tmp_df['diff']&#60;acceptance_threshold].sort_values('val_avg', ascending=False)</code></pre>
<table class="table table-striped">
  <thead>
    <tr style="text-align: left;">
      <th>index</th>
      <th>current_params</th>
      <th>tr_avg</th>
      <th>tr_std</th>
      <th>val_avg</th>
      <th>val_std</th>
      <th>numtrees_avg</th>
      <th>numtrees_std</th>
      <th>diff</th>
      <th>time_taken</th>
    </tr>
  </thead>
<tbody>
    <tr>
      <th scope="row">124</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.784</td>
      <td>0.017</td>
      <td>0.767</td>
      <td>0.033</td>
      <td>1859.600</td>
      <td>1409.712</td>
      <td>0.016</td>
      <td>475.21</td>
    </tr>
    <tr>
      <th scope="row">119</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.777</td>
      <td>0.014</td>
      <td>0.765</td>
      <td>0.037</td>
      <td>547.000</td>
      <td>536.632</td>
      <td>0.011</td>
      <td>207.12</td>
    </tr>
    <tr>
      <th scope="row">123</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.775</td>
      <td>0.010</td>
      <td>0.761</td>
      <td>0.039</td>
      <td>214.250</td>
      <td>229.384</td>
      <td>0.014</td>
      <td>146.70</td>
    </tr>
    <tr>
      <th scope="row">118</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.776</td>
      <td>0.015</td>
      <td>0.760</td>
      <td>0.041</td>
      <td>691.100</td>
      <td>889.920</td>
      <td>0.015</td>
      <td>235.01</td>
    </tr>
    <tr>
      <th scope="row">114</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.777</td>
      <td>0.016</td>
      <td>0.760</td>
      <td>0.045</td>
      <td>1015.400</td>
      <td>1382.065</td>
      <td>0.017</td>
      <td>292.21</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th scope="row">5</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.211</td>
      <td>0.039</td>
      <td>0.211</td>
      <td>0.055</td>
      <td>862.600</td>
      <td>364.775</td>
      <td>-0.000</td>
      <td>215.35</td>
    </tr>
    <tr>
      <th scope="row">10</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.203</td>
      <td>0.037</td>
      <td>0.210</td>
      <td>0.068</td>
      <td>1018.400</td>
      <td>510.480</td>
      <td>-0.007</td>
      <td>239.26</td>
    </tr>
    <tr>
      <th scope="row">1</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.211</td>
      <td>0.052</td>
      <td>0.197</td>
      <td>0.062</td>
      <td>703.450</td>
      <td>378.373</td>
      <td>0.014</td>
      <td>191.28</td>
    </tr>
    <tr>
      <th scope="row">2</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.211</td>
      <td>0.052</td>
      <td>0.197</td>
      <td>0.062</td>
      <td>703.450</td>
      <td>378.373</td>
      <td>0.014</td>
      <td>191.46</td>
    </tr>
    <tr>
      <th scope="row">0</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.211</td>
      <td>0.052</td>
      <td>0.197</td>
      <td>0.062</td>
      <td>703.450</td>
      <td>378.373</td>
      <td>0.014</td>
      <td>191.91</td>
    </tr>
  </tbody>
</table>
<p>125 rows × 9 columns</p>
              <p>
                Note that all the hyperparameter candidates satisfy the <a href="#acceptance_condition">acceptance condition</a>. We can query the first 10 hyperparameters with the best average PR-AUC on the validation data using:
              </p>
              <pre class="prettyprint lang-language"><code class="language-python">[list(map(tmp_df[tmp_df['diff']&#60;acceptance_threshold] \
          .sort_values('val_avg', ascending=False)['current_params'] \
          .iloc[x] \
          .get,
         ['subsample', 'colsample_bytree', 'colsample_bylevel']
         )
     ) for x in range(10)
]</code></pre>
              <pre>[[1.0, 1.0, 1.0],
 [1.0, 0.775, 1.0],
 [1.0, 1.0, 0.775],
 [1.0, 0.775, 0.775],
 [1.0, 0.55, 1.0],
 [1.0, 1.0, 0.325],
 [1.0, 1.0, 0.55],
 [1.0, 0.775, 0.55],
 [1.0, 0.55, 0.775],
 [0.775, 1.0, 1.0]]</pre>
              <p>
                The results suggest that <code>colsample_bylevel</code> doesn't influenece the accuracy of the predictions. We can take a look at the evolution of PR-AUC for <code>colsample_bylevel=1</code> and different values of <code>subsample</code> and <code>colsample_bytree</code> in our grid-search space.
              </p>
                <button class="btn btn-primary plus-minus-code collapsed" type="button" data-toggle="collapse" data-target="#fig2code" aria-expanded="false" aria-controls="fig2code">
                </button>  
                <div class="collapse" id="fig2code" aria-expanded="false" style="height: 0px;"><pre class="prettyprint lang-language"><code class="language-python">fig = plt.figure(figsize=(10,10))
fig.tight_layout()
ax = fig.add_subplot(111, projection='3d')
ax.view_init(elev=45, azim=-135)
params = ['subsample', 'colsample_bytree', 'colsample_bylevel']
param_values = np.linspace(0.1,1,5)
indices = [x for x in range(len(tmp_df)) if tmp_df['current_params'] \
                   .iloc[x] \
                   .get('colsample_bylevel')==1]   
_xx, _yy = np.meshgrid(param_values, param_values)
width = depth = 0.225
xdata, ydata = _xx.ravel()-width/2, _yy.ravel()-depth/2

zdata = tmp_df['val_avg'] \
             .iloc[indices] \
             .values \
             .reshape(xdata.shape)

bottom = np.zeros_like(zdata)
# Get desired colormap - you can change this!
cmap = cm.get_cmap('jet') 
rgba = [cmap(k) for k in zdata] 
ax.bar3d(xdata, ydata, bottom, width, depth, zdata, color=rgba, zsort='max',shade=True)
ax.set_xlim([min(xdata)+0.03, max(xdata)+width])
ax.set_ylim([min(ydata)+0.03, max(ydata)+depth])
ax.set_zlim([0, 1])
ax.set_xticks(param_values)
ax.set_yticks(param_values)
plt.title("Average PR-AUC for different values of subsample \nand colsample_bytree for colsample_bytree=1", fontsize=20)
plt.ylabel("subsample", labelpad=20)
plt.xlabel("colsample_bylevel", labelpad=20)
ax.grid(False)
plt.savefig("paramgr")
plt.show()</code></pre></div>
              <div style="text-align:center">
                 <a id="fig3" style="font-size: 1.5em;">Figure 3</a>
              </div>
              
              <p class="text-align: center;">
                <img src="output_3.png" style="width: 70%; display: block; margin: 10px auto 20px;"/>
              </p>
              <p>
                <a href="#fig3">Figure 3</a> shows that the variation of <code>subsample</code> has the most significant effect on the accuracy of the model.
              </p>
              <p>
                Finally, we set the first row of the resulting <code class="object">DataFrame</code> as the initial hyperparameter for the next round of grid search.
              </p>
              <pre class="prettyprint lang-language"><code class="language-python">cur_params = tmp_df[tmp_df['diff']&#60;acceptance_threshold] \
                .sort_values('val_avg', ascending=False)['current_params'] \
                .iloc[0]
pprint(cur_params)</code></pre>
              <pre>{'alpha': 1, 
 'colsample_bylevel': 1.0, 
 'colsample_bytree': 1.0, 
 'gamma': 1, 
 'learning_rate': 0.002, 
 'max_depth': 3, 
 'min_child_weight': 30, 
 'scale_pos_weight': 1, 
 'subsample': 1.0}</pre>
              <h4><a class="header_arg" id="gs3"></a>Gridsearch for parameter group 3: <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">alpha</code> and <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">gamma</code></h4>
              <pre class="prettyprint lang-language"><code class="language-python">cur_params.update(param_group_3)
pprint(cur_params)</code></pre>
              <pre>{'alpha': array([1.e-06, 1.e-03, 1.e+00, 1.e+03]),
 'colsample_bylevel': 1.0,
 'colsample_bytree': 1.0,
 'gamma': array([1., 3., 5., 7., 9.]),
 'learning_rate': 0.002,
 'max_depth': 3,
 'min_child_weight': 30,
 'scale_pos_weight': 1,
 'subsample': 1.0}</pre>
<pre class="prettyprint lang-language"><code class="language-python">tmp_df, evals = cv_search_params(X_train, y_train, cur_params, all_features, rskf)
tmp_df.to_pickle('hyperparams_round3.pkl')</code></pre><pre>Grid search on:

{'alpha': array([1.e-06, 1.e-03, 1.e+00, 1.e+03]),
 'colsample_bylevel': 1.0,
 'colsample_bytree': 1.0,
 'eval_metric': 'aucpr',
 'gamma': array([1., 3., 5., 7., 9.]),
 'learning_rate': 0.002,
 'max_depth': 3,
 'min_child_weight': 30,
 'objective': 'binary:logistic',
 'scale_pos_weight': 1.0,
 'subsample': 1.0,
 'tree_method': 'gpu_hist'}

started at 2021-06-07 12:26:51.336554
Total number of hyperparameter candidates = 20

CV 1 on:

{'alpha': 1e-06, 'gamma': 1.0}

 started!

Summary:
 mean PR-AUC training =  0.781
 mean PR-AUC validation = 0.762
 mean PR-AUC difference = 0.0192
 average number of boosting rounds tr = 1450
CV 1 ended! (took 384.88 seconds)

</pre>
<button class="btn btn-primary plus-minus-output collapsed" type="button" data-toggle="collapse" data-target="#collapseCVOutput3" aria-expanded="false" aria-controls="collapseCVOutput3">
</button>  
<div class="collapse" id="collapseCVOutput3" aria-expanded="false" style="height: 0px;">
    <pre >CV 2 on:

{'alpha': 1e-06, 'gamma': 3.0}

 started!

Summary:
 mean PR-AUC training =  0.788
 mean PR-AUC validation = 0.773
 mean PR-AUC difference = 0.0157
 average number of boosting rounds tr = 2000
CV 2 ended! (took 471.16 seconds)

CV 3 on:

{'alpha': 1e-06, 'gamma': 5.0}

 started!

Summary:
 mean PR-AUC training =  0.784
 mean PR-AUC validation = 0.772
 mean PR-AUC difference = 0.0129
 average number of boosting rounds tr = 1809
CV 3 ended! (took 432.78 seconds)

CV 4 on:

{'alpha': 1e-06, 'gamma': 7.0}

 started!

Summary:
 mean PR-AUC training =  0.782
 mean PR-AUC validation = 0.764
 mean PR-AUC difference = 0.0177
 average number of boosting rounds tr = 1881
CV 4 ended! (took 440.80 seconds)

CV 5 on:

{'alpha': 1e-06, 'gamma': 9.0}

 started!

Summary:
 mean PR-AUC training =  0.778
 mean PR-AUC validation = 0.759
 mean PR-AUC difference = 0.0187
 average number of boosting rounds tr = 1506
CV 5 ended! (took 375.05 seconds)

CV 6 on:

{'alpha': 0.001, 'gamma': 1.0}

 started!

Summary:
 mean PR-AUC training =  0.782
 mean PR-AUC validation = 0.763
 mean PR-AUC difference = 0.0190
 average number of boosting rounds tr = 1516
CV 6 ended! (took 395.33 seconds)

CV 7 on:

{'alpha': 0.001, 'gamma': 3.0}

 started!

Summary:
 mean PR-AUC training =  0.788
 mean PR-AUC validation = 0.773
 mean PR-AUC difference = 0.0158
 average number of boosting rounds tr = 2023
CV 7 ended! (took 474.74 seconds)

CV 8 on:

{'alpha': 0.001, 'gamma': 5.0}

 started!

Summary:
 mean PR-AUC training =  0.785
 mean PR-AUC validation = 0.772
 mean PR-AUC difference = 0.0130
 average number of boosting rounds tr = 2004
CV 8 ended! (took 463.54 seconds)

CV 9 on:

{'alpha': 0.001, 'gamma': 7.0}

 started!

Summary:
 mean PR-AUC training =  0.782
 mean PR-AUC validation = 0.764
 mean PR-AUC difference = 0.0177
 average number of boosting rounds tr = 1880
CV 9 ended! (took 441.33 seconds)

CV 10 on:

{'alpha': 0.001, 'gamma': 9.0}

 started!

Summary:
 mean PR-AUC training =  0.778
 mean PR-AUC validation = 0.759
 mean PR-AUC difference = 0.0188
 average number of boosting rounds tr = 1506
CV 10 ended! (took 375.77 seconds)

CV 11 on:

{'alpha': 1.0, 'gamma': 1.0}

 started!

Summary:
 mean PR-AUC training =  0.784
 mean PR-AUC validation = 0.767
 mean PR-AUC difference = 0.0171
 average number of boosting rounds tr = 1859
CV 11 ended! (took 452.86 seconds)

CV 12 on:

{'alpha': 1.0, 'gamma': 3.0}

 started!

Summary:
 mean PR-AUC training =  0.784
 mean PR-AUC validation = 0.770
 mean PR-AUC difference = 0.0147
 average number of boosting rounds tr = 1930
CV 12 ended! (took 457.49 seconds)

CV 13 on:

{'alpha': 1.0, 'gamma': 5.0}

 started!

Summary:
 mean PR-AUC training =  0.779
 mean PR-AUC validation = 0.765
 mean PR-AUC difference = 0.0137
 average number of boosting rounds tr = 1604
CV 13 ended! (took 395.10 seconds)

CV 14 on:

{'alpha': 1.0, 'gamma': 7.0}

 started!

Summary:
 mean PR-AUC training =  0.780
 mean PR-AUC validation = 0.761
 mean PR-AUC difference = 0.0186
 average number of boosting rounds tr = 1687
CV 14 ended! (took 410.15 seconds)

CV 15 on:

{'alpha': 1.0, 'gamma': 9.0}

 started!

Summary:
 mean PR-AUC training =  0.776
 mean PR-AUC validation = 0.756
 mean PR-AUC difference = 0.0197
 average number of boosting rounds tr = 1556
CV 15 ended! (took 379.72 seconds)

CV 16 on:

{'alpha': 1000.0, 'gamma': 1.0}

 started!

Summary:
 mean PR-AUC training =  0.672
 mean PR-AUC validation = 0.672
 mean PR-AUC difference = -0.0001
 average number of boosting rounds tr = 420
CV 16 ended! (took 168.22 seconds)

CV 17 on:

{'alpha': 1000.0, 'gamma': 3.0}

 started!

Summary:
 mean PR-AUC training =  0.672
 mean PR-AUC validation = 0.672
 mean PR-AUC difference = -0.0001
 average number of boosting rounds tr = 420
CV 17 ended! (took 168.11 seconds)

CV 18 on:

{'alpha': 1000.0, 'gamma': 5.0}

 started!

Summary:
 mean PR-AUC training =  0.672
 mean PR-AUC validation = 0.672
 mean PR-AUC difference = -0.0001
 average number of boosting rounds tr = 420
CV 18 ended! (took 168.26 seconds)

CV 19 on:

{'alpha': 1000.0, 'gamma': 7.0}

 started!

Summary:
 mean PR-AUC training =  0.672
 mean PR-AUC validation = 0.672
 mean PR-AUC difference = -0.0001
 average number of boosting rounds tr = 420
CV 19 ended! (took 167.94 seconds)
</pre></div><pre>
CV 20 on:

{'alpha': 1000.0, 'gamma': 9.0}

 started!

Summary:
 mean PR-AUC training =  0.672
 mean PR-AUC validation = 0.672
 mean PR-AUC difference = -0.0001
 average number of boosting rounds tr = 420
CV 20 ended! (took 168.19 seconds)

Grid search ended at 2021-06-07 14:34:06.880364
</pre>
<pre class="prettyprint lang-language"><code class="language-python">tmp_df[tmp_df['diff']&#60;acceptance_threshold] \
    .sort_values('val_avg', ascending=False) \
    .head()</code></pre>
<table class="table table-striped">
  <thead>
    <tr style="text-align: left;">
      <th scope="row"></th>
      <th>current_params</th>
      <th>tr_avg</th>
      <th>tr_std</th>
      <th>val_avg</th>
      <th>val_std</th>
      <th>numtrees_avg</th>
      <th>numtrees_std</th>
      <th>diff</th>
      <th>time_taken</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">1</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.788</td>
      <td>0.017</td>
      <td>0.773</td>
      <td>0.033</td>
      <td>1999.550</td>
      <td>1415.553</td>
      <td>0.016</td>
      <td>471.16</td>
    </tr>
    <tr>
      <th scope="row">6</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.788</td>
      <td>0.017</td>
      <td>0.773</td>
      <td>0.033</td>
      <td>2022.600</td>
      <td>1409.712</td>
      <td>0.016</td>
      <td>474.74</td>
    </tr>
    <tr>
      <th scope="row">7</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.785</td>
      <td>0.018</td>
      <td>0.772</td>
      <td>0.035</td>
      <td>2003.700</td>
      <td>1534.701</td>
      <td>0.013</td>
      <td>463.54</td>
    </tr>
    <tr>
      <th scope="row">2</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.784</td>
      <td>0.017</td>
      <td>0.772</td>
      <td>0.034</td>
      <td>1809.200</td>
      <td>1205.948</td>
      <td>0.013</td>
      <td>432.78</td>
    </tr>
    <tr>
      <th scope="row">11</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.784</td>
      <td>0.018</td>
      <td>0.770</td>
      <td>0.036</td>
      <td>1929.900</td>
      <td>1284.865</td>
      <td>0.015</td>
      <td>457.49</td>
    </tr>
  </tbody>
</table>
                <p>
                  At this point, the resulting optimal parameters achieve extremely close results in terms of the model performance metric.
                </p>
<pre class="prettyprint lang-language"><code class="language-python">[list(map(tmp_df[tmp_df['diff']&#60;acceptance_threshold] \
          .sort_values('val_avg', ascending=False)['current_params'] \
          .iloc[x] \
          .get,
         ['alpha', 'gamma']
         )
     ) for x in range(5)
]</pre></code>
<pre>[[1e-06, 3.0], [0.001, 3.0], [0.001, 5.0], [1e-06, 5.0], [1.0, 3.0]]</pre>
                <p>
                  When looking at <code>val_avg</code> for PR-AUC, the first two sets of hyperparameters in the table demonstrate the same performance, hence, we choose the one with larger <code class="arg">alpha</code> (=<code>0.001</code>) as it leads to a more conservative model. Because of the way we find the best number of boosting rounds when we use early stopping, one might question whether we use enuogh number of boosting rounds? I decided to run the same grid search as above, this time withough early stopping and also setting the maximum number of boosting rounds to 10000. In addition, we keep track of the variation of <code>logloss</code> and <code>aucpr</code> for the training and validation data with the number of boosting rounds.
                </p>
<pre class="prettyprint lang-language"><code class="language-python">tmp_df_no_early, evals = cv_search_params(X_train, y_train, cur_params, all_features, rskf, 
                                 eval_metric=['logloss','aucpr'], 
                                 num_boost_rounds=10000, early_stop_rounds=None)
tmp_df_no_early.to_pickle(f'hyperparam3_10000.pkl')
with open(f'hyperparam3_10000_evals.pkl', 'wb') as pickle_file:
    pickle.dump(evals, pickle_file)</code></pre>
    <pre>
      Grid search on:

{'alpha': array([1.e-06, 1.e-03, 1.e+00, 1.e+03]),
 'colsample_bylevel': 1,
 'colsample_bytree': 1,
 'gamma': array([1., 3., 5., 7., 9.]),
 'learning_rate': 0.002,
 'max_depth': 3,
 'min_child_weight': 30,
 'scale_pos_weight': 1,
 'subsample': 1}

started at 2021-06-09 05:23:46.155894
Total number of hyperparameter candidates = 20

CV 1 on:

{'alpha': 1e-09, 'gamma': 1.0}

 started!

Summary:
 mean PR-AUC training =  0.815
 mean PR-AUC validation = 0.789
 mean PR-AUC difference = 0.0258
CV 1 ended! (took 1753.62 seconds)
</pre>
<button class="btn btn-primary plus-minus-output collapsed" type="button" data-toggle="collapse" data-target="#collapseCVOutput9" aria-expanded="false" aria-controls="collapseCVOutput9">
</button>  
<div class="collapse" id="collapseCVOutput9" aria-expanded="false" style="height: 0px;">
    <pre >
CV 2 on:

{'alpha': 1e-09, 'gamma': 3.0}

 started!

Summary:
 mean PR-AUC training =  0.811
 mean PR-AUC validation = 0.788
 mean PR-AUC difference = 0.0235
CV 2 ended! (took 1659.27 seconds)

CV 3 on:

{'alpha': 1e-09, 'gamma': 5.0}

 started!

Summary:
 mean PR-AUC training =  0.807
 mean PR-AUC validation = 0.785
 mean PR-AUC difference = 0.0220
CV 3 ended! (took 1573.70 seconds)

CV 4 on:

{'alpha': 1e-09, 'gamma': 7.0}

 started!

Summary:
 mean PR-AUC training =  0.805
 mean PR-AUC validation = 0.781
 mean PR-AUC difference = 0.0236
CV 4 ended! (took 1542.03 seconds)

CV 5 on:

{'alpha': 1e-09, 'gamma': 9.0}

 started!

Summary:
 mean PR-AUC training =  0.801
 mean PR-AUC validation = 0.777
 mean PR-AUC difference = 0.0237
CV 5 ended! (took 1503.45 seconds)

CV 6 on:

{'alpha': 1e-06, 'gamma': 1.0}

 started!

Summary:
 mean PR-AUC training =  0.815
 mean PR-AUC validation = 0.789
 mean PR-AUC difference = 0.0258
CV 6 ended! (took 1759.45 seconds)

CV 7 on:

{'alpha': 1e-06, 'gamma': 3.0}

 started!

Summary:
 mean PR-AUC training =  0.811
 mean PR-AUC validation = 0.788
 mean PR-AUC difference = 0.0235
CV 7 ended! (took 1674.50 seconds)

CV 8 on:

{'alpha': 1e-06, 'gamma': 5.0}

 started!

Summary:
 mean PR-AUC training =  0.807
 mean PR-AUC validation = 0.785
 mean PR-AUC difference = 0.0220
CV 8 ended! (took 1590.78 seconds)

CV 9 on:

{'alpha': 1e-06, 'gamma': 7.0}

 started!

Summary:
 mean PR-AUC training =  0.805
 mean PR-AUC validation = 0.781
 mean PR-AUC difference = 0.0236
CV 9 ended! (took 1560.62 seconds)

CV 10 on:

{'alpha': 1e-06, 'gamma': 9.0}

 started!

Summary:
 mean PR-AUC training =  0.801
 mean PR-AUC validation = 0.777
 mean PR-AUC difference = 0.0237
CV 10 ended! (took 1539.97 seconds)

CV 11 on:

{'alpha': 0.001, 'gamma': 1.0}

 started!

Summary:
 mean PR-AUC training =  0.815
 mean PR-AUC validation = 0.789
 mean PR-AUC difference = 0.0257
CV 11 ended! (took 1793.02 seconds)

CV 12 on:

{'alpha': 0.001, 'gamma': 3.0}

 started!

Summary:
 mean PR-AUC training =  0.811
 mean PR-AUC validation = 0.788
 mean PR-AUC difference = 0.0234
CV 12 ended! (took 1690.34 seconds)

CV 13 on:

{'alpha': 0.001, 'gamma': 5.0}

 started!

Summary:
 mean PR-AUC training =  0.807
 mean PR-AUC validation = 0.785
 mean PR-AUC difference = 0.0221
CV 13 ended! (took 1600.69 seconds)

CV 14 on:

{'alpha': 0.001, 'gamma': 7.0}

 started!

Summary:
 mean PR-AUC training =  0.805
 mean PR-AUC validation = 0.781
 mean PR-AUC difference = 0.0236
CV 14 ended! (took 1561.39 seconds)

CV 15 on:

{'alpha': 0.001, 'gamma': 9.0}

 started!

Summary:
 mean PR-AUC training =  0.801
 mean PR-AUC validation = 0.777
 mean PR-AUC difference = 0.0237
CV 15 ended! (took 1534.42 seconds)

CV 16 on:

{'alpha': 1.0, 'gamma': 1.0}

 started!

Summary:
 mean PR-AUC training =  0.813
 mean PR-AUC validation = 0.789
 mean PR-AUC difference = 0.0247
CV 16 ended! (took 1747.98 seconds)

CV 17 on:

{'alpha': 1.0, 'gamma': 3.0}

 started!

Summary:
 mean PR-AUC training =  0.809
 mean PR-AUC validation = 0.786
 mean PR-AUC difference = 0.0230
CV 17 ended! (took 1656.95 seconds)

CV 18 on:

{'alpha': 1.0, 'gamma': 5.0}

 started!

Summary:
 mean PR-AUC training =  0.806
 mean PR-AUC validation = 0.783
 mean PR-AUC difference = 0.0225
CV 18 ended! (took 1587.28 seconds)

CV 19 on:

{'alpha': 1.0, 'gamma': 7.0}

 started!

Summary:
 mean PR-AUC training =  0.803
 mean PR-AUC validation = 0.779
 mean PR-AUC difference = 0.0241
CV 19 ended! (took 1556.41 seconds)
</pre></div><pre>
CV 20 on:

{'alpha': 1.0, 'gamma': 9.0}

 started!

Summary:
 mean PR-AUC training =  0.800
 mean PR-AUC validation = 0.775
 mean PR-AUC difference = 0.0244
CV 20 ended! (took 1520.48 seconds)

Grid search ended at 2021-06-09 16:10:16.820294
    </pre>
    <h5><a class="header_arg" id="gs3_notes_1">Variation of <code>logloss</code> and <code>PR-AUC</code> for <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">alpha=0.001</code> and different values of <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">gamma</code></h5>
    <button class="btn btn-primary plus-minus-code collapsed" type="button" data-toggle="collapse" data-target="#fig4code" aria-expanded="false" aria-controls="fig4code"> 
      </button> 
      <div class="collapse" id="fig4code" aria-expanded="false" style="height: 0px;">
<pre class="prettyprint lang-language"><code class="language-python">fig = plt.figure(dpi=280, figsize=(16,8))
num_folds = 20
test_params = {'alpha': [1e-3], 'gamma':[1., 3., 5., 7., 9.]}
test_evals = evals[10:15]
metrics = ['logloss','aucpr']
num_cases = len(list(product(*test_params.values())))
colors=[plt.cm.terrain(int(i/num_cases*256)) for i,_ in enumerate(product(*test_params.values()))]
step=1000
num_trees=10000

for j,metric in enumerate(metrics):
    ax = fig.add_subplot(1,2,j+1)
    for i,(alpha,gamma) in enumerate(product(*test_params.values())):
        eval_tr = np.mean(np.asarray([test_evals[i][x]['train'][metric] for x in range(num_folds)]), axis=0)[::step]
        eval_val = np.mean(np.asarray([test_evals[i][x]['eval'][metric] for x in range(num_folds)]), axis=0)[::step]
        len_eval = len(np.mean(np.asarray([test_evals[i][x]['train'][metric] for x in range(num_folds)]), axis=0))
        x_data = np.arange(1,len(eval_tr)+1)*step
        ax.plot(x_data, eval_tr, 
                            marker='o', markersize=6, 
                            markerFaceColor='white', 
                            color=colors[i],
                            label=f"alpha={alpha}, gamma={gamma} (tr-{metric})")
        ax.plot(x_data, eval_val, 
                            marker='s', markersize=6, 
                            markerFaceColor='white', 
                            color=colors[i], linestyle=':',
                            label=f"alpha={alpha}, gamma={gamma} (val-{metric})")
    ax.set_xlabel('number of boosting rounds', fontsize=18)
    ax.set_ylabel(f'{metric}', fontsize=18, labelpad=15)
    legend_loc = {0:'upper', 1:'lower'}.get(j)
    ax.legend(loc=f"{legend_loc} right",fontsize=12)
plt.suptitle('variation of logloss and PR-AUC with the number of\n' +
             'boosting rounds for alpha=0.001 and gamma=[1,3,5,7,9]', 
            fontsize=22)
plt.show()</pre></code></div>
    
              <div style="text-align:center">
                 <a id="fig4" style="font-size: 1.5em;">Figure 4</a>
              </div>
              
              <p class="text-align: center;">
                <img src="round3_1.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
              </p> 
        <h5><a class="header_arg" id="gs3_notes_2">Variation of <code>logloss</code> and <code>PR-AUC</code> for <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">gamma=3</code> and different values of <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">alpha</code></h5>
    <button class="btn btn-primary plus-minus-code collapsed" type="button" data-toggle="collapse" data-target="#fig5code" aria-expanded="false" aria-controls="fig5code">
</button>  
<div class="collapse" id="fig5code" aria-expanded="false" style="height: 0px;">
<pre class="prettyprint lang-language"><code class="language-python"><pre >fig = plt.figure(dpi=280, figsize=(16,8))
num_folds = 20
test_params = {'alpha': np.logspace(-6,0,3), 'gamma':[3]}
test_evals = [evals[i] for i in [6,7,8]]
metrics = ['logloss','aucpr']
num_cases = len(list(product(*test_params.values())))
colors=[plt.cm.jet(int(i/num_cases*256)) for i,_ in enumerate(product(*test_params.values()))]
step=1000
num_trees=10000

for j,metric in enumerate(metrics):
    ax = fig.add_subplot(1,2,j+1)
    for i,(alpha,gamma) in enumerate(product(*test_params.values())):
        eval_tr = np.mean(np.asarray([test_evals[i][x]['train'][metric] for x in range(num_folds)]), axis=0)[::step]
        eval_val = np.mean(np.asarray([test_evals[i][x]['eval'][metric] for x in range(num_folds)]), axis=0)[::step]
        len_eval = len(np.mean(np.asarray([test_evals[i][x]['train'][metric] for x in range(num_folds)]), axis=0))
        x_data = np.arange(1,len(eval_tr)+1)*step
        ax.plot(x_data, eval_tr, 
                            marker='o', markersize=6, 
                            markerFaceColor='white', 
                            color=colors[i],
                            label=f"alpha={alpha}, gamma={gamma} (tr-{metric})")
        ax.plot(x_data, eval_val, 
                            marker='s', markersize=6, 
                            markerFaceColor='white', 
                            color=colors[i], linestyle=':',
                            label=f"alpha={alpha}, gamma={gamma} (val-{metric})")
    ax.set_xlabel('number of boosting rounds', fontsize=18)
    ax.set_ylabel(f'{metric}', fontsize=18, labelpad=15)
    legend_loc = {0:'upper', 1:'lower'}.get(j)
    ax.legend(loc=f"{legend_loc} right",fontsize=12)
plt.suptitle('variation of logloss and PR-AUC with the number of\n' +
             'boosting rounds for alpha=[1e-6,1e-3,1]  and gamma=3', 
            fontsize=22)
plt.show()</pre></code></div>
              <div style="text-align:center">
                 <a id="fig5" style="font-size: 1.5em;">Figure 5</a>
              </div>
              
              <p class="text-align: center;">
                <img src="round3_2.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
              </p> 
              <p>
                There are a few observations to make from Figures <a href="#fig4">4</a> and <a href="#fig5">5</a>.
                <ul>
                  <li>Unlike Pr-AUC, Logloss is insensitive to the variation of <code>alpha</code> and <code>gamma</code>. This is most likely due to the severe imbalance of the classes. While PR-AUC is sensitive to imbalance, logloss is about the <strong>model's confidence in predicting the correct class</strong>, be it positive or negative.</li>
                  <li><strong>Validation PR-AUC</strong> reaches its peak somewhere around 5000 trees followed by a slight reduction and then it plateaus. Train PR-AUC on the other hand is strictly increasing. This causes the gap between the training and validation PR-AUC to increase, i.e., overitting.</li>
                  <li>Model is more sensitive to the variation of <code>gamma</code> than <code>alpha</code>.</li>
                  <li>5000 seems to be a safe maximum for the number of boosting rounds.</li>
                </ul>
                We move on with <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">gamma=3</code> and <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">alpha=1e-3</code> to the next round of grid search.
              </p>
              
<pre class="prettyprint lang-language"><code class="language-python">cur_params = tmp_df[tmp_df['diff']&#60;threshold].sort_values('val_avg', ascending=False)['current_params'].iloc[1]</code></pre>
                <h4><a class="header_arg" id="gs4"></a>Gridsearch for parameter group 4: <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">scale_pos_weight</code></h4>
                <p>
                  <code class="arg">scale_pos_weight</code> controls the balance of positive and negative weights which is specifically useful when dealing with highly imbalanced cases. A typical value to consider is <code>sum(negative instances)/sum(positive instances)</code>, i.e.
                </p>
$$
\frac{\text{The number of observations with }\textbf{valid}\text{ transactions}}{\text{The number of observations with }\textbf{fraudulent}\text{ transactions}}
$$
                <pre class="prettyprint lang-language"><code class="language-python">from collections import Counter
counter = Counter(df['Class'])
print(f'Class distribution of the response variable: {counter}')
print(f'Majority and minority classes correspond to {100*counter[0]/(counter[0]+counter[1]):.3f}% ', 
      f'and {100*counter[1]/(counter[0]+counter[1]):.3f}% of the data, respectvively,', 
      f'\nwith positive-to-negative-ratio = {counter[0]/counter[1]:.1f}')</code></pre>
<pre>Class distribution of the response variable: Counter({0: 284315, 1: 492})
Majority and minority classes correspond to 99.827%  and 0.173% of the data, respectvively, 
with positive-to-negative-ratio = 577.9</pre>
                <p>
                  Therefore, we make sure that this ratio or a value close to it is included in our hyperparameter candidates for <code class="arg">scale_pos_weight</code>.
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">pprint(cur_params)</code></pre>
<pre>{'alpha': 0.001,
 'colsample_bylevel': 1,
 'colsample_bytree': 1,
 'eval_metric': 'aucpr',
 'gamma': 3,
 'learning_rate': 0.002,
 'max_depth': 3,
 'min_child_weight': 30,
 'objective': 'binary:logistic',
 'scale_pos_weight': 1,
 'subsample': 1,
 'tree_method': 'gpu_hist'}</pre>
                <p>
                  <strong>Note:</strong> the only reason to include <code class="arg">scale_pos_weight=0.5</code> in the parameter search is to observe the model behavior.
                </p>
                <pre class="prettyprint lang-language"><code class="language-python">search_params = param_group_4
cur_params.update(search_params)
pprint(cur_params)</code></pre>
                <pre>{'alpha': 0.001,
 'colsample_bylevel': 1,
 'colsample_bytree': 1,
 'gamma': 3,
 'learning_rate': 0.002,
 'max_depth': 3,
 'min_child_weight': 30,
 'scale_pos_weight': [0.5, 1, 2, 5, 10, 20, 50, 100, 500 ,1000],
 'subsample': 1}</pre>
                <pre class="prettyprint lang-language"><code class="language-python">tmp_df, evals = cv_search_params(X_train, y_train, cur_params, all_features, rskf)
tmp_df.to_pickle('hyperparams_round4.pkl')</code></pre>
<pre> Grid search started at 2021-05-31 14:23:33.726916
Total number of hyperparameter candidates = 10

CV 1 on:

{'scale_pos_weight': 0.5}

 started!

Summary:
 mean PR-AUC training =  0.729
 mean PR-AUC validation = 0.710
 mean PR-AUC difference = 0.0191
CV 1 ended! (took 178.08 seconds)
</pre>
<button class="btn btn-primary plus-minus-output collapsed" type="button" data-toggle="collapse" data-target="#collapseCVOutput4" aria-expanded="false" aria-controls="collapseCVOutput4">
</button>  
<div class="collapse" id="collapseCVOutput4" aria-expanded="false" style="height: 0px;">
    <pre >CV 2 on:

{'scale_pos_weight': 1.0}

 started!

Summary:
 mean PR-AUC training =  0.788
 mean PR-AUC validation = 0.772
 mean PR-AUC difference = 0.0161
CV 2 ended! (took 260.34 seconds)

CV 3 on:

{'scale_pos_weight': 2.0}

 started!

Summary:
 mean PR-AUC training =  0.815
 mean PR-AUC validation = 0.787
 mean PR-AUC difference = 0.0281
CV 3 ended! (took 312.25 seconds)

CV 4 on:

{'scale_pos_weight': 5.0}

 started!

Summary:
 mean PR-AUC training =  0.829
 mean PR-AUC validation = 0.803
 mean PR-AUC difference = 0.0281
CV 4 ended! (took 344.57 seconds)

CV 5 on:

{'scale_pos_weight': 10.0}

 started!

Summary:
 mean PR-AUC training =  0.832
 mean PR-AUC validation = 0.801
 mean PR-AUC difference = 0.0311
CV 5 ended! (took 299.70 seconds)

CV 6 on:

{'scale_pos_weight': 50.0}

 started!

Summary:
 mean PR-AUC training =  0.829
 mean PR-AUC validation = 0.801
 mean PR-AUC difference = 0.0281
CV 6 ended! (took 277.62 seconds)

CV 7 on:

{'scale_pos_weight': 100.0}

 started!

Summary:
 mean PR-AUC training =  0.762
 mean PR-AUC validation = 0.739
 mean PR-AUC difference = 0.0241
CV 7 ended! (took 213.36 seconds)

CV 8 on:

{'scale_pos_weight': 500.0}

 started!

Summary:
 mean PR-AUC training =  0.754
 mean PR-AUC validation = 0.731
 mean PR-AUC difference = 0.0231
CV 8 ended! (took 195.64 seconds)

CV 9 on:

{'scale_pos_weight': 10000.0}

 started!

Summary:
 mean PR-AUC training =  0.749
 mean PR-AUC validation = 0.715
 mean PR-AUC difference = 0.0341
CV 9 ended! (took 190.46 seconds)

</pre></div>
<pre>
CV 10 on:

{'scale_pos_weight': }

 started!

Summary:
 mean PR-AUC training =  0.759
 mean PR-AUC validation = 0.722
 mean PR-AUC difference = 0.0361
 
CV 10 ended! (took 448.14 seconds)

Grid search ended at 2021-05-31 15:21:49.174025</pre>
<pre class="prettyprint lang-language"><code class="language-python">tmp_df[tmp_df['diff']&#60;acceptance_threshold] \
    .sort_values('val_avg', ascending=False) \
    .head()</code></pre>
<table class="table table-striped">
  <thead>
    <tr style="text-align: left;">
            <th></th>
      <th>current_params</th>
      <th>tr_avg</th>
      <th>tr_std</th>
      <th>val_avg</th>
      <th>val_std</th>
      <th>numtrees_avg</th>
      <th>numtrees_std</th>
      <th>diff</th>
      <th>time_taken</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row">1</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.777</td>
      <td>0.014</td>
      <td>0.765</td>
      <td>0.037</td>
      <td>307.913</td>
      <td>301.897</td>
      <td>0.011</td>
      <td>0:03:26</td>
    </tr>
    <tr>
      <th scope="row">0</th>
      <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
      <td>0.702</td>
      <td>0.013</td>
      <td>0.690</td>
      <td>0.048</td>
      <td>91.575</td>
      <td>169.184</td>
      <td>0.012</td>
      <td>0:02:14</td>
    </tr>
  </tbody>
</table>

              <p>
                Note that only two of the search candidates for <code class="arg">scale_pos_weight</code> satisfy the <a href="#acceptance_condition">acceptance condition</a>. 
              </p>
<pre class="prettyprint lang-language"><code class="language-python">[x['scale_pos_weight'] for x in tmp_df[tmp_df['diff']&#60;acceptance_threshold] \
    ['current_params']]</code></pre>
<pre>[0.5, 1]</pre>
            <p>
              This is surprising because we expected a value close to <code>sum(negative instances)/sum(positive instances)</code> giving the best model performance. Looking at the variation of evaluation metric with might give us a hint about what might have gone wrong. Plotting the evolution of <code class="arg">eval_metric</code> with <code class="arg">scale_pos_weight</code> may reveal some more details regarding this discrepancy.
            </p>
<button class="btn btn-primary plus-minus-code collapsed" type="button" data-toggle="collapse" data-target="#fig6code" aria-expanded="false" aria-controls="fig6code">
</button>  
<div class="collapse" id="fig6code" aria-expanded="false" style="height: 0px;">
<pre class="prettyprint lang-language"><code class="language-python">fig = plt.figure(dpi=280, figsize=(8, 8))
ax = fig.add_subplot(111)
labels = []
x_data = [np.log10(x['scale_pos_weight']) for x in tmp_df['current_params']]
colors = ['dodgerblue', 'lightsalmon','forestgreen']
labs = ['train', 'validation']
avgs = ['tr_avg', 'val_avg']
stds = ['tr_std', 'val_std']
stdev_labs = [r'$\pm$ 1 std. dev. (train)', r'$\pm$ 1 std. dev. (validation)']
axes = []

for i, _ in enumerate(avgs):
    axes.append(ax.plot(x_data, tmp_df[avgs[i]], marker='o', color=colors[i], label=labs[i]))
    
for i, _ in enumerate(avgs):
    axes.append(ax.fill_between(x_data, 
                    tmp_df[avgs[i]]-tmp_df[stds[i]], 
                    tmp_df[avgs[i]]+tmp_df[stds[i]], 
                    color=colors[i], alpha=.3, label=stdev_labs[i]))
# diff
ax.fill_between(x_data, tmp_df['tr_avg'], tmp_df['val_avg'], color='none',
                hatch="XXX", edgecolor="b", linewidth=0.0, alpha=.6)

offset_coef = 0.5
ax.set_xlim([min(x_data), max(x_data)])
ax.set_xlabel('$\log(\mathrm{scale\_pos\_weight})$', fontsize=14, labelpad=20)
ax.set_ylabel('PR-AUC', fontsize=14, labelpad=20)
ax_min = min(min(tmp_df['tr_avg']-tmp_df['tr_std']), min(tmp_df['val_avg']-tmp_df['val_std']))
ax_max = max(max(tmp_df['tr_avg']+tmp_df['tr_std']), max(tmp_df['val_avg']+tmp_df['val_std']))
ax.set_ylim(ax_min-offset_coef*(ax_max-ax_min), ax_max+offset_coef*(ax_max-ax_min))

# plot number of boosting trees
# initiate a second axes that shares the same x-axis
ax2 = ax.twinx()  
ax2.set_ylabel('Number of boosting trees',fontsize=14, labelpad=20) 
axes.append(ax2.plot(x_data, tmp_df['numtrees_avg'], marker='1',linewidth=2,
                     markersize=10, linestyle=':',color=colors[2], 
                     label='average number of boosting trees'))
ax2.tick_params(axis='y')
ax2_min = min(tmp_df['numtrees_avg'])
ax2_max = max(tmp_df['numtrees_avg'])
ax2.set_ylim(ax2_min-offset_coef*(ax2_max-ax2_min), ax2_max+offset_coef*(ax2_max-ax2_min))


# fix legend
axes_ = [a[0] if isinstance(a, list) else a for a in axes]
labels = [lab.get_label() for lab in axes_]
ax.legend(axes_, labels,  fontsize=10, loc='upper right')

# title
plt.title('The difference between training and validation PR-AUC for\n'+ 
          'different values of XGBoost\'s scale_pos_weight parameter', 
           fontsize=16)
fig.tight_layout()
plt.show()</code></pre></div>
              <div style="text-align:center">
                 <a id="fig6" style="font-size: 1.5em;">Figure 6</a>
              </div>
              
              <p class="text-align: center;">
                <img src="output_4.png" style="width: 70%; display: block; margin: 10px auto 20px;"/>
              </p>
              <p>
                Looking at Figure <a href="#fig6">6</a> above, we note that the <code class="arg">scale_pos_weight</code> values 10 an. We also observe that the deviation between the model performance on training versus validation data increases with the increase in <code class="arg">scale_pos_weight</code>. The increase in the width of the hatched region shows this deviation and we can see that for $\log(\mathrm{scale\_pos\_weight})\geq1$ the model is starting to overfit. The fact that the model does not demonstrate its best performance at XGBoost's recommended value should not be too concerning as these are general recommendations which can change with the imbalance ratio, the nature of data, the train-to-validation ratio, etc. Finally, we can see that there might be a <code class="arg">scale_pos_weight</code> value bigger than $1$ that demonstrates a better performance than <code class="arg">scale_pos_weight=1</code> while satisfying the <a href="#acceptance_condition">acceptance condition</a>. We can explore this by performing another round of parameter search over a much smaller window as below.
              </p>
<pre class="prettyprint lang-language"><code class="language-python">param_group_4_new = {'scale_pos_weight': np.linspace(1,2,6)}
search_params = param_group_4_new
cur_params.update(search_params)
pprint(cur_params)</code></pre>
<pre>{'alpha': 0.001,
 'colsample_bylevel': 1,
 'colsample_bytree': 1,
 'gamma': 3,
 'learning_rate': 0.002,
 'max_depth': 3,
 'min_child_weight': 30,
 'scale_pos_weight': array([1. , 1.2, 1.4, 1.6, 1.8, 2.]),
 'subsample': 1}</pre>
 <pre class="prettyprint lang-language"><code class="language-python">tmp_df, evals = cv_search_params(X_train, y_train, cur_params, all_features, rskf)
tmp_df.to_pickle('hyperparams_round4_new.pkl')</code></pre>
<pre>Grid search on:

{'alpha': 0.001,
 'colsample_bylevel': 1,
 'colsample_bytree': 1,
 'gamma': 3,
 'learning_rate': 0.002,
 'max_depth': 3,
 'min_child_weight': 30,
 'scale_pos_weight': array([1.0, 1.2, 1.4, 1.6, 1.8, 2.0]),
 'subsample': 1}

started at 2021-06-07 15:07:46.962607
Total number of hyperparameter candidates = 6

CV 1 on:

{'scale_pos_weight': 1.0}

 started!

Summary:
 mean PR-AUC training =  0.788
 mean PR-AUC validation = 0.772
 mean PR-AUC difference = 0.0167
 average number of boosting rounds tr = 1958
CV 1 ended! (took 260.34 seconds)
</pre>
<button class="btn btn-primary plus-minus-output collapsed" type="button" data-toggle="collapse" data-target="#collapseCVOutput6" aria-expanded="false" aria-controls="collapseCVOutput6">
</button>  
<div class="collapse" id="collapseCVOutput6" aria-expanded="false" style="height: 0px;">
    <pre >
CV 2 on:

{'scale_pos_weight': 1.2}

 started!

Summary:
 mean PR-AUC training =  0.794
 mean PR-AUC validation = 0.778
 mean PR-AUC difference = 0.0158
 average number of boosting rounds tr = 1947
CV 2 ended! (took 258.66 seconds)

CV 3 on:

{'scale_pos_weight': 1.4}

 started!

Summary:
 mean PR-AUC training =  0.794
 mean PR-AUC validation = 0.773
 mean PR-AUC difference = 0.0203
 average number of boosting rounds tr = 1425
CV 3 ended! (took 220.09 seconds)

CV 4 on:

{'scale_pos_weight': 1.6}

 started!

Summary:
 mean PR-AUC training =  0.799
 mean PR-AUC validation = 0.777
 mean PR-AUC difference = 0.0225
 average number of boosting rounds tr = 1450
CV 4 ended! (took 212.62 seconds)

CV 5 on:

{'scale_pos_weight': 1.8}

 started!

Summary:
 mean PR-AUC training =  0.807
 mean PR-AUC validation = 0.780
 mean PR-AUC difference = 0.0266
 average number of boosting rounds tr = 1835
CV 5 ended! (took 257.85 seconds)
</pre></div>
<pre>
CV 6 on:

{'scale_pos_weight': 2.0}

 started!

Summary:
 mean PR-AUC training =  0.815
 mean PR-AUC validation = 0.787
 mean PR-AUC difference = 0.0282
 average number of boosting rounds tr = 2168
CV 6 ended! (took 312.25 seconds)

Grid search ended at 2021-06-07 15:23:36.177770</pre>
<button class="btn btn-primary plus-minus-code collapsed" type="button" data-toggle="collapse" data-target="#fig7code" aria-expanded="false" aria-controls="fig7code">
</button>  
<div class="collapse" id="fig7code" aria-expanded="false" style="height: 0px;">
<pre class="prettyprint lang-language"><code class="language-python">fig = plt.figure(dpi=280, figsize=(8, 8))
ax = fig.add_subplot(111)
labels = []
x_data = [x['scale_pos_weight'] for x in tmp_df['current_params']]
colors = ['dodgerblue', 'lightsalmon','forestgreen']
labs = ['train', 'validation']
avgs = ['tr_avg', 'val_avg']
stds = ['tr_std', 'val_std']
stdev_labs = [r'$\pm$ 1 std. dev. (train)', r'$\pm$ 1 std. dev. (validation)']
axes = []

for i, _ in enumerate(avgs):
    axes.append(ax.plot(x_data, tmp_df[avgs[i]], marker='o', color=colors[i], label=labs[i]))
    
for i, _ in enumerate(avgs):
    axes.append(ax.fill_between(x_data, 
                    tmp_df[avgs[i]]-tmp_df[stds[i]], 
                    tmp_df[avgs[i]]+tmp_df[stds[i]], 
                    color=colors[i], alpha=.3, label=stdev_labs[i]))
# diff
ax.fill_between(x_data, tmp_df['tr_avg'], tmp_df['val_avg'], color='none',
                hatch="XXX", edgecolor="b", linewidth=0.0, alpha=.6)
offset_coef = 0.5
ax.set_xlim([min(x_data), max(x_data)])
ax.set_xlabel('scale_pos_weight', fontsize=14)
ax.set_ylabel('PR-AUC', fontsize=14)
ax_min = min(min(tmp_df['tr_avg']-tmp_df['tr_std']), min(tmp_df['val_avg']-tmp_df['val_std']))
ax_max = max(max(tmp_df['tr_avg']+tmp_df['tr_std']), max(tmp_df['val_avg']+tmp_df['val_std']))
ax.set_ylim(ax_min-offset_coef*(ax_max-ax_min), ax_max+offset_coef*(ax_max-ax_min))

# plot number of boosting trees
# initiate a second axes that shares the same x-axis
ax2 = ax.twinx()  
ax2.set_ylabel('Number of boosting trees') 
axes.append(ax2.plot(x_data, tmp_df['numtrees_avg'], marker='1',linewidth=2,
                     markersize=10, linestyle=':',color=colors[2], 
                     label='average number of boosting trees'))
ax2.tick_params(axis='y')
ax2_min = min(tmp_df['numtrees_avg'])
ax2_max = max(tmp_df['numtrees_avg'])
ax2.set_ylim(ax2_min-offset_coef*(ax2_max-ax2_min), ax2_max+offset_coef*(ax2_max-ax2_min))


# fix legend
axes_ = [a[0] if isinstance(a, list) else a for a in axes]
labels = [lab.get_label() for lab in axes_]
ax.legend(axes_, labels,  fontsize=10, loc='upper left')

# title
plt.title('The difference between training and validation PR-AUC for\n'+ 
          'different values of XGBoost\'s scale_pos_weight parameter', 
           fontsize=16)
fig.tight_layout()
plt.show()</code></pre></div>
              <div style="text-align:center">
                 <a id="fig7" style="font-size: 1.5em;">Figure 7</a>
              </div>
              
              <p class="text-align: center;">
                <img src="output_5.png" style="width: 70%; display: block; margin: 10px auto 20px;"/>
              </p>
<pre class="prettyprint lang-language"><code class="language-python">tmp_df[tmp_df['diff']&#60;acceptance_threshold] \
    .sort_values('val_avg', ascending=False) \
    .head()</code></pre>
              <table class="table table-striped">
                <thead>
                  <tr style="text-align: left;">
                    <th></th>
                    <th>current_params</th>
                    <th>tr_avg</th>
                    <th>tr_std</th>
                    <th>val_avg</th>
                    <th>val_std</th>
                    <th>numtrees_avg</th>
                    <th>numtrees_std</th>
                    <th>diff</th>
                    <th>time_taken</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th scope="row">1</th>
                    <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
                    <td>0.794</td>
                    <td>0.016</td>
                    <td>0.778</td>
                    <td>0.038</td>
                    <td>1958.311</td>
                    <td>1139.170</td>
                    <td>0.016</td>
                    <td>258.66</td>
                  </tr>
                  <tr>
                    <th scope="row">0</th>
                    <td>{'max_depth': 3, 'min_child_weight': 30, 'subs...</td>
                    <td>0.788</td>
                    <td>0.016</td>
                    <td>0.772</td>
                    <td>0.037</td>
                    <td>1947.413</td>
                    <td>1279.757</td>
                    <td>0.011</td>
                    <td>260.34</td>
                  </tr>
                </tbody>
              </table>
 <pre class="prettyprint lang-language"><code class="language-python">tmp_df[tmp_df['diff']&#60;acceptance_threshold] \
    .sort_values('val_avg', ascending=False) \
    .iloc[0]['current_params']['scale_pos_weight']</code></pre>
 <pre>1.2</pre>
              <p>
                Note how this little change of <code class="arg">scale_pos_weight</code> from 1 to 1.2 improves the validation PR-AUC by roughly 1$\%$. <em>This shows how important this hyperparameter is especially when dealing with highly imbalanced datasets</em>.
              </p>
              <h5><a class="header_arg" id="gs4_notes_1"></a>Important note regarding overfitting and the <em><a href="#acceptance_threshold">acceptance threshold</a></em></h5>
              <p>
                Looking at the figure above, one might ask why we limited ourselves to <a href="#acceptance_condition">acceptance condition</a>? For instance, taking <code class="arg">scale_pos_weight</code> to be 3 rather than 1.2 increases the average train and validation PR-AUC by roughly 4$\%$ while the difference between the two is only about 0.7$\%$ above the <a href="#acceptance_condition">acceptance threshold</a>. This is a great question and in fact, some would characterize over fitting as a <strong><em><a href="https://stats.stackexchange.com/questions/9053/how-does-cross-validation-overcome-the-overfitting-problem">situation where increasing the complexity of the model slightly tends to increase the validation error</a></em></strong>, e.g. $\log(\mathrm{scale\_pos\_weight})>1$ in Figure<a href="#fig6">6</a>. There is no good answer to questions like this as the answer might change depending on the business objectives, computational resources, etc. The important thing here is to make sure that we understand the reasoning behind eliminating or accepting some candidates. I performed a similar analysis to previous round of grid search by repeating the grid search for <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">scale_pos_weight</code> with no early stopping.
              </p>
              <h5><a class="header_arg" id="gs4_notes_2"></a>Variation of <code>logloss</code> and <code>PR-AUC</code> for <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">scale_pos_weight=[1,5,20,100,1000]</code></h5>
              <button class="btn btn-primary plus-minus-code collapsed" type="button" data-toggle="collapse" data-target="#fig8code" aria-expanded="false" aria-controls="fig8code">
</button>  
<div class="collapse" id="fig8code" aria-expanded="false" style="height: 0px;">
<pre class="prettyprint lang-language"><code class="language-python">fig = plt.figure(dpi=280, figsize=(16, 8))
num_folds = 20
test_params = {'scale_pos_weight': [0.5, 1, 2, 5, 10, 20, 50, 100, 500, 1000][1::2]}
test_evals = evals[1::2]
metrics = ['logloss','aucpr']
num_cases = len(list(product(*test_params.values())))
colors=[plt.cm.jet(int(i/num_cases*256)) for i,_ in enumerate(product(*test_params.values()))]
step=1000
num_trees=10000

for j,metric in enumerate(metrics):
    ax = fig.add_subplot(1,2,j+1)
    for i,(scale_pos_weight) in enumerate(product(*test_params.values())):
        eval_tr = np.mean(np.asarray([test_evals[i][x]['train'][metric] for x in range(num_folds)]), axis=0)[::step]
        eval_val = np.mean(np.asarray([test_evals[i][x]['eval'][metric] for x in range(num_folds)]), axis=0)[::step]
        len_eval = len(np.mean(np.asarray([test_evals[i][x]['train'][metric] for x in range(num_folds)]), axis=0))
        x_data = np.arange(1,len(eval_tr)+1)*step
        ax.plot(x_data, eval_tr, 
                            marker='o', markersize=6, 
                            markerFaceColor='white', 
                            color=colors[i],
                            label=f"scale_pos_weight={scale_pos_weight[0]} (tr-{metric})")
        ax.plot(x_data, eval_val, 
                            marker='s', markersize=6, 
                            markerFaceColor='white', 
                            color=colors[i], linestyle=':',
                            label=f"scale_pos_weight={scale_pos_weight[0]} (val-{metric})")
    ax.set_xlabel('number of boosting rounds', fontsize=18)
    ax.set_ylabel(f'{metric}', fontsize=18, labelpad=15)
    legend_loc = {0:'upper', 1:'lower'}.get(j)
    ax.legend(loc=f"{legend_loc} right",fontsize=12)
plt.suptitle('Variation of logloss and PR-AUC with the number of\n' +
             'boosting rounds for scale_pos_weight=[1,5,20,100,1000]', 
            fontsize=22)
plt.show()</code></pre></div>
              <div style="text-align:center">
                 <a id="fig8" style="font-size: 1.5em;">Figure 8</a>
              </div>

              <p class="text-align: center;">
                <img src="round4.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
              </p> 
              <p>
                <ul>
                  <li>Notice that the reduction in logloss slows down for the higher values of <code>scale_pos_weight</code>. I believe that the possible reson behind this behavior is the model up samples the fraudulent class, i.e., repeats the positive instances <code>scale_pos_weight</code> times to make it a balanced problem but since the positive class is generally harder to predict, it requires a higher number of boosting rounds for the logloss to reduce.</li>
                  <li>Note that until around 2000 boosting rounds, the training and validation PR-AUC improve simultaneously and roughly with the same rate. In this period, <code>scale_pos_weight=5</code> demonstrates a superior performance compared to other values.</li>
                </ul>
              </p>

              <p>
                Nevertheless, we stick to the rule that we initially defined and move forward with <code class="arg">scale_pos_weight=1.2</code>.
              </p>
              </p>

              <h4><a class="header_arg" id="gs5"></a>Gridsearch for parameter group 5: <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;"> learning_rate </code></h4>
              <p>
                The last step in our hyperparameter grid search is tuning the learning rate where we explore 11 learning rates in the range (0.0001, 0.01). Furthermore, to study the effect of <code>Early_Stop_Rounds</code> in more detail, we repeat the grid search for multiple values of <code>Early_Stop_Rounds</code>. This can help us to understand the benefits and potential disadvantages of using <code>Early_Stop_Rounds</code>. We also keep track of <code>'error'</code> in addition to <code>aucpr</code> and <code>logloss</code> to see if we can get more insights about the evolution of <code>eval_metrics</code> with the grid search parameter. Finally, we increase the maximum number of boosting rounds to 10000.
              </p>
 <pre class="prettyprint lang-language"><code class="language-python">cur_params.update({'scale_pos_weight': 1.2})
search_params = param_group_5
cur_params.update(search_params)
pprint(cur_params)</code></pre>
<pre>
  {'alpha': 0.001,
 'colsample_bylevel': 1,
 'colsample_bytree': 1,
 'gamma': 3,
 'learning_rate': array([0.0001 , 0.00015849, ..., 0.00630957, 0.01]),
 'max_depth': 3,
 'min_child_weight': 30,
 'scale_pos_weight': 1.2,
 'subsample': 1}</pre>
<h5><a class="header_arg" id="gs5_notes_1"></a>Evolution of <code>eval_metrics</code> with the number of boosting rounds and <code>learning_rate</code></h5>
  <pre class="prettyprint lang-language"><code class="language-python">dfs = []
early_stoppng_values = [200, 400, 1000, 2000, 5000, 10000]
for i, es in enumerate(early_stoppng_values):
    tmp_df, evals = cv_search_params(X_train, y_train, cur_params, all_features, \
                                     rskf, eval_metric=['error','logloss','aucpr'], \
                                     num_boost_rounds=10000, early_stop_rounds=es)
    tmp_df['early_stop'] = es
    with open('hyperparam5_{es}_evals.pkl', 'wb') as pickle_file:
        pickle.dump(evals, pickle_file, protocol=4)
    tmp_df.to_pickle('hyperparam5_{es}_df.pkl')
    dfs.append(tmp_df)</code></pre>
              <p>
                First, we look at the variation of average <code>error</code>, <code>logloss</code>, and <code>aucpr</code> for training and validation data when there is no early stopping.
              </p>
</pre>
<button class="btn btn-primary plus-minus-code collapsed" type="button" data-toggle="collapse" data-target="#codefig9" aria-expanded="false" aria-controls="codefig9">
</button>  
<div class="collapse" id="codefig9" aria-expanded="false" style="height: 0px;">
    <pre class="prettyprint lang-language"><code class="language-python">fig,axes = plt.subplots(3,1,dpi=280, figsize=(10,12), sharex=True)
sns.axes_style('white')
sns.set(rc={'axes.facecolor':'lightgray', 'figure.facecolor':'white'})
num_folds = 20
test_params = {}
test_params['learning_rate'] = np.logspace(-4, -2, 11)[::2]
fig.tight_layout()
test_evals = eevals[::2]
metrics = ['error','logloss','aucpr']
num_cases = len(list(product(*test_params.values())))
colors=[plt.cm.jet(int(i/num_cases*256)) for i,_ in enumerate(product(*test_params.values()))]
step=250
ylims = [[6.1e-4, 7.7e-4],[-0.05, 0.75],[0.67, 0.83]]
plt.subplots_adjust(hspace=0.02)
for j,metric in enumerate(metrics):
    for i,lr in enumerate(product(*test_params.values())):
        ax = axes[j]
        eval_tr = np.mean(np.asarray([test_evals[i][x]['train'][metric] for x in range(num_folds)]), 
                                     axis=0)[::step]
        eval_val = np.mean(np.asarray([test_evals[i][x]['eval'][metric] for x in range(num_folds)]), 
                                     axis=0)[::step]
        len_eval = len(np.mean(np.asarray([test_evals[i][x]['train'][metric] for x in range(num_folds)]), 
                                     axis=0))
        x_data = np.arange(1,len(eval_tr)+1)*step
        ax.plot(x_data, eval_tr, 
                            marker='o', markersize=6, 
                            markerFaceColor='white', 
                            color=colors[i],
                            label=f"LR={lr[0]:.2e} (tr-{metric})")
        ax.plot(x_data, eval_val, 
                            marker='s', markersize=6, 
                            markerFaceColor='white', 
                            color=colors[i], linestyle=':',
                            label=f"LR={lr[0]:.2e} (val-{metric})")
        sns.despine(left=False)
        ax.set_ylim(ylims[j])
    if j==1:
        ax.legend(bbox_to_anchor=(1,0.5,0,0),loc=f"center left",fontsize=10)
    ax.set_ylabel(f'{metric}', fontsize=18, labelpad=15)
plt.suptitle('Variation of error, logloss, and aucpr for training and validation\n' +
             'with the number of boosting rounds with learning rate', y=1.04,
            fontsize=22)
plt.xlabel('number of boosting rounds', fontsize=18, labelpad=10)
plt.show();</code></pre></div>
              <div style="text-align:center">
                 <a id="fig9" style="font-size: 1.5em;">Figure 9</a>
              </div>
              <p class="text-align: center;">
                <img src="fig15.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
              </p>
              <p>
                There are some interesting observations to point out in <a href="#fig9">Figure 9</a>
                <ul>
                  <li>
                    All evaluation metric for the higher learning rates, i.e., <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">lr=1e-2, 3.98e-3</code>, remain unchanged after $n_{\text{tree}}\approx3000$. This means adding more complexity to the model does not imprpove the performance of the model. 
                  </li>
                  <li>
                    In contrast, when looking at the results for <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">lr=2.5e-4</code>, we notice that even at a much higher number of boosting rounds, the <strong>validation metrics</strong> are still improving. 
                  </li>

                  <li>
                    For the really low values of learning rate, i.e., <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">lr=1e-4</code>, the model is certainly underfitting as all the evaluation metrics are still improving at $n_{\text{tree}}\approx10000$.
                  </li>

                  <li>
                    The higher the learning rate gets, the sooner the model starts to overfit. This can be seen by looking at the error and aucpr subplots in <a href="#fig9">Figure 9</a> where for the highest learning rate (<code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">lr=0.01</code>), the divergence between the training and validation data results start at the very beginning where $n_{\text{tree}}\approx250$.
                  </li>
                </ul>
              </p>
              <h5><a class="header_arg" id="gs5_notes_2"></a>How <strong>Early Stopping</strong> influences the optimal <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">learning_rate</code></h5>
              <p>
                We now look at the way using <strong>Early Stopping</strong> affects the computation cost, the best learning rate accorading to the criteria <a href="#acceptance_condition">discussed previously</a>, and whether or not that might affect the accuracy of the model. Note that the <em>size of markers in <a href="#fig10">Figure 10</a> are scaled by the square-root of the time taken to train the model</em>.
              </p>
</pre>
<button class="btn btn-primary plus-minus-code collapsed" type="button" data-toggle="collapse" data-target="#codefig10" aria-expanded="false" aria-controls="codefig10">
</button>  
<div class="collapse" id="codefig10" aria-expanded="false" style="height: 0px;">
    <pre class="prettyprint lang-language"><code class="language-python">fig,ax = plt.subplots(1,1,dpi=400, figsize=(15,15))
sns.axes_style('white')
sns.set(rc={'axes.facecolor':'lightgray', 'figure.facecolor':'white'})
x_data = np.logspace(-4, -2, 11)
num_case = len(early_stoppng_values)
metric='aucpr'
colors=[plt.cm.jet(int(i/num_cases*256)) for i,_ in enumerate(early_stoppng_values)]

for i,es in enumerate(early_stoppng_values):
    
    y_data = dfs[i].iloc[:,1] # train
    # scale the plot markers by the square root of the time taken
    time_taken = np.sqrt(dfs[i].iloc[:,8].astype(float))*3
    ax.plot(x_data, y_data, 
                marker=None, 
                color=colors[i], linestyle=':')
    ax.scatter(x_data, y_data, 
                marker='o', s=time_taken, 
                color=colors[i],
                label=f"Estop={es} (tr-{metric})", zorder=-i+10)
    
    y_data = dfs[i].iloc[:,3] # validation
    ax.plot(x_data, y_data, 
                marker=None, 
                color=colors[i], linestyle=':')
    ax.scatter(x_data, y_data, 
                marker='d', s=time_taken, 
                color=colors[i], linestyle=':',
                label=f"Estop={es} (val-{metric})", zorder=-i+10)
    sns.despine(left=False)
ax.set_xscale('log')
ax.set_ylabel(f'aucpr', fontsize=22, labelpad=15)
plt.suptitle('Variation of aucpr for training and validation data for different\n' +
             'learning rates and different values of early stopping', y=0.95,
             fontsize=28)
plt.setp(ax.get_xticklabels(), fontsize=18)
plt.setp(ax.get_yticklabels(), fontsize=18)
plt.legend(bbox_to_anchor=(1,0.5,0,0),loc=f"center left",fontsize=14)
plt.xlabel('learning rate', fontsize=22)
plt.show()</code></pre></div>
              <div style="text-align:center">
                 <a id="fig10" style="font-size: 1.5em;">Figure 10</a>
              </div>
              <p class="text-align: center;">
                <img src="fig16.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
              </p>
              <p>
                <a href="#fig10">Figure 10</a> shed light on choosing the best number of trees as the <strong>Early Stopping criteria</strong>. 
                <ul>
                  <li>
                    The lower <strong>Early Stopping</strong> means the less computation cost. This can clearly be seen by the increase in the markers' size as the <strong>Early Stopping</strong>  increases. 
                  </li>
                  <li>
                    Notice that there is a bottleneck in all training-validation-pair curves for each Early Stop value where the training and validation aucpr are the closest. For instance for <code>Estop=200</code>, the bottleneck occurs at <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">lr=2.5e-4</code> whereas for <code>Estop=2000</code>, it occurs at <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">lr=6.51e-4</code>.
                  </li>
                  <li>
                    Comparing the training and validation curves for <code>(Estop=200, lr=2.51e-3)</code> and <code>(Estop=10000, lr=2.51e-4</code>) with roughly the same performance for the training and validation PR-AUC. Notice that early stopping can significantly reduce the computation cost as the first case takes 107 seconds whereas the second case takes 1409 seconds. An almost 14 folds increase in the compuation cost.
                  </li>
                </ul>
              </p>
              <p>
                Finally, we can take a look at all the learning rates that meet the <a href="#acceptance_condition">Acceptance Condition</a>. 
              </p>
<pre class="prettyprint lang-language"><code class="language-python">for i,es in enumerate(early_stoppng_values): 
    dfs[i]['early_stop'] = es
    dfs[i]['learning_rate'] = 0
    for j in range(len(dfs[i])):
        dfs[i]['learning_rate'].iloc[j]=f"{dfs[i]['current_params'].iloc[j]['learning_rate']:.2e}"
df_es = pd.concat(dfs, axis=0)
df_es = df_es[df_es['diff']&#60;acceptance_threshold].sort_values('val_avg', ascending=False)
df_es = df_es.reset_index(drop=True)
df_es[['tr_avg','val_avg','time_taken','early_stop','learning_rate']].iloc[:,:10]</code></pre>
              <div>
               <table class="table table-striped">
                <thead>
                  <tr style="text-align: left;">
                    <th></th>
                    <th>tr_avg</th>
                    <th>val_avg</th>
                    <th>numtrees_avg</th>
                    <th>time_taken</th>
                    <th>early_stop</th>
                    <th>learning_rate</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th scopre="row">0</th>
                    <td>0.801</td>
                    <td>0.782</td>
                    <td>3227.400</td>
                    <td>438.81</td>
                    <td>1000</td>
                    <td>1.58e-03</td>
                  </tr>
                  <tr>
                    <th scopre="row">1</th>
                    <td>0.798</td>
                    <td>0.781</td>
                    <td>1963.000</td>
                    <td>285.39</td>
                    <td>400</td>
                    <td>2.51e-03</td>
                  </tr>
                  <tr>
                    <th scopre="row">2</th>
                    <td>0.800</td>
                    <td>0.780</td>
                    <td>5000.000</td>
                    <td>556.59</td>
                    <td>5000</td>
                    <td>1.00e-03</td>
                  </tr>
                  <tr>
                    <th scopre="row">3</th>
                    <td>0.799</td>
                    <td>0.780</td>
                    <td>4220.050</td>
                    <td>529.53</td>
                    <td>2000</td>
                    <td>1.00e-03</td>
                  </tr>
                  <tr>
                    <th scopre="row">4</th>
                    <td>0.795</td>
                    <td>0.778</td>
                    <td>3801.550</td>
                    <td>482.29</td>
                    <td>1000</td>
                    <td>1.00e-03</td>
                  </tr>
                  <tr>
                    <th scopre="row">5</th>
                    <td>0.794</td>
                    <td>0.778</td>
                    <td>985.150</td>
                    <td>138.15</td>
                    <td>200</td>
                    <td>3.98e-03</td>
                  </tr>
                  <tr>
                    <th scopre="row">6</th>
                    <td>0.793</td>
                    <td>0.776</td>
                    <td>10000.000</td>
                    <td>1409.15</td>
                    <td>10000</td>
                    <td>2.51e-04</td>
                  </tr>
                  <tr>
                    <th scopre="row">7</th>
                    <td>0.789</td>
                    <td>0.773</td>
                    <td>5000.000</td>
                    <td>551.06</td>
                    <td>5000</td>
                    <td>6.31e-04</td>
                  </tr>
                  <tr>
                    <th scopre="row">8</th>
                    <td>0.789</td>
                    <td>0.773</td>
                    <td>4322.450</td>
                    <td>539.58</td>
                    <td>2000</td>
                    <td>6.31e-04</td>
                  </tr>
                  <tr>
                    <th scopre="row">9</th>
                    <td>0.788</td>
                    <td>0.773</td>
                    <td>1848.850</td>
                    <td>263.90</td>
                    <td>400</td>
                    <td>1.58e-03</td>
                  </tr>
                </tbody>
               </table>
              </div>
              <p>
                Note that the value that we chose for <em>Early Stopping</em> gives the second best performance among all the combinations with only 0.1% lower accuracy than the best performance for <code>val_avg</code>. However, it takes rouhgly half the time that takes to train a model with <code>(Estop=400, lr=2.51e-3)</code> than the time that takes to train using <code>(Estop=1000, lr=1.58e-3)</code>. We keep the early stopping value to be $400$ and move on to the last part of the project with <code class="arg" style="font-size: 0.8em; background-color: #17b8a138;">lr=2.51e-3</code>.
              </p>
              <h3><a class="header_arg" id="best_threshold"></a>Plotting the PR curves and finding the best threshold for the winner model</h3>
              <p>
                In this part, we will find the <a href="../bank_marketing_data_analysis/project.html#threshold">threshold</a> that gives the best balance of precision and recall. In addition, we plot the <a href="../bank_marketing_data_analysis/project.html#pr_curve">precision-recall curve</a> for the training, validation, and test data. We could locate the threshold that gives the optimal balance between precision and recall using the <strong>Harmonic Mean</strong> of the two. <strong>Harmonic Mean</strong> or <strong>H-Mean</strong> of precision and recall is called <strong>F-measure</strong> or <a href="../bank_marketing_data_analysis/project.html#fscore"><strong>F-score</strong></a> which when optimized, will seek a balance between the precision and recall. 
              </p>
$$
\text{F-score} = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$
              or
$$
\frac{1}{\text{F-score}} = \frac{1}{\dfrac{1}{2}\big(\dfrac{1}{\text{Precision}} + \dfrac{1}{\text{Recall}}\big)}
$$
              <p>
                One way to think about why optimizing F-score gives the best balance between precision and recall is that we can not get a high F-score if either one is very low. The threshold that gives the highest F-score is what we're looking for. First thing, we re-evaluate the model using the winner hyperparameters. This time, we also return the <code class="object">clf</code> so that we can make predictions on the test data. Finally, we also need <code>fold_preds</code> so that we can plot the curves.
              </p>
<pre class="prettyprint lang-language"><code class="language-python">winner_params = {
    'max_depth': 3,
    'min_child_weight': 30,
    'subsample': 1,
    'colsample_bytree': 1,
    'colsample_bylevel': 1,
    'alpha': 1e-3,
    'gamma': 3,
    'scale_pos_weight': 1.2,
    'learning_rate': 2.51e-3,
}

final_eval_df, evals, fold_preds, clf = cv_summary(X_train, y_train, winner_params, all_features, rskf, 
                                                   eval_metric=['logloss','aucpr'], spit_out_fold_results=True,
                                                   num_boost_rounds=1963, early_stop_rounds=None, save_fold_preds=True)</code></pre>
<pre> Repeat 1, Fold 1 - PR-AUC tr = 0.807, PR-AUC vl = 0.776 (diff = 0.0314)
 Repeat 1, Fold 2 - PR-AUC tr = 0.809, PR-AUC vl = 0.748 (diff = 0.0611)
 Repeat 1, Fold 3 - PR-AUC tr = 0.809, PR-AUC vl = 0.772 (diff = 0.0366)
 Repeat 1, Fold 4 - PR-AUC tr = 0.786, PR-AUC vl = 0.848 (diff = -0.0613)
 Repeat 2, Fold 1 - PR-AUC tr = 0.800, PR-AUC vl = 0.787 (diff = 0.0131)
 Repeat 2, Fold 2 - PR-AUC tr = 0.791, PR-AUC vl = 0.718 (diff = 0.0725)
 Repeat 2, Fold 3 - PR-AUC tr = 0.816, PR-AUC vl = 0.787 (diff = 0.0291)
 Repeat 2, Fold 4 - PR-AUC tr = 0.800, PR-AUC vl = 0.800 (diff = -0.0006)
 Repeat 3, Fold 1 - PR-AUC tr = 0.809, PR-AUC vl = 0.745 (diff = 0.0640)
 Repeat 3, Fold 2 - PR-AUC tr = 0.811, PR-AUC vl = 0.747 (diff = 0.0637)
 Repeat 3, Fold 3 - PR-AUC tr = 0.803, PR-AUC vl = 0.802 (diff = 0.0007)
 Repeat 3, Fold 4 - PR-AUC tr = 0.787, PR-AUC vl = 0.854 (diff = -0.0669)
 Repeat 4, Fold 1 - PR-AUC tr = 0.809, PR-AUC vl = 0.734 (diff = 0.0756)
 Repeat 4, Fold 2 - PR-AUC tr = 0.791, PR-AUC vl = 0.771 (diff = 0.0203)
 Repeat 4, Fold 3 - PR-AUC tr = 0.793, PR-AUC vl = 0.837 (diff = -0.0434)
 Repeat 4, Fold 4 - PR-AUC tr = 0.802, PR-AUC vl = 0.765 (diff = 0.0375)
 Repeat 5, Fold 1 - PR-AUC tr = 0.800, PR-AUC vl = 0.774 (diff = 0.0262)
 Repeat 5, Fold 2 - PR-AUC tr = 0.805, PR-AUC vl = 0.776 (diff = 0.0294)
 Repeat 5, Fold 3 - PR-AUC tr = 0.797, PR-AUC vl = 0.818 (diff = -0.0212)
 Repeat 5, Fold 4 - PR-AUC tr = 0.805, PR-AUC vl = 0.782 (diff = 0.0228)
Summary:
 mean PR-AUC training =  0.801
 mean PR-AUC validation = 0.782
 mean PR-AUC difference = 0.0195</pre>
<pre class="prettyprint lang-language"><code class="language-python"># making predictions on the train and test data
# Winner model parameters
param_dict = {'tree_method':'gpu_hist'
              'objective':'binary:logistic'
              'eval_metric':'aucpr'}
param_dict.update(winner_params)
# train data into xgb.DMatrix
xg_train = xgb.DMatrix(X_train, feature_names=all_features, label=y_train)
# train
clf = xgb.train(param_dict, xg_train, num_boost_round=1977)
# predict train
y_pred_train = clf.predict(xg_train)
# predict test
y_pred_test = clf.predict(xgb.DMatrix(X_test, feature_names=all_features))

# chnage key names and update with train and test predictions
fold_preds['cv-train'] = fold_preds.pop('train')
fold_preds['cv-val'] = fold_preds.pop('eval')
fold_preds['train'] = [[y_train, y_pred_train]]
fold_preds['test'] = [[y_test, y_pred_test]]
</code></pre>
<h4>Plotting the PR curves</h4>
<button class="btn btn-primary plus-minus-code collapsed" type="button" data-toggle="collapse" data-target="#fig9code" aria-expanded="false" aria-controls="fig9code">
</button>  
<div class="collapse" id="fig9code" aria-expanded="false" style="height: 0px;">
<pre class="prettyprint lang-language"><code class="language-python">def col_to_hex(colmap, n):
    """colormap to n hex colors"""
    out = []
    for i in range(n):
        r,g,b,_ = plt.cm.get_cmap(colmap,n)(i)
        out.append(f"#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}")
    return out

def f_score(re,pr):
    return 2*re*pr/(re+pr)

sns.set(rc={'axes.facecolor':'whitesmoke', 'figure.facecolor':'silver', 'legend.facecolor':'white'})
fig, axes = plt.subplots(4, 1, figsize=(8,30), dpi=150)
axes = axes.flat
dataset_labels = list(fold_preds.keys())
n_divs = 1000
for i,ax in enumerate(axes):
    data = fold_preds[dataset_labels[i]]
    cols = col_to_hex('tab20', len(data))
    aucs = []
    pr_interp = []
    fs_best_ = []
    for j,fold_data in enumerate(data):
        y_true = fold_data[0]
        y_pred = fold_data[1]
        pr, re, thr = precision_recall_curve(y_true,y_pred,pos_label=1)
        pr_auc = average_precision_score(y_true,y_pred)
        aucs.append(pr_auc)
        # approximate AUC
        x_range = np.linspace(0, 1, n_divs)
        yinterp_pr = np.interp(x_range, re[::-1],pr[::-1])
        # threshold = 0.5
        thr_50_id = np.where(thr>0.49999)[0][0]
        # fs(threshold=0.5)
        fs_50 = f_score(re[thr_50_id],pr[thr_50_id])
        pr_interp.append(yinterp_pr)
        interp_auc=  auc(x_range[::-1],yinterp_pr)
        # best fs and best threshold
        f_scores = [(re[:-1][i],pr[:-1][i],t) for i,t in enumerate(thr)]
        best_r, best_p, best_t = sorted(f_scores, key=lambda x: -f_score(x[0],x[1]))[0]
        fs_best = f_score(best_r, best_p)
        fs_best_.append(fs_best)
        # show recall and precision that gives the best fs
        if j==0: # cv-train or cv-val
            if len(data)>1:
                ax.scatter(re[thr_50_id],pr[thr_50_id], s=30, lw=1, marker="x", color='black',label=r'$\theta$=0.5')
            else:
                ax.scatter(re[thr_50_id],pr[thr_50_id], s=30, lw=1, marker="x", color='black',label=r'$\theta$=0.5 (FS=%.2f)'%(f_score(re[thr_50_id],pr[thr_50_id])))
        else: # test or train
            ax.scatter(re[thr_50_id],pr[thr_50_id], s=30, lw=1, marker="x", color='black')
        # show recall and precision at threshold=0.5
        ax.scatter(best_r,best_p, s=30, lw=1, marker="+", color=cols[j], zorder=10)
        # plot PR curve
        lab = r'AUC=%.2f'%(aucs[-1])
        if len(data)>1:
            lab += r'(%.2f) - FS$_{\max}$=%.2f($\theta_{\mathrm{best}}$=%.2f)'%(interp_auc,fs_best,best_t)
        ax.plot(re,pr,':',linewidth=1, color=cols[j], label=lab, zorder=-1)
    if len(data)>1:  # cv-train or cv-val  
        # mean PR curve with std shown by averaging over folds
        mean_pr = np.mean(pr_interp, axis=0)
        std_pr = np.std(pr_interp, axis=0)
        std_auc = np.std(aucs)
        precisions_upper = np.minimum(mean_pr + std_pr, 1)
        precisions_lower = np.maximum(mean_pr - std_pr, 0)
        # AUC of the mean PR curve
        mean_prauc = auc(x_range[::-1],mean_pr[::-1])
        # plot mean PR curve and add relevant info to the legend
        ax.plot(x_range[::-1],mean_pr[::-1],linewidth=2,linestyle='--',
                label=r'AUC=%.2f - AUC($\hat{\mathrm{cv}}$)=%.2f $\pm$ %0.3f'%(mean_prauc,np.mean(aucs),np.std(aucs)))
        ax.fill_between(x_range[::-1], precisions_lower[::-1], precisions_upper[::-1], color='red', alpha=.2, label=r'$\pm$ 1 std. dev. (precision)')    
        # best fscore of the mean PR curve
        f_scores = [(i, f_score(x_range[::-1][i],pr)) for i,pr in enumerate(mean_pr[::-1])]
    else: # test or train
        f_scores = [(i, f_score(re[i],pr[i])) for i,_ in enumerate(pr)]
    best_id, fs_best = sorted(f_scores, key=lambda x: -x[1])[0]        
    # annotate the recall and precision that lead to the best fscore and add relevant info to the legend
    if len(data)>1: # cv-train or cv-val  
        ax.scatter(x_range[::-1][best_id], mean_pr[::-1][best_id],s=100, 
                      lw=1, marker="*", color='orange',edgecolor='red', zorder=20,
                   label=r'FS$_{\max}$=%.2f - FS$_{\max}$($\hat{\mathrm{cv}}$)=%.2f $\pm$ %0.3f'%(fs_best,np.mean(fs_best_),np.std(fs_best_)))
    else: # test or train
        ax.scatter(re[best_id], pr[best_id],s=100, lw=1, marker="*", color='orange',edgecolor='red',zorder=20,
                   label=r'FS$_{\max}$=%.2f($\theta_{\mathrm{best}}$=%.2f)'%(fs_best,thr[best_id]))
    # perfect classifier
    ax.scatter(1, 1, s=20, marker="o", color='green', label='perfect classifier')
    ax.plot([0, 1, 1], [1, 1, 0],color='green', linestyle=":", lw=2, alpha=0.6)
    # no-skill classifier
    no_skill = len(y[y==1]) / len(y) # Taking the positive class to be "fraud"/1
    ax.plot([0, 1], [no_skill, no_skill], linestyle='--', lw=1, color='r', label='no-skill classifier')
    ax.legend(bbox_to_anchor=(1,1.01,0,0), fontsize=7)
    ax.set_xlabel("recall", fontsize=12)
    ax.set_ylabel("precision", fontsize=12)
    ax.set_title(f"PR curve for {dataset_labels[i]} data", fontsize=20)
plt.show()</code></pre></div>
              <div style="text-align:center">
                 <a id="fig12" style="font-size: 1.5em;">Figure 11</a>
              </div>
              <p class="text-align: center;">
                <img src="prcurves.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
              </p>
              <p>
                <strong>Notes:</strong>
                <ul>
                  <li><a href="#fig12">Figure 11</a> shows the precision-recall curve for <code>cv-train</code>, <code>cv-val</code>, <code>train</code>, and <code>test</code>, where <code>cv</code> denotes the plots of all the folds in cross-validation folds.</li>
                  <li>For cv curves, the mean PR curve is calculated by averaging over the folds. Because the number of data points in each fold and the corresponding PR curve is different, these curves are approximated using <code>np.interp()</code> so that they have the same shape and therefore could be averaged. <code>Area1</code> and <code>Area2</code> in cv plot legends <code>AUC=Area1(Area2)</code> denote the <a href="#prcurve">PR-AUC</a> of the actual and approximated curves, respectively. </li>
                  <li>For cv curves, the average PR curve is shown using a thick, blue dashed line <hr style="border-top: dashed 5px; width: 40px; display:inline-table; color: #4C72B0;" /> </li>
                  <li>Note that there's a negligible difference between the
                  <ul>
                    <li>AUC of the approximated curves and the actual curves</li>
                    <li>AUC($\hat{\mathrm{cv}}$) (average over cv AUCs) and the AUC of the mean PR curve</li>
                  </ul>
                  </li>
                  <li><span style="color: black; font-size:20px;">&times;</span> and colored <span style="color: black;  font-size:20px;">&plus;</span> (in cv plots) show the precision and recall calculated at $\theta=0.5$ and $\theta=\theta_{\mathrm{best}}$, respectively, where $\theta=\theta_{\mathrm{best}}$ is the threshold that maximizes the <a href="../bank_marketing_data_analysis/project.html#fscore">F-score</a>. FS$_{\max}$ is the F-score calculated at $\theta=\theta_{\mathrm{best}}$.</li>
                  <li>By looking at the cv-val plot, we notice that in some folds, there's a <strong>significant drop in precision at threshold values close to 1 (top left)</strong>. I reproduced those instances in the figure below:
              <div style="text-align:center">
                 <a id="fig15" style="font-size: 1.5em;">Figure 12</a>
              </div>
              <p class="text-align: center;">
                <img src="prcurves_3.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
              </p>                
                  <p>
                    Considering that the positive class includes that fraudulent transactions, this shows that the model is mistakenly classifying a lot of valid transactions as fraud ($\text{FP}$), causing the sudden drop in precision. This can mean that these specific folds contain a lot of observations from the positive class (fraud) that are hard to predict (close to the decision boundary). 
                  </p>
                  <p>
                    For the sake of clarity, let's focus on the green curve in <a href="#fig15">Figure 12</a> where a slight decrease in threshold from $0.9047$ to $0.9039$ leads to an increase in <code>(recall,precision)</code> from <code>(0.01,0.33)</code> to <code>(0.09,0.82)</code>. We can translate this into something even more clear; For every observation $X_i$, model prediction is a probability $p_i$ that measures the chance of $X_i$ belonging to the positive class, where $0 \leq p_i \leq 1$. What $\theta_1=0.9047$ means is that at this threshold, $X_i$ will be classified positive (fraud transaction) only if $p_i>0.9047$ and otherwise it will be classified as negative (valid transaction). 
                  </p>             

$$
\begin{align}
\text{Recall}&=\dfrac {\text{TP}}{\text{TP+FN}}\\[2ex]
\text{Precision}&=\dfrac {\text{TP}}{\text{TP+FP}}
\end{align}
$$
                  <p>
                    The fact that there are roughly 100 positive observations in each fold and the recall value $0.01$ at $\theta_1=0.9047$ means that of the $\approx$100 fraud observations, there's only one observaation that the model can classifiy as fraud with a probability higher than $0.9047$, i.e., $\text{TP}=1$. Noting that $\text{Precision}=0.33\approx \dfrac{1}{3}$, this means $\text{FP}=2$. 
                  </p>
                  <p>
                    Similarly, at the slightly lower threshold $\theta_1=0.9039$, the recall value $0.09$ means that with the new threshold, the model can correctly identify 9 fraud instances $X_j$ with $p_i>0.9039$ while the rest of the fraud observations $X_j$ will be classified as valid because $p_j\leq0.9039$. Noting that $\text{Precision}=0.82\approx \dfrac{9}{11}$, this means that $\text{FP}$ remains unchanged at 2.
                  </p>
                  </p>

                </li>
                </ul>
              </p>
              <h3><a class="header_arg" id="baseline"></a>Comparison with the baseline model</h3>
              <p>
                In this last part, we compare the performance of <em>hypertuned model</em> with that of the <em>baseline</em> model. First, let's take a look at the hyperparameter values of the two models:
              </p>
              <div>
              <table class="table table-striped" style="display: contents;">
                <thead>
                  <tr style="text-align:left;">
                    <th style="text-align:left; background-color: #ffebe6;">Parameter</th>
                    <th style="text-align:left; background-color: #ffebe6;">Baseline Model</th>
                    <th style="text-align:left; background-color: #ffebe6;">hypertuned Model</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <th scope="row"><code class="arg">n_estimator</code></th>
                    <td>100</td>
                    <td>1963</td>
                  </tr>
                  <tr>
                    <th scope="row"><code class="arg">max_depth</code></th>
                    <td>6</td>
                    <td>3</td>
                  </tr>
                  <tr>
                    <th scope="row"><code class="arg">min_child_weight</code></th>
                    <td>1</td>
                    <td>30</td>
                  </tr>
                  <tr>
                    <th scope="row"><code class="arg">subsample</code></th>
                    <td>1</td>
                    <td>1</td>
                  </tr>
                  <tr>
                    <th scope="row"><code class="arg">colsample_bytree</code></th>
                    <td>1</td>
                    <td>1</td>
                  </tr>
                  <tr>
                    <th scope="row"><code class="arg">colsample_bylevel</code></th>
                    <td>1</td>
                    <td>1</td>
                  </tr>
                  <tr>
                    <th scope="row"><code class="arg">alpha</code></th>
                    <td>0</td>
                    <td>1e-3</td>
                  </tr>
                  <tr>
                    <th scope="row"><code class="arg">gamma</code></th>
                    <td>1</td>
                    <td>3</td>
                  </tr>
                  <tr>
                    <th scope="row"><code class="arg">scale_pos_weight</code></th>
                    <td>1</td>
                    <td>1.2</td>
                  </tr>
                  <tr>
                    <th scope="row"><code class="arg">learning_rate</code></th>
                    <td>0.3</td>
                    <td>2.51e-3</td>
                  </tr>
                </tbody>
              </table>
            </div>
              <p>
                Notice that the biggest differences in parameter values are for <code class="arg">n_estimators</code>, <code class="arg">max_depth</code>, <code class="arg">min_child_weight</code>, and <code class="arg">learning_rate</code>. We first train and test the baseline model. Next, we apply the best thresholds that we obtained for the training and test data and save them while also saving them using the default threshold $\theta=0.5$. This can shed light on the importance of finding $\theta_{\mathrm{best}}$ that maximizes the F-score. Finally, we make comparisons with the winner model regarding the performance of the models where we use <code class="library">sklearn</code>'s <code class="method">classification_report</code> and <code class="method">confusion_matrix</code> as our tools. These comparisons can help us assess how well the winner model parameters generalize and categorize the unseen data. 
              </p>
              <h4>Training and testing the baseline model</h4>
              <pre class="prettyprint lang-language"><code class="language-python"># Baseline model parameters
param_dict = {'tree_method':'gpu_hist'
              'objective':'binary:logistic'
              'eval_metric':'aucpr'}
# fit and predict baseline
baseline_XGBoost = xgb.XGBClassifier(**param_dict)
baseline_XGBoost.fit(X_train, y_train)
y_pred_train_baseline = baseline_XGBoost.predict(X_train)
y_pred_test_baseline = baseline_XGBoost.predict(X_test)</code></pre>
              <h4>Applying PR curve optial thresholds and isualizing the classification report</h4>
              <p>
                First, let's measure <code>aucpr</code>:
              </p>
<pre class="prettyprint lang-language"><code class="language-python">preds = [y_pred_train, y_pred_test, 
         y_pred_train_50, y_pred_test_50,
         y_pred_train_baseline, y_pred_test_baseline]
trues = [y_train, y_test]
for i,pred in enumerate(preds):
    print(f"{average_precision_score(trues[i%2],pred):.2f}")</code></pre>
<pre>0.81
0.84
0.66
0.73
1.00
0.81</pre>
              <p>
                Next, we apply the best thresholds to the winner model predictions:
              </p>
              <pre class="prettyprint lang-language"><code class="language-python"># threshold=0.5
y_pred_train_50 = np.round(y_pred_train)
y_pred_test_50 = np.round(y_pred_test)

# apply threshold

y_tr_pred[y_tr_pred&#60;0.297]=0
y_tr_pred[y_tr_pred&#62;=0.297]=1

y_test_pred[y_test_pred&#60;0.257]=0
y_test_pred[y_test_pred&#62;=0.257]=1</code></pre>
<button class="btn btn-primary plus-minus-code collapsed" type="button" data-toggle="collapse" data-target="#fig13code" aria-expanded="false" aria-controls="fig13code">
</button>  
<div class="collapse" id="fig13code" aria-expanded="false" style="height: 0px;">
<pre class="prettyprint lang-language"><code class="language-python">from sklearn.metrics import classification_report
from matplotlib import gridspec
sns.set(rc={'axes.facecolor':'silver', 'figure.facecolor':'gainsboro', 'legend.facecolor':'white'})
fig = plt.figure(figsize=(12, 12), dpi=80)
gs = gridspec.GridSpec(2, 2, width_ratios=[4, 5]) # because colorbar takes space!
plt.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0.1, hspace=0.25)
model_labels = ['hypertuned', 'hypertuned', 'baseline', 'baseline']
dataset_labels = ['train', 'test']*2
observation_labels = ['valid', 'fraud']
preds = [y_pred_train, y_pred_test, y_pred_train_baseline, y_pred_test_baseline]
trues = [y_train, y_test]
cbars = [False, True]
y_ticklabel_visibility = [True, False]

for i,pred in enumerate(preds):
    ax = plt.subplot(gs[i])
    clf_report = classification_report(trues[i%2], pred, labels=[0, 1], target_names=observation_labels, output_dict=True)    
    sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T,cmap="gist_heat", vmin=0, vmax=1, annot=True, 
                annot_kws={'fontsize':'large'}, fmt=".3f", ax=ax, cbar=cbars[i%2], 
                cbar_kws = dict(use_gridspec=False, location="right"))
    ax.set_title(f'{model_labels[i]} model ({dataset_labels[i]} data)', fontsize=18, y=1.1)
    ax.xaxis.tick_top()
    plt.setp(ax.get_xticklabels(), fontsize=14)
    plt.setp(ax.get_yticklabels(), fontsize=14, rotation=0, visible=y_ticklabel_visibility[i%2])
plt.suptitle('Classification report', fontsize=26, y=1.11, x=0.45)</code></pre></div>
              <div style="text-align:center">
                 <a id="fig13" style="font-size: 1.5em;">Figure 13</a>
              </div>
              <p class="text-align: center;">
                <img src="clf_report.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
              </p>
              
<h4>Visualizing the confusion matrix for the baseline and winner model predictions (using $\theta=0.5$ and $\theta=\theta_{\mathrm{best}}$)</h4>
<button class="btn btn-primary plus-minus-code collapsed" type="button" data-toggle="collapse" data-target="#fig14code" aria-expanded="false" aria-controls="fig14code">
</button>  
<div class="collapse" id="fig14code" aria-expanded="false" style="height: 0px;">
<pre class="prettyprint lang-language"><code class="language-python">from sklearn.metrics import confusion_matrix
fig, ax = plt.subplots(3, 2, figsize=(10,15), dpi=200)
ax = ax.flat
plt.tight_layout(h_pad=8, w_pad=5)
trues = [y_train, y_test]
pred_dict = {r'Tuned train data($\theta=0.5$)': y_pred_train_50, 
             r'Tuned test data($\theta=0.5$)': y_pred_test_50,
             r'Tuned train data($\theta=\theta_{best}$)': y_pred_train, 
             r'Tuned test data($\theta=\theta_{best}$)': y_pred_test,
             'Baseline train data': y_pred_train_baseline,
             'Baseline test data': y_pred_test_baseline
             }
labels = ["Valid", "Fraud"]
for i, (model_, pred) in enumerate(pred_dict.items()):
    true = trues[i%2]
    conf_mat = confusion_matrix(true, pred)
    sns.heatmap(conf_mat, xticklabels=labels, 
                yticklabels=labels, cmap="Accent_r", 
                annot=True, annot_kws={'fontsize':'x-large'}, fmt="d", ax=ax[i], cbar=False);
    ax[i].set_title(r'{0} model for {1} {2}'.format(*model_.split()), fontsize=14, y=-0.1)
    ax[i].set_ylabel('True class', fontsize=14, labelpad=10)
    ax[i].set_xlabel('Predicted class', fontsize=14, labelpad=15)
    ax[i].xaxis.set_ticks_position('top')
    ax[i].xaxis.set_label_position('top')
    plt.setp(ax[i].get_xticklabels(), fontsize=12)
    plt.setp(ax[i].get_yticklabels(), fontsize=12)
plt.suptitle('Confusion matrix', fontsize=22, y=1.05)
plt.show()</code></pre></div>
              <div style="text-align:center">
                 <a id="fig14" style="font-size: 1.5em;">Figure 14</a>
              </div>
              <p class="text-align: center;">
                <img src="conf_mat.png" style="width: 100%; display: block; margin: 10px auto 20px;"/>
              </p>

              <p>
                <strong>Notes:</strong>
                <ul>
                  <li>
                    The baseline model is severly overfitting. This is evident in both Figures <a href="#fig13">13</a> and <a href="#fig14">14</a> when comparing the training and test results (<code>aucpr train=1.00, aucpr train=0.81</code>). In fact, according to the acceptance criteria, the baseline model needs to be discarded.
                  </li>
                  <li>
                    The hypertuned model with the optimal threshold has a lower <strong>Type II error</strong> (#False negative) compared to the model with $\theta=0.5$; This comes at the cost of a higher <strong>Type I error</strong> (#$\mathrm{FN}$). For the credit card data, however, one would naturally care much more about reducing the <em>Type II error</em> than the <em>Type I error</em>.
                  </li>
                  <li>
                    Depending on the business objective, the success/failure of the model can be phrased differently. For instance, if we consider <strong>the ability to detect all fraud transactions (regardless of some valid transactions labeled as fraud)</strong> we can say the hypertuned and baseline models were $84.7$% and $87.7$% successful, respectively. This success measure chnage to $99.806$% and $99.815$% if the business cares about the success in predicting valid transactions only. In real life scenarios, the model evaluation function is persumably more complicated than this. Realisticly, the business objective should be combination of
                    <ul>
                      <li>Minimizing the losses due to fraud transactions</li>
                      <li>Minimizing the model complexity</li>
                      <li>Minimizing the customer churn</li>
                      <li>Maximizing the business profit</li>
                      <li>Minimizing the Type II error</li>
                      <li>...</li>
                    </ul>
                  </li>
                </ul>
              </p>
              
              <h3><a class="header_arg" id="conclusion"></a>Summary and Conclusion</h3>
              <ul>
                <li>In this project, we went through the process of finding the best supervised model to identify fraudulent transactions in a credit card transaction data</li>
                <li>The dataset was extremely imbalance with a valid-to-fraudulent class ratio of 581 to 1</li>
                <li>Our analysis found the <code class="method">XGBClassifier()</code> to be the model that best performs on this specific data set and assuming the PR-AUC to be the performance criteria.</li>
                <li>Due to the large number of parameters that <code class="library">XGBoost</code> takes, we employed a sequential grid search to find the optimal hyperparameters of the model.</li>
                <li>The sequential search was performed in 5 steps where at each step, the optimal value for a single or a group of hyperparameters was obtained via cross validation.</li>
                <li>We used <code>RepeatedStratifiedKFold</code> to reduce the error in the estimate of mean model performance.</li>
                <li>We learned that early stoppping can benefits the training and tuning process by <strong>reducing the overfitting</strong> and <strong>reducing the computation cost</strong>.</li>
                <li>We defined a $2%$ threshold as the maximum allowed difference between the train and validation PR-AUC. We did this so that we don't end up with a winner model that the train and test performances are significantly different(overfitting); however, one can set their criteria according to the business objective. For instance, if the business only cares about maximizing the recall (minimizing $\mathrm{FN}$), they set a threshold for the difference in performance between train and validation data in predicting recall instead of PR-AUC.</li>
                
                <li>We were able to reduce the overfitting in the baseline model by conducting a sequntial grid search on the <code class="library">xgboost</code> classfier model parameters relevant to the imbalanced classification problem.</li>

              </ul>

            </div>
          </div>
        </div>
      </section>
    </main>

<!-- ======= Footer ======= -->

    <footer id="footer" data-aos="fade-up" data-aos-easing="ease-in-out" data-aos-duration="500">
      <div class="footer-newsletter">
        <div class="container">
          <div class="row">
            <div class="col-lg-6">
              <h4><a class="header_arg" id=""></a>Newsletter</h4>
              <p>Subscribe here to get notified when a new material is posted</p>
            </div>
            <div class="col-lg-6">
              <form action="" method="post">
                <input type="email" name="email"><input type="submit" value="Subscribe">
              </form>
            </div>
          </div>
        </div>
      </div>
      <div class="footer-top">
        <div class="container">
          <div class="row">
            <div class="col-lg-4 col-md-6 footer-links">
              <h4><a class="header_arg" id=""></a>Useful Links</h4>
              <ul>
                <li><i class="bx bx-chevron-right"></i> <a href="../../index.html">Home</a></li>
                <li><i class="bx bx-chevron-right"></i> <a href="../../about.html">About Me</a></li>
                <li><i class="bx bx-chevron-right"></i> <a href="../../projects.html">Projects</a></li>
                <li><i class="bx bx-chevron-right"></i> <a href="../../tutorials.html">Tutorials</a></li>
              </ul>
            </div>
            <div class="col-lg-4 col-md-6 footer-contact">
              <h4><a class="header_arg" id=""></a>Contact</h4>
              <p>
                6455 2nd St<br>
               <p> Alexandria, VA 22312<br>
               <p> United States <br><br>
                <strong>Phone:</strong> +1 617 460 0555<br>
                <strong>Email:</strong> alimehdirahim@gmail.com<br>
              </p>
            </div>
            <div class="col-lg-4 col-md-6 footer-info">
              <h3><a class="header_arg" id=""></a>About Me</h3>
              <p> A data science enthusiast! I love developing new machine learning algorithms to address real-world problems. I try to learn something new on a daily basis.
              </p>
              <div class="social-links mt-3">
                <a href="https://github.com/alineu" class="Github"><i class="bx bxl-github"></i></a>
                <a href="https://www.linkedin.com/in/alimehdizadehrahimi/" class="linkedin"><i class="bx bxl-linkedin"></i></a>
                <a href="https://twitter.com/estoyali" class="twitter"><i class="bx bxl-twitter"></i></a>
                <a href="https://instagram.com/aliexplores" class="instagram"><i class="bx bxl-instagram"></i></a>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="container">
        <div class="copyright">
          &copy; Copyright 2021 <strong><span>Ali Mehdizadeh</span></strong>. All Rights Reserved
        </div>
        <div class="credits">
          <p>
            Page design is adopted from <a href="https://bootstrapmade.com/">BootstrapMade</a>.<br>Page icons were taken from <a href="https://www.flaticon.com">flaticon.com</a>.
          </p>
        </div>
      </div>
    </footer><!-- End Footer -->
    <a href="#" class="back-to-top"><i class="icofont-simple-up"></i></a>
    <!-- Vendor JS Files -->
    <script type="text/javascript" src="../../assets/vendor/jquery/jquery.min.js"></script>
    <script type="text/javascript" src="../../assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
    <script type="text/javascript" src="../../assets/vendor/jquery.easing/jquery.easing.min.js"></script>
    <script type="text/javascript" src="../../assets/vendor/php-email-form/validate.js"></script>
    <script type="text/javascript" src="../../assets/vendor/venobox/venobox.min.js"></script>
    <script type="text/javascript" src="../../assets/vendor/waypoints/jquery.waypoints.min.js"></script>
    <script type="text/javascript" src="../../assets/vendor/counterup/counterup.min.js"></script>
    <script type="text/javascript" src="../../assets/vendor/owl.carousel/owl.carousel.min.js"></script>
    <script type="text/javascript" src="../../assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
    <script type="text/javascript" src="../../assets/vendor/prism/prism.js"></script>
    <script type="text/javascript" src="../../assets/vendor/aos/aos.js"></script>
    <!-- Template Main JS File -->
    <script src="../../assets/js/main.js"></script>
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
    </script>
    <script id="MathJax-script"  type="text/javascript" async src="../../assets/mathjax/tex-chtml.js"></script>
    </div>
  </body>
</html>
